{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "709c8e52",
   "metadata": {},
   "source": [
    "# Example with high-resolution CMIP6 models (~100 km) using Pangeo catalog \n",
    "\n",
    "\n",
    "# Table of Contents\n",
    "<ul>\n",
    "<li><a href=\"#introduction\">1. Introduction</a></li>\n",
    "<li><a href=\"#data_wrangling\">2. Data Wrangling</a></li>\n",
    "<li><a href=\"#exploratory\">3. Exploratory Data Analysis</a></li>\n",
    "<li><a href=\"#conclusion\">4. Conclusion</a></li>\n",
    "<li><a href=\"#references\">5. References</a></li>\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f77edda",
   "metadata": {},
   "source": [
    "# 1. Introduction <a id='introduction'></a>\n",
    "Cloud feedbacks are a major contributor to the spread of climate sensitivity in global climate models (GCMs) [Zelinka et al. (2020)](https://doi-org.ezproxy.uio.no/10.1029/2019GL085782). Among the most poorly understood cloud feedbacks is the one associated with the cloud phase, which is expected to be modified with climate change [Bjordal et al. (2020)](https://doi-org.ezproxy.uio.no/10.1038/s41561-020-00649-1). Cloud phase bias, in addition, has significant implications for the simulation of radiative properties and glacier and ice sheet mass balances in climate models. \n",
    "\n",
    "In this context, this work aims to expand our knowledge on how the representation of the cloud phase affects snow formation in GCMs. Better understanding this aspect is necessary to develop climate models further and improve future climate predictions. \n",
    "\n",
    "* Retrieve CMIP6 data through [Pangeo](https://pangeo-data.github.io/pangeo-cmip6-cloud/)\n",
    "* Hybrid sigma-pressure coordinates to isobaric pressure levels of the European Centre for Medium-Range Weather Forecast Re-Analysis 5 (ERA5) with [GeoCAT-comb](https://geocat-comp.readthedocs.io/en/latest/index.html)\n",
    "* Regridd the CMIP6 variables to the exact horizontal resolution with [`xesmf`](https://xesmf.readthedocs.io/en/latest/)\n",
    "* Calculate an ensemble mean of all used models\n",
    "* Calculate and plot the seasonal mean of the ensemble mean\n",
    "\n",
    "**Questions**\n",
    "* How is the cloud phase and snowfall varying between 1985 and 2014?\n",
    "\n",
    "> **_NOTE:_** We answer questions related to the comparison of CMIP models to ERA5 in another [Jupyter Notebook](../CMIP6_ERA5_CloudSat/plt_seasonal_mean.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f939501",
   "metadata": {},
   "source": [
    "# 2. Data Wrangling <a id='data_wrangling'></a>\n",
    "\n",
    "This study will compare surface snowfall, ice, and liquid water content from the Coupled Model Intercomparison Project Phase 6 ([CMIP6](https://esgf-node.llnl.gov/projects/cmip6/)) climate models (accessed through [Pangeo](https://pangeo.io/)) to the European Centre for Medium-Range Weather Forecast Re-Analysis 5 ([ERA5](https://www.ecmwf.int/en/forecasts/datasets/reanalysis-datasets/era5)) data from **1985 to 2014**. We conduct statistical analysis at the annual and seasonal timescales to determine the biases in cloud phase and precipitation (liquid and solid) in the CMIP6 models and their potential connection between them. \n",
    "\n",
    "- Time period: 1985 to 2014\n",
    "- horizonal resolution: ~100km\n",
    "- time resolution: monthly atmospheric data (Amon, AERmon)\n",
    "- Variables:\n",
    "  \n",
    "| shortname     |             Long name                   |      Units    |  levels |\n",
    "| ------------- |:---------------------------------------:| -------------:|--------:|\n",
    "|  prsn         |    Snowfall Flux                        | [kg m-2 s-1]  | surface |\n",
    "| clw           |    Mass Fraction of Cloud Liquid Water  |  [kg kg-1]    |    ml   | \n",
    "|               |                                         | to calculate lwp use integral clw -dp/dg | |\n",
    "| cli           |    Mass Fraction of Cloud Ice           | [kg kg-1]     |    ml   |\n",
    "| tas           |    Near-Surface Air Temperature         |   [K]         | surface |\n",
    "| ta            |    Air Temperature                      |  [K]          |  plev   |\n",
    "| clivi         |    Ice Water Path                       | [kg m-2]      |         |\n",
    "| lwp           |    Liquid Water Path                    | [kg m-2]      |         |\n",
    "| pr            |    Precipitation                        | [kg m-2 s-1]  | surface |\n",
    "\n",
    "- CMIP6 models:\n",
    "\n",
    "| Institution                                            |     Model name    | Reference                                                     |\n",
    "| ------------------------------------------------------ |:-----------------:|--------------------------------------------------------------:|\n",
    "| [AS-RCEC](https://www.rcec.sinica.edu.tw/index_en.php) | TaiESM1           | [Lee et al. (2020)](https://doi.org/10.5194/gmd-13-3887-2020) |\n",
    "| [BCC](http://bcc.ncc-cma.net/)                         | BCC-CSM2-M        | [Wu et al. (2019)](https://doi.org/10.5194/gmd-12-1573-2019)  |\n",
    "| [CAMS](http://www.cma.gov.cn/en2014/)                  | CAMS-CSM1-0       |                                                               |\n",
    "| [CAS](http://english.iap.cas.cn/)                      | FGOALS-f3-L       | [Bian et al. (2020)](https://doi-org.ezproxy.uio.no/10.1080/16742834.2020.1778419) |\n",
    "| [CMCC](https://www.cmcc.it/)                           | CMCC-CM2-SR5      | [Cherchi et al. (2019)](https://doi-org.ezproxy.uio.no/10.1029/2018MS001369)|\n",
    "|                                                        | CMCC-CM2-HR4      | [Cherchi et al. (2019)](https://doi-org.ezproxy.uio.no/10.1029/2018MS001369)|\n",
    "|                                                        | CMCC-ESM2         | [CMCC website](https://www.cmcc.it/models/cmcc-esm-earth-system-model)    |\n",
    "| [EC-Earth-Consortium](http://www.ec-earth.org/)        | EC-Earth3-AerChem | [van Noije et al. (2021)](https://doi.org/10.5194/gmd-14-5637-2021)  |\n",
    "| [E3SM-Project](https://e3sm.org/)                      | E3SM-1-1          | [Golaz et al. (2019)](https://doi-org.ezproxy.uio.no/10.1029/2018MS001603); [Burrows et al. (2020)](https://doi-org.ezproxy.uio.no/10.1029/2019MS001766) Text S8|\n",
    "|                                                        | E3SM-1-1-ECA      | |\n",
    "| [MPI-M](https://mpimet.mpg.de/en/homepage)             | MPI-ESM1-2-HR     | [MÃ¼ller et al. (2018)](https://doi-org.ezproxy.uio.no/10.1029/2017MS001217)|\n",
    "| [MRI](https://www.mri-jma.go.jp/index_en.html)         | MRI-ESM2-0        | [Yukimoto et al. (2019)](https://doi.org/10.2151/jmsj.2019-051) |\n",
    "| [NCC](https://folk.uib.no/ngfhd/EarthClim/index.htm)   | NorESM2-MM        | [Seland et al. (2020)](https://doi.org/10.5194/gmd-13-6165-2020)|\n",
    "| [NOAA-GFDL](https://www.gfdl.noaa.gov/)                | GFDL-CM4          | [Held et al. (2019)](https://doi-org.ezproxy.uio.no/10.1029/2019MS001829) |\n",
    "|                                                        | GFDL-ESM4         | [Dunne et al. (2020)](https://doi-org.ezproxy.uio.no/10.1029/2019MS002015) |\n",
    "| [SNU](https://en.snu.ac.kr/index.html)                 | SAM0-UNICON       | [Park et al. (2019)](https://doi-org.ezproxy.uio.no/10.1175/JCLI-D-18-0796.1) |\n",
    "| [THU](https://www.tsinghua.edu.cn/en/)                 | CIESM             | [Lin et al. (2020)](https://doi-org.ezproxy.uio.no/10.1029/2019MS002036) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b693459",
   "metadata": {},
   "source": [
    "## Organize my data\n",
    "\n",
    "- Define a prefix for my project (you may need to adjust it for your own usage on your infrastructure).\n",
    "    - input folder where all the data used as input to my Jupyter Notebook is stored (and eventually shared)\n",
    "    - output folder where all the results to keep are stored\n",
    "    - tool folder where all the tools\n",
    "\n",
    "The ERA5 0.25deg data is located in the folder `/input/cmip6_hist/daily_means`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9f298f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mimi.uio.no\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import sys\n",
    "import socket\n",
    "hostname = socket.gethostname()\n",
    "\n",
    "abs_path = str(pathlib.Path(hostname).parent.absolute())\n",
    "WORKDIR = abs_path[:- (len(abs_path.split('/')[-2] + abs_path.split('/')[-1])+1)]\n",
    "\n",
    "\n",
    "if \"mimi\" in hostname:\n",
    "    print(hostname)\n",
    "    DATA_DIR = \"/scratch/franzihe/\"\n",
    "    FIG_DIR = \"/uio/kant/geo-metos-u1/franzihe/Documents/Figures/CMIP6/\"\n",
    "elif \"glefsekaldt\" in hostname: \n",
    "    DATA_DIR = \"/home/franzihe/Data/\"\n",
    "    FIG_DIR = \"/home/franzihe/Documents/Figures/CMIP6/\"\n",
    "\n",
    "INPUT_DATA_DIR = os.path.join(DATA_DIR, 'input')\n",
    "OUTPUT_DATA_DIR = os.path.join(DATA_DIR, 'output')\n",
    "UTILS_DIR = os.path.join(WORKDIR, 'utils')\n",
    "\n",
    "sys.path.append(UTILS_DIR)\n",
    "# make figure directory\n",
    "try:\n",
    "    os.mkdir(FIG_DIR)\n",
    "except OSError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c652dc0b",
   "metadata": {},
   "source": [
    "## Import python packages\n",
    "- `Python` environment requirements: file [requirements_globalsnow.txt](../../requirements_globalsnow.txt) \n",
    "- load `python` packages from [imports.py](../../utils/imports.py)\n",
    "- load `functions` from [functions.py](../../utils/functions.py)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4caa42d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.core.options.set_options at 0x7f9e685504f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# supress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # don't output warnings\n",
    "\n",
    "# import packages\n",
    "from imports import (xr, intake, cftime, xe, glob, np, cm, pd, fct,ccrs, cy, plt, da, gc, datetime, LogNorm)\n",
    "xr.set_options(display_style=\"html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb920800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d1aca48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Create dask cluster to work parallel in large datasets\n",
    "\n",
    "# from dask.distributed import Client\n",
    "# client = Client(n_workers=4, \n",
    "#                 threads_per_worker=2, \n",
    "#                 memory_limit='100GB',\n",
    "#                 processes=False)\n",
    "# client\n",
    "# chunks={'time' : 10,}\n",
    "# client "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35d34cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # These are the usual ipython objects, including this one you are creating\n",
    "# ipython_vars = ['In', 'Out', 'exit', 'quit', 'get_ipython', 'ipython_vars']\n",
    "\n",
    "# # Get a sorted list of the objects and their sizes\n",
    "# sorted([(x, sys.getsizeof(globals().get(x))) for x in dir() if not x.startswith('_') and x not in sys.modules and x not in ipython_vars], key=lambda x: x[1], reverse=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620cb825",
   "metadata": {},
   "source": [
    "## Open CMIP6 variables\n",
    "Get the data required for the analysis. Beforehand we downloaded the daily averaged data on single levels and model levels via."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99ceedce",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmip_in = os.path.join(INPUT_DATA_DIR, 'cmip6_hist/daily_means')\n",
    "cmip_out = os.path.join(OUTPUT_DATA_DIR, 'cmip6_hist/daily_means/common_grid')\n",
    "\n",
    "# make output data directory\n",
    "try:\n",
    "    os.mkdir(cmip_out)\n",
    "except OSError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "994d8804",
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_id = ['clw', 'cli', 'clivi', 'tas', 'prsn']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424276a6",
   "metadata": {},
   "source": [
    "At the moment we have downloaded the end of the historical simulations for CMIP6 models. We define start and end year to ensure to only extract the 4-year period between 2007 and 2010.\n",
    "\n",
    "$\\rightarrow$ Define a start and end year\n",
    "\n",
    "We will load all available models into one dictonary, which includes an xarray dataset with `xarray.open_mfdataset(file)` and select the time range [by name](https://xarray.pydata.org/en/stable/user-guide/indexing.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "598731dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source_id\n",
    "list_models = [\n",
    "               # 'MIR/C6', \n",
    "               'CESM2', \n",
    "            #    'CanESM5', \n",
    "            #    'AWI-ESM-1-1-LR', \n",
    "            #    'MPI-ESM1-2-LR', \n",
    "            # #    'UKESM1-0-LL', \n",
    "            # #    'HadGEM3-GC31-LL',\n",
    "            #    'CNRM-CM6-1',\n",
    "            #    'CNRM-ESM2-1',\n",
    "            #    'IPSL-CM6A-LR',\n",
    "            #    'IPSL-CM5A2-INCA'\n",
    "            ]\n",
    "\n",
    "## experiment\n",
    "experiment_id = ['historical']\n",
    "\n",
    "## time resolution\n",
    "t_res = ['day',]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20590565",
   "metadata": {},
   "source": [
    "## Search corresponding data\n",
    "Get the data required for the analysis. Define variables, models, experiment, and time resolution as defined in <a href=\"#data_wrangling\">2. Data Wrangling</a>\n",
    ". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4824d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "starty = 2006; endy = 2009\n",
    "year_range = range(starty, endy+1)\n",
    "\n",
    "dset_dict = dict()\n",
    "for model in list_models:\n",
    "    cmip_file_in = glob('{}/*{}_{}_{}*'.format(cmip_in, t_res[0], model, experiment_id[0]))\n",
    "    if len(cmip_file_in) != 0:\n",
    "        dset_dict[model] = xr.open_mfdataset(sorted(cmip_file_in), combine='nested', compat='override', use_cftime=True)\n",
    "        # select only years needed for analysis\n",
    "        dset_dict[model] = dset_dict[model].sel(time = dset_dict[model]['time'].dt.year.isin(year_range)).squeeze()\n",
    "        # shift longitude to be from -180 to 180\n",
    "        dset_dict[model] = dset_dict[model].assign_coords(lon=(((dset_dict[model]['lon'] + 180) % 360) - 180)).sortby('lon').sortby('time')\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2058c693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for model in list_models:\n",
    "#     cmip_file_in = glob('{}/*{}*45*'.format(cmip_in, model))\n",
    "#     ds_cmip = xr.open_mfdataset(cmip_file_in)\n",
    "\n",
    "#     ds_cmip = ds_cmip.sel(time = ds_cmip.time.dt.year.isin(year_range)).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f662c1a1",
   "metadata": {},
   "source": [
    "## Calendar\n",
    "Not all models in CMIP6 use the same calendar. Hence we double check the time axis. Later, when we regrid to the same horizontal resolution (<a href=\"#regrid_hz\">Regrid CMIP6 data</a>) we will assign the same calendars for each model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21f8541f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # metadata of the historical run:\n",
    "# _d2 = pd.Series([\"calendar\",\n",
    "#                  \"branch_time_in_parent\", #\"parent_activity_id\", \"parent_experiment_id\",\t\"parent_mip_era\",\n",
    "#                  \"parent_source_id\",#\"parent_sub_experiment_id\", \n",
    "#                  \"parent_time_units\",# \"parent_variant_label\"\n",
    "#                   ])\n",
    "# _d2 = pd.DataFrame(_d2).rename(columns={0:'index'})\n",
    "# for model in dset_dict.keys():\n",
    "#     _data = []\n",
    "#     _names =[]\n",
    "#     _data.append(dset_dict[model].time.to_index().calendar)\n",
    "#     for k, v in dset_dict[model].attrs.items():\n",
    "        \n",
    "#         if 'parent_time_units' in k or 'branch_time_in_parent' in k or 'parent_source_id' in k:\n",
    "#             _data.append(v)\n",
    "#             _names.append(k)\n",
    "#     _d2 = pd.concat([_d2,   pd.Series(_data)], axis=1)\n",
    "\n",
    "# _d2.dropna(how='all', axis=1, inplace=True)\n",
    "# _d2 = _d2.set_index('index')\n",
    "# _d2.columns = _d2.loc['parent_source_id']\n",
    "# _d2.drop('parent_source_id').T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1b8fba",
   "metadata": {},
   "source": [
    "## Show attributes and individual identifier\n",
    "... is going to be the reference model for the horizontal grid. The `xarray` datasets inside `dset_dict` can be extracted as any value in a Python dictionary.\n",
    "\n",
    "The dictonary key is the source_id from `list_models`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "671441bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for model in dset_dict.keys():\n",
    "#     print('Institution: {}, \\\n",
    "#           Model: {},   \\\n",
    "#           Nominal res: {},  \\\n",
    "#           lon x lat, level, top,: {}, \\\n",
    "#           tracking_id: {}'.format(dset_dict[model].attrs['institution_id'],\n",
    "#                                   dset_dict[model].attrs['source_id'], \n",
    "#                                   dset_dict[model].attrs['nominal_resolution'], \n",
    "#                                   dset_dict[model].attrs['source'],\n",
    "#                                   dset_dict[model].attrs['tracking_id']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a7bc1a",
   "metadata": {},
   "source": [
    "## Assign attributes to the variables\n",
    "\n",
    "We will assign the attributes to the variables as in ERA5 to make CMIP6 and ERA5 variables comperable.\n",
    "\n",
    "* [`cli`](http://clipc-services.ceda.ac.uk/dreq/u/dd916e3e2eca18cda5d9f81749d0c91c.html) and [`clw`](http://clipc-services.ceda.ac.uk/dreq/u/86b2b3318a73839edfafa9d46864aadc.html) in **kg kg-1** $\\rightarrow$ Multiply by **1000** to get **g kg-1**\n",
    "* [`clivi`](http://clipc-services.ceda.ac.uk/dreq/u/73c496f5669cc122cf1cddfe4df2a27a.html) and [`lwp`](http://clipc-services.ceda.ac.uk/dreq/u/e6b31a1928879fcd3c92fe7b592f070e.html) in **kg m-2** $\\rightarrow$ Multiply by **1000** to get **g m-2**\n",
    "* [`pr`](http://clipc-services.ceda.ac.uk/dreq/u/62f26742cf240c1b5169a5cd511196b6.html) and [`prsn`](http://clipc-services.ceda.ac.uk/dreq/u/051919eddec810e292c883205c944ceb.html) in **kg m-2 s-1** $\\rightarrow$ Multiply by **3600** to get **mm h-1**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a275d842",
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.utcnow()\n",
    "for model in dset_dict.keys():\n",
    "# \n",
    "    for var_id in dset_dict[model].keys():\n",
    "        if var_id == 'clivi' or var_id == 'clw' or var_id == 'cli':\n",
    "            dset_dict[model][var_id] = dset_dict[model][var_id]*1000\n",
    "        if var_id == 'cli':\n",
    "            dset_dict[model][var_id] = dset_dict[model][var_id].assign_attrs({'standard_name': 'mass_fraction_of_cloud_ice_in_air',\n",
    "    'long_name': 'Mass Fraction of Cloud Ice',\n",
    "    'comment': 'Includes both large-scale and convective cloud. This is calculated as the mass of cloud ice in the grid cell divided by the mass of air (including the water in all phases) in the grid cell. It includes precipitating hydrometeors ONLY if the precipitating hydrometeors affect the calculation of radiative transfer in model.',\n",
    "    'units': 'g kg-1',\n",
    "    'original_units': 'kg/kg',\n",
    "    'history': \"{}Z altered by F. Hellmuth: Converted units from 'kg kg-1' to 'g kg-1'. Interpolate data from hybrid-sigma levels to isobaric levels with geocat.comp.interpolation.interp_hybrid_to_pressure\".format(now.strftime(\"%d/%m/%Y %H:%M:%S\")),\n",
    "    'cell_methods': 'area: time: mean',\n",
    "    'cell_measures': 'area: areacella'})\n",
    "                \n",
    "        if var_id == 'clw':\n",
    "            dset_dict[model][var_id] = dset_dict[model][var_id].assign_attrs({'standard_name': 'mass_fraction_of_cloud_liquid_water_in_air',\n",
    "    'long_name': 'Mass Fraction of Cloud Liquid Water',\n",
    "    'comment': 'Includes both large-scale and convective cloud. Calculate as the mass of cloud liquid water in the grid cell divided by the mass of air (including the water in all phases) in the grid cells. Precipitating hydrometeors are included ONLY if the precipitating hydrometeors affect the calculation of radiative transfer in model.',\n",
    "    'units': 'g kg-1',\n",
    "    'original_units': 'kg/kg',\n",
    "    'history': \"{}Z altered by F. Hellmuth: Converted units from 'kg kg-1' to 'g kg-1'. Interpolate data from hybrid-sigma levels to isobaric levels with geocat.comp.interpolation.interp_hybrid_to_pressure\".format(now.strftime(\"%d/%m/%Y %H:%M:%S\")),\n",
    "    'cell_methods': 'area: time: mean',\n",
    "    'cell_measures': 'area: areacella'})\n",
    "            \n",
    "        if var_id == 'clivi':\n",
    "            dset_dict[model][var_id] = dset_dict[model][var_id].assign_attrs({'standard_name': 'atmosphere_mass_content_of_cloud_ice',\n",
    "    'long_name': 'Ice Water Path',\n",
    "    'comment': 'mass of ice water in the column divided by the area of the column (not just the area of the cloudy portion of the column). Includes precipitating frozen hydrometeors ONLY if the precipitating hydrometeor affects the calculation of radiative transfer in model.',\n",
    "    'units': 'g m-2',\n",
    "    'original_units': 'kg m-2',\n",
    "    'history': \"{}Z altered by F. Hellmuth: Converted units from 'kg m-2' to 'g m-2'.\".format(now.strftime(\"%d/%m/%Y %H:%M:%S\")),\n",
    "    'cell_methods': 'area: time: mean',\n",
    "    'cell_measures': 'area: areacella',})         \n",
    "        if var_id == 'prsn':\n",
    "            dset_dict[model][var_id] = dset_dict[model][var_id]*3600\n",
    "            dset_dict[model][var_id] = dset_dict[model][var_id].assign_attrs({'standard_name': 'snowfall_flux',\n",
    "    'long_name': 'Snowfall Flux',\n",
    "    'comment': 'At surface; includes precipitation of all forms of water in the solid phase',\n",
    "    'units': 'mm h-1',\n",
    "    'original_units': 'kg m-2 s-1',\n",
    "    'history': \"{}Z altered by F. Hellmuth: Converted units from 'kg m-2 s-1' to 'mm h-1'.\".format(now.strftime(\"%d/%m/%Y %H:%M:%S\")),\n",
    "    'cell_methods': 'area: time: mean',\n",
    "    'cell_measures': 'area: areacella'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931123a2",
   "metadata": {},
   "source": [
    " ## Interpolate from CMIP6 hybrid sigma-pressure levels to ERA5 isobaric pressure levels\n",
    "\n",
    "The vertical variables in the CMIP6 models are in hybrid sigma-pressure levels. Hence the vertical variable in the xarray datasets in `dset_dict` will be calculated by using the [GeoCAT-comb](https://geocat-comp.readthedocs.io/en/latest/index.html#) function to [interpolate data from hybrid-sigma levels to isobaric levels](https://geocat-comp.readthedocs.io/en/latest/user_api/generated/geocat.comp.interpolation.interp_hybrid_to_pressure.html#geocat.comp.interpolation.interp_hybrid_to_pressure).\n",
    "\n",
    "The GeoCAT-comb function takes the following input:\n",
    "* `data`:   Multidimensional data array, which holds hybrid-sigma levels and has a lev_dim coordinate.\n",
    "* `ps`:     A multi-dimensional array of surface pressures (Pa), same time/space shape as data. Not all variables include the surface pressure, hence we will search the `Pangeo.io` catalog to find the surface pressure associated with the model. \n",
    "* `hyam`:     One-dimensional arrays containing the hybrid A coefficients. Must have the same dimension size as the lev_dim dimension of data.\n",
    "* `hybm`:     One-dimensional arrays containing the hybrid B coefficients. Must have the same dimension size as the lev_dim dimension of data.\n",
    "* `p0`:       Scalar numeric value equal to surface reference pressure (Pa). Defaults to 100000 Pa.\n",
    "* `new_levels`: A one-dimensional array of output pressure levels (Pa). We will use the pressure of `air_temperature` (19 levels).\n",
    "\n",
    "\n",
    "$$ P(i,j,k) = hyam(k) p0 + hybm(k) ps(i,j)$$\n",
    "\n",
    "```\n",
    "import geocat\n",
    "\n",
    "geocat.comp.interpolation.interp_hybrid_to_pressure(data      =ds['variable'], \n",
    "                                                    ps        =ds['ps'], \n",
    "                                                    hyam      =ds['a'], \n",
    "                                                    hybm      =ds['b'], \n",
    "                                                    p0        =ds['p0'], \n",
    "                                                    new_levels=ds['plev'])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c1904aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_levels = np.array([100000., 97500., 95000., 92500., 90000., \n",
    "                       87500., 85000., 82500., 80000.,\n",
    "                       77500., 75000., 70000., \n",
    "                       65000., 60000., \n",
    "                       55000., 50000., \n",
    "                       45000., 40000., \n",
    "                       35000., 30000., \n",
    "                       25000., 22500., 20000., \n",
    "                       17500., 15000., 12500., 10000., \n",
    "                       7000., 5000., 3000., 2000., 1000., 700., 500., 300., 200., 100.], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65502d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename datasets with different naming convention for constant hyam\n",
    "for model in dset_dict.keys():\n",
    "    if ('a' in list(dset_dict[model].keys())) == True:\n",
    "        dset_dict[model] = dset_dict[model].rename({'a':'ap', 'a_bnds': 'ap_bnds'})\n",
    "    if model == 'IPSL-CM6A-LR':\n",
    "        dset_dict[model] = dset_dict[model].rename({'presnivs':'plev'})\n",
    "    if model == 'IPSL-CM5A2-INCA':\n",
    "        dset_dict[model] = dset_dict[model].rename({'lev':'plev'})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b34de05e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CESM2 lev, ap, ps, p0\n",
      "CESM2 variable on pressure levels\n"
     ]
    }
   ],
   "source": [
    "for model in dset_dict.keys():\n",
    "    for var_id in dset_dict[model].keys():#['clw', 'cli']:\n",
    "        if var_id == 'clw' or var_id == 'cli':\n",
    "            # Convert the model level to isobaric levels\n",
    "            #### ap, b, ps, p0\n",
    "            if ('ap' in list(dset_dict[model].keys())) == True and \\\n",
    "                ('ps' in list(dset_dict[model].keys())) == True and \\\n",
    "                ('p0' in list(dset_dict[model].keys())) == True:\n",
    "                if ('lev' in list(dset_dict[model][var_id].coords)) == True and \\\n",
    "                    ('lev' in list(dset_dict[model]['ap'].coords)) == True and \\\n",
    "                    ('lev' in list(dset_dict[model]['b'].coords)) == True:\n",
    "                        print(model, 'lev, ap, ps, p0')\n",
    "                        dset_dict[model][var_id] = gc.interpolation.interp_hybrid_to_pressure(data = dset_dict[model][var_id],\n",
    "                                                                                                        ps   = dset_dict[model]['ps'], \n",
    "                                                                                                        hyam = dset_dict[model]['ap'], \n",
    "                                                                                                        hybm = dset_dict[model]['b'], \n",
    "                                                                                                        p0   = dset_dict[model]['p0'], \n",
    "                                                                                                        new_levels=new_levels,\n",
    "                                                                                                        lev_dim='lev')\n",
    "\n",
    "                \n",
    "                if ('plev' in list(dset_dict[model][var_id].coords)) == True:\n",
    "                    print(model, 'variable on pressure levels', )\n",
    "                # if ('lev' in list(dset_dict[model][var_id].coords)) == True and \\\n",
    "                #     ('lev' in list(dset_dict[model]['ap'].coords)) == False and \\\n",
    "                #     ('lev' in list(dset_dict[model]['b'].coords)) == False:\n",
    "                #         print(model, 'variable on pressure levels', 'lev, ap, ps,')\n",
    "            # Convert the model level to isobaric levels\n",
    "            #### ap, b, p0\n",
    "            if ('ap' in list(dset_dict[model].keys())) == True and \\\n",
    "                ('ps' in list(dset_dict[model].keys())) == True and \\\n",
    "                ('p0' in list(dset_dict[model].keys())) == False:\n",
    "                if ('lev' in list(dset_dict[model][var_id].coords)) == True and \\\n",
    "                    ('lev' in list(dset_dict[model]['ap'].coords)) == True and \\\n",
    "                    ('lev' in list(dset_dict[model]['b'].coords)) == True:\n",
    "                        print(model, 'lev, ap, ps,')\n",
    "                        dset_dict[model][var_id] = gc.interpolation.interp_hybrid_to_pressure(data = dset_dict[model][var_id],\n",
    "                                                                                                        ps   = dset_dict[model]['ps'], \n",
    "                                                                                                        hyam = dset_dict[model]['ap'], \n",
    "                                                                                                        hybm = dset_dict[model]['b'], \n",
    "                                                                                                        new_levels=new_levels,\n",
    "                                                                                                        lev_dim='lev')\n",
    "\n",
    "                \n",
    "                if ('plev' in list(dset_dict[model][var_id].coords)) == True:\n",
    "                    print(model, 'variable on pressure levels', )\n",
    "                \n",
    "            if ('b' in list(dset_dict[model].keys())) == True and \\\n",
    "                ('orog' in list(dset_dict[model].keys())) == True:\n",
    "                if ('lev' in list(dset_dict[model][var_id].coords)) == True and \\\n",
    "                    ('lev' in list(dset_dict[model]['pfull'].coords)) == True:\n",
    "                        print(model, 'hybrid height coordinate')\n",
    "                \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ddb32978",
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_season=(dset_dict[model]['prsn'].groupby('time.season').mean('time',skipna=True))#.plot(x='lon', y='lat', col='season', col_wrap=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4bcdcd7e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt_seasonal_NH_SH' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plt_seasonal_NH_SH(sf_season,np\u001b[39m.\u001b[39marange(\u001b[39m0\u001b[39m,\u001b[39m0.3\u001b[39m,\u001b[39m0.01\u001b[39m),\u001b[39m'\u001b[39m\u001b[39mcbar_label\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mplt_title\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt_seasonal_NH_SH' is not defined"
     ]
    }
   ],
   "source": [
    "plt_seasonal_NH_SH(sf_season,np.arange(0,0.3,0.01),'cbar_label','plt_title')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8057c788",
   "metadata": {},
   "source": [
    "## Calculate liquid water path from content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f2187336",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in dset_dict.keys():\n",
    "    if ('plev' in list(dset_dict[model]['clw'].coords)) == True:\n",
    "        _lwp = xr.DataArray(data=da.full(shape=dset_dict[model]['clw'].shape,fill_value=np.nan),\n",
    "                            dims=dset_dict[model]['clw'].dims,\n",
    "                            coords=dset_dict[model]['clw'].coords)\n",
    "\n",
    "        dset_dict[model] = dset_dict[model].reindex(plev=dset_dict[model]['plev'][::-1])\n",
    "        for lev2, lev, i in zip(dset_dict[model]['plev'].values, dset_dict[model]['plev'][1:].values, range(1,len(dset_dict[model]['plev']))):\n",
    "            \n",
    "            # calculate pressure difference between two levels\n",
    "            dp = dset_dict[model]['plev'].sel(plev=slice(lev2,lev)).diff(dim='plev')\n",
    "            # calculate liquid water path in each layer\n",
    "            _lwp[:,i,:,:] = (dset_dict[model]['clw'].sel(plev=slice(lev2,lev)).diff(dim='plev') * 9.81 * dp)[:,0,:,:]\n",
    "            # sum over all layers to get the liquid water path in the atmospheric column\n",
    "            dset_dict[model]['lwp'] = _lwp.sum(dim='plev', skipna=True)\n",
    "            # assign attributes to data array\n",
    "            dset_dict[model]['lwp'] = dset_dict[model]['lwp'].assign_attrs(dset_dict[model]['clw'].attrs)\n",
    "            dset_dict[model]['lwp'] = dset_dict[model]['lwp'].assign_attrs({'long_name':'Liquid Water Path', \n",
    "                                                                            'mipTable':'', 'out_name': 'lwp',\n",
    "                                                                            'standard_name': 'atmosphere_mass_content_of_cloud_liquid_water',\n",
    "                                                                            'title': 'Liquid Water Path',\n",
    "                                                                            'variable_id': 'lwp', 'original_units': 'kg/kg',\n",
    "                                                                            'history': \"{}Z altered by F. Hellmuth: Converted units from 'kg kg-1' to 'g kg-1'. Interpolate data from hybrid-sigma levels to isobaric levels with geocat.comp.interpolation.interp_hybrid_to_pressure. Calculate lwp with hydrostatic equation.\".format(now.strftime(\"%d/%m/%Y %H:%M:%S\"))})\n",
    "        # when ice water path does not exist\n",
    "        if ('clivi' in list(dset_dict[model].keys())) == False:\n",
    "            # print(model)\n",
    "            _iwp = xr.DataArray(data=da.full(shape=dset_dict[model]['cli'].shape, fill_value=np.nan,),\n",
    "                                dims=dset_dict[model]['cli'].dims,\n",
    "                                coords=dset_dict[model]['cli'].coords)\n",
    "            dset_dict[model] = dset_dict[model].reindex(plev=dset_dict[model]['plev'][::-1])\n",
    "            for lev2, lev, i in zip(dset_dict[model]['plev'].values, dset_dict[model]['plev'][1:].values, range(1,len(dset_dict[model]['plev']))):\n",
    "                \n",
    "                # calculate pressure difference between two levels\n",
    "                dp = dset_dict[model]['plev'].sel(plev=slice(lev2,lev)).diff(dim='plev')\n",
    "                # calculate Ice water path in each layer\n",
    "                _iwp[:,i,:,:] = (dset_dict[model]['cli'].sel(plev=slice(lev2,lev)).diff(dim='plev') * 9.81 * dp)[:,0,:,:]\n",
    "                # sum over all layers to get the Ice water path in the atmospheric column\n",
    "                dset_dict[model]['clivi'] = _iwp.sum(dim='plev', skipna=True)\n",
    "                # assign attributes to data array\n",
    "                dset_dict[model]['clivi'] = dset_dict[model]['clivi'].assign_attrs(dset_dict[model]['cli'].attrs)\n",
    "                dset_dict[model]['clivi'] = dset_dict[model]['clivi'].assign_attrs({'long_name':'Ice Water Path', \n",
    "                                                                                    'mipTable':'', 'out_name': 'clivi',\n",
    "                                                                                    'standard_name': 'atmosphere_mass_content_of_cloud_ice',\n",
    "                                                                                    'title': 'Ice Water Path',\n",
    "                                                                                    'variable_id': 'clivi', 'original_units': 'kg/kg',\n",
    "                                                                                    'history': \"{}Z altered by F. Hellmuth: Converted units from 'kg kg-1' to 'g kg-1'. Interpolate data from hybrid-sigma levels to isobaric levels with geocat.comp.interpolation.interp_hybrid_to_pressure. Calculate clivi with hydrostatic equation.\".format(now.strftime(\"%d/%m/%Y %H:%M:%S\"))})\n",
    "        \n",
    "            \n",
    "    \n",
    "    if ('pfull' in list(dset_dict[model].keys())) == True:\n",
    "        _lwp = xr.DataArray(data=da.full(shape=dset_dict[model]['clw'].shape,fill_value=np.nan),\n",
    "                            dims=dset_dict[model]['clw'].dims,\n",
    "                            coords=dset_dict[model]['clw'].coords)\n",
    "        dset_dict[model] = dset_dict[model].reindex(lev=dset_dict[model]['lev'][::-1])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b999907",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4b6f845b",
   "metadata": {},
   "source": [
    "# Set values between -45S and 45N to nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "942dee84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set3D_lat_values_nan(array, upper_lat, lower_lat):\n",
    "    l_lat = array['lat'].loc[array['lat'] == (array['lat'].sel(lat=slice(lower_lat,upper_lat))).min()][0].values\n",
    "    u_lat = array['lat'].loc[array['lat'] == (array['lat'].sel(lat=slice(lower_lat,upper_lat))).max()][0].values\n",
    "    array[:,array['lat'].where(array['lat']==l_lat).argmin('lat').values: array['lat'].where(array['lat']==u_lat).argmin('lat').values,:] = xr.DataArray(data=da.full(shape=(array[:,array['lat'].where(array['lat']==l_lat).argmin('lat').values: array['lat'].where(array['lat']==u_lat).argmin('lat').values,:]).shape,\n",
    "                                                                                                                                                                      fill_value=np.nan),\n",
    "                                                                                                                                                         dims=(array[:,array['lat'].where(array['lat']==l_lat).argmin('lat').values: array['lat'].where(array['lat']==u_lat).argmin('lat').values,:]).dims,\n",
    "                                                                                                                                                         coords=(array[:,array['lat'].where(array['lat']==l_lat).argmin('lat').values: array['lat'].where(array['lat']==u_lat).argmin('lat').values,:]).coords)\n",
    "    return array\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ac998bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in dset_dict.keys():\n",
    "    \n",
    "    dset_dict[model]['prsn'] = set3D_lat_values_nan(dset_dict[model]['prsn'], 45, -45)\n",
    "    dset_dict[model]['lwp']  = set3D_lat_values_nan(dset_dict[model]['lwp'], 45, -45)\n",
    "    dset_dict[model]['clivi']= set3D_lat_values_nan(dset_dict[model]['clivi'], 45, -45)\n",
    "    dset_dict[model]['tas']  = set3D_lat_values_nan(dset_dict[model]['tas'], 45, -45)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e27c9f",
   "metadata": {},
   "source": [
    "## Reduce dataset to only include 3D values (time,lat,lon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5ce95656",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in dset_dict.keys():\n",
    "    if 'plev' in list(dset_dict[model].dims) and 'lev' in list(dset_dict[model].dims):\n",
    "        # print('drop: plev, lev', list(dset_dict[model].dims))\n",
    "        dset_dict[model] = dset_dict[model].drop_dims(('lev', 'plev'))\n",
    "        try: \n",
    "            dset_dict[model] = dset_dict[model].drop_dims(('nbnd'))\n",
    "        except ValueError:\n",
    "            try:\n",
    "                dset_dict[model] = dset_dict[model].drop_dims(('bnds'))\n",
    "            except ValueError:\n",
    "                try: \n",
    "                    dset_dict[model] = dset_dict[model].drop_dims(('axis_nbounds'))\n",
    "                except ValueError:\n",
    "                    print('model does not contain')\n",
    "                \n",
    "    if 'plev' in list(dset_dict[model].dims) and 'klevp1' in list(dset_dict[model].dims):\n",
    "        # print('drop: plev, kplev1', list(dset_dict[model].dims))\n",
    "        dset_dict[model] = dset_dict[model].drop_dims(('plev', 'klevp1'))\n",
    "        try: \n",
    "            dset_dict[model] = dset_dict[model].drop_dims(('nbnd'))\n",
    "        except ValueError:\n",
    "            try:\n",
    "                dset_dict[model] = dset_dict[model].drop_dims(('bnds'))\n",
    "            except ValueError:\n",
    "                try: \n",
    "                    dset_dict[model] = dset_dict[model].drop_dims(('axis_nbounds'))\n",
    "                except ValueError:\n",
    "                    print('model does not contain')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6f7c5f",
   "metadata": {},
   "source": [
    "## Create files with variables needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f856d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dset_dict[model]['clivi'].sel(time=slice(str(year)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1f07ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for model in dset_dict.keys():\n",
    "#     for var in ['prsn', 'tas', 'clivi', 'lwp',]:\n",
    "#         for year in year_range:\n",
    "#             print('Writing files: var: {}, year: {}, model: {}'.format(var, year, model))\n",
    "#             (dset_dict[model][var].sel(time=slice(str(year)), lat=slice(-90,-45))).to_netcdf('{}/{}_{}_-90_-45_historical_{}_gn_{}0101-{}1231.nc'.format(cmip_in,var, model, dset_dict[model].attrs['variant_label'],year, year))\n",
    "#             (dset_dict[model][var].sel(time=slice(str(year)), lat=slice(45,90))).to_netcdf('{}/{}_{}_45_90_historical_{}_gn_{}0101-{}1231.nc'.format(cmip_in,var, model, dset_dict[model].attrs['variant_label'],year, year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a906d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # These are the usual ipython objects, including this one you are creating\n",
    "# ipython_vars = ['In', 'Out', 'exit', 'quit', 'get_ipython', 'ipython_vars']\n",
    "\n",
    "# # Get a sorted list of the objects and their sizes\n",
    "# sorted([(x, sys.getsizeof(globals().get(x))) for x in dir() if not x.startswith('_') and x not in sys.modules and x not in ipython_vars], key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6104a157",
   "metadata": {},
   "source": [
    "## Statistics\n",
    "For variables:\n",
    "- Snowfall [sf]\n",
    "- Total column cloud liquid, supercooled liqid, and rain water [tclslrw]\n",
    "- Total column cloud ice, snow water [tcisw]\n",
    "- 2m-Temperature [2t]\n",
    "\n",
    "1. Find where liquid water path is $\\ge$ 5 g m-2 \n",
    "2. Find where snowfall is $\\ge$ 0.01mm h-1\n",
    "3. Find where 2m-temperature $\\le$ 0 $^o$ C "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "14e83166",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_precip_cloud(dset):\n",
    "    # 1. find where LWP >=5 gm-2\n",
    "    sf = dset['prsn'].where(dset['lwp']>=5)\n",
    "    lwp = dset['lwp'].where(dset['lwp']>=5)\n",
    "    iwp = dset['clivi'].where(dset['lwp']>=5)\n",
    "    t2 = dset['tas'].where(dset['lwp']>=5)\n",
    "    \n",
    "    print(1,'sf', sf.min(skipna=True).values, sf.max(skipna=True).values, 'lwp', lwp.min(skipna=True).values, lwp.max(skipna=True).values, 't2', t2.min(skipna=True).values, t2.max(skipna=True).values)\n",
    "    # 2. find where snowfall >= 0.01mmh-1\n",
    "    unit_sf = dset['prsn']\n",
    "    sf = sf.where(unit_sf>=0.01)\n",
    "    lwp = lwp.where(unit_sf>=0.01)\n",
    "    iwp = iwp.where(unit_sf>=0.01)\n",
    "    t2 = t2.where(unit_sf>=0.01)\n",
    "    print(2,'sf', sf.min(skipna=True).values, sf.max(skipna=True).values, 'lwp', lwp.min(skipna=True).values, lwp.max(skipna=True).values, 't2', t2.min(skipna=True).values, t2.max(skipna=True).values)\n",
    "    \n",
    "    # 3. find where 2m-temperature <= 0C\n",
    "    sf = sf.where(dset['tas']<=273.15)\n",
    "    lwp = lwp.where(dset['tas']<=273.15)\n",
    "    iwp = iwp.where(dset['tas']<=273.15)\n",
    "    t2 = t2.where(dset['tas']<=273.15)\n",
    "    print(3,'sf', sf.min(skipna=True).values, sf.max(skipna=True).values, 'lwp', lwp.min(skipna=True).values, lwp.max(skipna=True).values, 't2', t2.min(skipna=True).values, t2.max(skipna=True).values)\n",
    "    \n",
    "    sf_count = sf.groupby('time.season').count(dim='time',keep_attrs=True)\n",
    "    lwp_count = lwp.groupby('time.season').count(dim='time',keep_attrs=True)\n",
    "    iwp_count = iwp.groupby('time.season').count(dim='time', keep_attrs=True)\n",
    "    t2_count = t2.groupby('time.season').count(dim='time', keep_attrs=True)\n",
    "    \n",
    "    return(sf_count, sf, iwp, lwp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cecb2105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 sf 0.0 4.6741643 lwp 5.000000479185868 29978.69404403374 t2 212.5522 273.6683\n"
     ]
    }
   ],
   "source": [
    "dset_dict[model]['lcc_count'], dset_dict[model]['sf_lcc'], dset_dict[model]['iwp_lcc'], dset_dict[model]['lwp_lcc'] =find_precip_cloud(dset_dict[model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07929c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def plt_seasonal_NH_SH(variable,levels,cbar_label,plt_title):\n",
    "\n",
    "    f, axsm = plt.subplots(nrows=2,ncols=4,figsize =[10,7], subplot_kw={'projection': ccrs.NorthPolarStereo(central_longitude=0.0,globe=None)})\n",
    "\n",
    "    coast = cy.feature.NaturalEarthFeature(category='physical', scale='110m',\n",
    "                            facecolor='none', name='coastline')\n",
    "    \n",
    "    for ax, season in zip(axsm.flatten()[:4], variable.season):\n",
    "        # ax.add_feature(cy.feature.COASTLINE, alpha=0.5)\n",
    "        ax.add_feature(coast,alpha=0.5)\n",
    "        ax.set_extent([-180, 180, 90, 45], ccrs.PlateCarree())\n",
    "        gl = ax.gridlines(draw_labels=True)\n",
    "        gl.top_labels   = False\n",
    "        gl.right_labels = False\n",
    "        variable.sel(season=season, lat=slice(45,90)).plot(ax=ax, transform=ccrs.PlateCarree(), extend='max', add_colorbar=False,\n",
    "                            cmap=cm.hawaii_r, levels=levels)\n",
    "        ax.set(title ='season = {}'.format(season.values))\n",
    "\n",
    "    for ax, i, season in zip(axsm.flatten()[4:], np.arange(5,9), variable.season):\n",
    "        ax.remove()\n",
    "        ax = f.add_subplot(2,4,i, projection=ccrs.SouthPolarStereo(central_longitude=0.0, globe=None))\n",
    "        # ax.add_feature(cy.feature.COASTLINE, alpha=0.5)\n",
    "        ax.add_feature(coast,alpha=0.5)\n",
    "        ax.set_extent([-180, 180, -90, -45], ccrs.PlateCarree())\n",
    "        gl = ax.gridlines(draw_labels=True)\n",
    "        gl.top_labels   = False\n",
    "        gl.right_labels = False\n",
    "        cf = variable.sel(season=season, lat=slice(-90,-45)).plot(ax=ax, transform=ccrs.PlateCarree(), extend='max', add_colorbar=False,\n",
    "                            cmap=cm.hawaii_r, levels=levels)\n",
    "        ax.set(title ='season = {}'.format(season.values))\n",
    "\n",
    "    cbaxes = f.add_axes([1.0125, 0.025, 0.025, 0.9])\n",
    "    cbar = plt.colorbar(cf, cax=cbaxes, shrink=0.5,extend='max', orientation='vertical', label=cbar_label)\n",
    "    f.suptitle(plt_title, fontweight=\"bold\");\n",
    "    plt.tight_layout(pad=0., w_pad=0., h_pad=0.)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbbce34",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Cummulative snowfall days\n",
    "figname = '{}_cum_sf_days_season_mean_{}_{}.png'.format(model,starty, endy)\n",
    "plt_seasonal_NH_SH(ds_cmip['lcc_count'].where(ds_cmip['lcc_count']>0.), levels=np.arange(0,350,10), cbar_label='Cummulative snowfall days', plt_title='ERA5 ({} - {})'.format(starty,endy))\n",
    "plt.savefig(FIG_DIR + figname, format = 'png', bbox_inches = 'tight', transparent = False)\n",
    "\n",
    "\n",
    "# Seasonal precipictation efficency ICE\n",
    "figname = '{}_ice_precip_eff_season_mean_{}_{}.png'.format(model,starty, endy)\n",
    "sf_iwp = ((ds_cmip['sf_lcc']/(ds_cmip['iwp_lcc'])*1000).groupby('time.season').mean('time', keep_attrs=True, skipna=True)) # to get unitless (g/g) from (kg/g)\n",
    "plt_seasonal_NH_SH(sf_iwp,np.arange(0, 2.6,0.1),cbar_label='Mean seasonal ice precip. efficency',plt_title='ERA5 Snowfall/IWP ({} - {})'.format(starty,endy))\n",
    "plt.savefig(FIG_DIR + figname, format = 'png', bbox_inches = 'tight', transparent = False)\n",
    "\n",
    "# seasonal precip efficeny ICE+LIQUID\n",
    "figname = '{}_ice_liquid_precip_eff_season_mean_{}_{}.png'.format(model,starty, endy)\n",
    "sf_iwp_lwp = ((ds_cmip['sf_lcc']/((ds_cmip['lwp_lcc']+ds_cmip['iwp_lcc']))*1000).groupby('time.season').mean('time', keep_attrs=True, skipna=True))\n",
    "plt_seasonal_NH_SH(sf_iwp_lwp, np.arange(0, 2.6, 0.1),cbar_label='Mean seasonal precip. efficency', plt_title='ERA Snowfall/(IWP + LWP) ({} - {})'.format(starty,endy))\n",
    "plt.savefig(FIG_DIR + figname, format = 'png', bbox_inches = 'tight', transparent = False)\n",
    "\n",
    "# seasonal liquid water path\n",
    "figname = '{}_lwp_season_mean_{}_{}.png'.format(model,starty, endy)\n",
    "lwp_season = ds_cmip['lwp_lcc'].groupby('time.season').mean(dim='time',keep_attrs=True, skipna=True)\n",
    "plt_seasonal_NH_SH(lwp_season, np.arange(0, 310, 10),cbar_label='{} ({})'.format(lwp_season.attrs['long_name'], lwp_season.attrs['units']),plt_title='ERA5 {} ({} - {})'.format(lwp_season.attrs['long_name'], starty,endy))\n",
    "plt.savefig(FIG_DIR + figname, format = 'png', bbox_inches = 'tight', transparent = False)\n",
    "\n",
    "# mixed phase path fraction (ICE)\n",
    "figname = '{}_iwp_twp_season_mean_{}_{}.png'.format(model,starty, endy)\n",
    "iwp_twp = ((ds_cmip['iwp_lcc']/(ds_cmip['lwp_lcc']+ds_cmip['iwp_lcc'])).groupby('time.season').mean('time', keep_attrs=True, skipna=True))\n",
    "plt_seasonal_NH_SH(iwp_twp, np.arange(0, 1.05, .05),cbar_label='Mixed phase path fraction', plt_title='ERA IWP/(IWP + LWP) ({} - {})'.format(starty,endy))\n",
    "plt.savefig(FIG_DIR + figname, format = 'png', bbox_inches = 'tight', transparent = False)\n",
    "\n",
    "# mixed phase path fraction (LIQUID)\n",
    "figname = '{}_lwp_twp_season_mean_{}_{}.png'.format(model,starty, endy)\n",
    "lwp_twp = ((ds_cmip['lwp_lcc']/(ds_cmip['lwp_lcc']+ds_cmip['iwp_lcc'])).groupby('time.season').mean('time', keep_attrs=True, skipna=True))\n",
    "plt_seasonal_NH_SH(lwp_twp, np.arange(0, 1.05, .05),cbar_label='Mixed phase path fraction', plt_title='ERA LWP/(IWP + LWP) ({} - {})'.format(starty,endy))\n",
    "plt.savefig(FIG_DIR + figname, format = 'png', bbox_inches = 'tight', transparent = False)\n",
    "\n",
    "\n",
    "figname = '{}_sf_iwp_twp_season_mean_{}_{}.png'.format(model,starty, endy)\n",
    "sf_iwp_twp = (ds_cmip['sf_lcc']*(ds_cmip['iwp_lcc']/(ds_cmip['lwp_lcc']+ds_cmip['iwp_lcc']))).groupby('time.season').mean('time', keep_attrs=True, skipna=True)\n",
    "plt_seasonal_NH_SH(sf_iwp_twp, np.arange(0,0.175, 0.025),cbar_label='Mean seasonal snowfall * Mixed phase path fraction (mm)', plt_title='ERA snowfall * IWP/TWP ({} - {})'.format(starty,endy))\n",
    "plt.savefig(FIG_DIR + figname, format = 'png', bbox_inches = 'tight', transparent = False)\n",
    "\n",
    "figname = '{}_sf_lwp_twp_season_mean_{}_{}.png'.format(model,starty, endy)\n",
    "sf_lwp_twp = (ds_cmip['sf_lcc']*(ds_cmip['lwp_lcc']/(ds_cmip['lwp_lcc']+ds_cmip['iwp_lcc']))).groupby('time.season').mean('time', keep_attrs=True, skipna=True)\n",
    "plt_seasonal_NH_SH(sf_lwp_twp, np.arange(0,0.175, 0.025),cbar_label='Mean seasonal snowfall * Mixed phase path fraction (mm)', plt_title='ERA snowfall * LWP/TWP ({} - {})'.format(starty,endy))\n",
    "plt.savefig(FIG_DIR + figname, format = 'png', bbox_inches = 'tight', transparent = False)\n",
    "\n",
    "figname = '{}_mixed_phase_precip_eff_season_mean_{}_{}.png'.format(model,starty, endy)\n",
    "plt_seasonal_NH_SH((sf_iwp_twp/sf_lwp_twp), np.arange(0, 5.5, 0.5),cbar_label='precip. efficiency', plt_title='ERA seasonal mean mixed-phase ({} - {})'.format(starty,endy))\n",
    "plt.savefig(FIG_DIR + figname, format = 'png', bbox_inches = 'tight', transparent = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28937f94",
   "metadata": {},
   "source": [
    "## Calculating bin and bin sizes\n",
    "https://www.statisticshowto.com/choose-bin-sizes-statistics/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63faf1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_seasonal_2dhist_wp_sf(x_value, y_value, plt_title, xlabel, ylabel):\n",
    "    f, axsm = plt.subplots(nrows=2,ncols=4,figsize =[10,5], sharex=True, sharey=True)\n",
    "    cmap = cm.batlow\n",
    "    # levels = np.arange(0.1,65000,5000)\n",
    "    # norm = BoundaryNorm(levels, ncolors=cmap.N, )\n",
    "    norm = LogNorm(vmin=1, vmax=50000)\n",
    "\n",
    "\n",
    "\n",
    "    for ax, season in zip(axsm.flatten()[:4], x_value.season):\n",
    "        Z, xedges, yedges = np.histogram2d((x_value.where(x_value['lat'] >=45).sel(season=season).values.flatten()), \n",
    "                                        (y_value.where(y_value['lat'] >=45).sel(season=season).values.flatten()), \n",
    "                                        bins=[40, 40], \n",
    "                                        range=[[0,4],[0, 4]])   \n",
    "\n",
    "        im = ax.pcolormesh(xedges, yedges, Z.transpose(),cmap=cmap,norm=norm,)\n",
    "        # cbar = f.colorbar(im, ax=ax,)\n",
    "        ax.set(title =r'lat$\\geq 45^\\circ$N; season = {}'.format(season.values))\n",
    "        ax.grid()\n",
    "        \n",
    "        _corr = xr.corr(x_value.where(x_value['lat'] >=45).sel(season=season), y_value.where(y_value['lat'] >=45).sel(season=season))\n",
    "        props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "        ax.text(0.55, 0.95, 'Corr: {}'.format(np.round(_corr,3).values), transform=ax.transAxes, fontsize=11,\n",
    "        verticalalignment='top', bbox=props)\n",
    "        \n",
    "        \n",
    "\n",
    "    for ax, season in zip(axsm.flatten()[4:], x_value.season):\n",
    "        Z, xedges, yedges = np.histogram2d((x_value.where(x_value['lat'] <=-45).sel(season=season).values.flatten()), \n",
    "                                        (y_value.where(y_value['lat'] <=-45).sel(season=season).values.flatten()), \n",
    "                                        bins=[40, 40], \n",
    "                                        range=[[0,4],[0, 4]])   \n",
    "\n",
    "        im = ax.pcolormesh(xedges, yedges, Z.transpose(), cmap=cmap,norm=norm,)\n",
    "        # cbar = f.colorbar(im, ax=ax, )\n",
    "        ax.set(title =r'lat$\\leq-45^\\circ$S; season = {}'.format(season.values))\n",
    "        \n",
    "        ax.set_xlabel('{} ({})'.format(xlabel,x_value.attrs['units']))\n",
    "        ax.grid()\n",
    "        \n",
    "        _corr = xr.corr(x_value.where(x_value['lat'] <=-45).sel(season=season), y_value.where(y_value['lat'] <=-45).sel(season=season))\n",
    "        props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "        ax.text(0.55, 0.95, 'Corr: {}'.format(np.round(_corr,3).values), transform=ax.transAxes, fontsize=11,\n",
    "        verticalalignment='top', bbox=props)\n",
    "        \n",
    "    axsm.flatten()[0].set_ylabel('{} ({})'.format(ylabel, y_value.attrs['units']))\n",
    "    axsm.flatten()[4].set_ylabel('{} ({})'.format(ylabel, y_value.attrs['units']))\n",
    "\n",
    "\n",
    "    cbaxes = f.add_axes([1.0125, 0.025, 0.025, 0.9])\n",
    "    cbar = plt.colorbar(im, cax=cbaxes, shrink=0.5, orientation='vertical', label='Frequency')\n",
    "    f.suptitle(plt_title, fontweight=\"bold\");\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    plt.tight_layout(pad=0.5, w_pad=0.5, h_pad=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb5c0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lwp = ds_cmip['lwp_lcc']/1000\n",
    "lwp.attrs = {'units': 'kg m-2', 'long_name': 'Total column cloud liquid water'}\n",
    "iwp = ds_cmip['iwp_lcc']/1000\n",
    "iwp.attrs = {'units': 'kg m-2', 'long_name': 'Total column cloud ice and snow water'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9065b782",
   "metadata": {},
   "outputs": [],
   "source": [
    "_lwp = lwp.groupby('time.season').mean(('time', ), keep_attrs=True, skipna=True)\n",
    "_iwp = iwp.groupby('time.season').mean(('time', ), keep_attrs=True, skipna=True)\n",
    "_sf = ds_cmip['sf_lcc'].groupby('time.season').mean(('time', ), keep_attrs=True, skipna=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1075dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# precip efficency from ice\n",
    "figname = '{}_2dhist_iwp_sf_season_mean_{}_{}.png'.format(model,starty, endy)\n",
    "plt_seasonal_2dhist_wp_sf(_iwp, _sf, 'ERA5 ({} - {}) Mean seasonal ice precip. efficency'.format(starty,endy), 'Ice Water Path', 'Snowfall')\n",
    "plt.savefig(FIG_DIR + figname, format = 'png', bbox_inches = 'tight', transparent = False)\n",
    "\n",
    "# precip efficency from liquid\n",
    "figname = '{}_2dhist_lwp_sf_season_mean_{}_{}.png'.format(model,starty, endy)\n",
    "plt_seasonal_2dhist_wp_sf(_lwp, _sf, 'ERA5 ({} - {}) Mean seasonal liquid precip. efficency'.format(starty,endy), 'Liquid Water Path', 'Snowfall')\n",
    "plt.savefig(FIG_DIR + figname, format = 'png', bbox_inches = 'tight', transparent = False)\n",
    "\n",
    "# precip efficency from mixed-phase clouds\n",
    "figname = '{}_2dhist_lwp_iwp_sf_season_mean_{}_{}.png'.format(model,starty, endy)\n",
    "plt_seasonal_2dhist_wp_sf((_iwp+_lwp), _sf, 'ERA5 ({} - {}) Mean seasonal ice+liquid precip. efficency'.format(starty,endy), 'Liquid + Ice Water Path', 'Snowfall')\n",
    "plt.savefig(FIG_DIR + figname, format = 'png', bbox_inches = 'tight', transparent = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dff2f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6142d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c4a75b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0daff25d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023ed889",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be82ac06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174e56d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ea8d6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5652151e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1900635d",
   "metadata": {},
   "source": [
    "## Open CMIP6 variables\n",
    "... by using `intake` from [pangeo.io](https://gallery.pangeo.io/repos/pangeo-data/pangeo-tutorial-gallery/intake.html), specifically `intake-esm`.\n",
    "\n",
    "An example on [Loading an ESM collection](https://pangeo-data.github.io/pangeo-cmip6-cloud/accessing_data.html#loading-an-esm-collection) and [searching for datasets](https://pangeo-data.github.io/pangeo-cmip6-cloud/accessing_data.html#searching-for-datasets) can also be found on the [Pangeo / ESGF Cloud Data Working Group documentation](https://pangeo-data.github.io/pangeo-cmip6-cloud/accessing_data.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69b9422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat_url = \"https://storage.googleapis.com/cmip6/pangeo-cmip6.json\"\n",
    "# col = intake.open_esm_datastore(cat_url)\n",
    "# col"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320dc599",
   "metadata": {},
   "source": [
    "## Search corresponding data\n",
    "Get the data required for the analysis. Define variables, models, experiment, and time resolution as defined in <a href=\"#data_wrangling\">2. Data Wrangling</a>\n",
    ". \n",
    "\n",
    "* use member_id = 'r1i1p1f1'.\n",
    "* using intake-esmâs `search()` function:\n",
    "  * `col.search(variable_id, source_id, experiment_id, table_id, member_id, institution_id, grid_label)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15a84fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Variables\n",
    "# variable_id=[\n",
    "#             'cli',\n",
    "#             'clivi',\n",
    "#             'clw',\n",
    "#             # 'lwp',\n",
    "#             # 'pr',\n",
    "#             'prsn',\n",
    "#             # 'ta', \n",
    "#             'tas'\n",
    "#              ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b7a5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Models\n",
    "# # list_models = [\n",
    "# #     'NorESM2-MM',\n",
    "# #     'TaiESM1',\n",
    "# #     'EC-Earth3-AerChem',\n",
    "# #     'GFDL-ESM4',\n",
    "# #     'SAM0-UNICON',\n",
    "# #     'CAMS-CSM1-0',\n",
    "# #     'CMCC-CM2-HR4',\n",
    "# #     'MPI-ESM1-2-HR',\n",
    "# #     'BCC-CSM2-MR',\n",
    "# #     'E3SM-1-1',\n",
    "# #     'CMCC-CM2-SR5',\n",
    "# #     'CMCC-ESM2',\n",
    "# #     'FGOALS-f3-L',\n",
    "# #     'E3SM-1-1-ECA',\n",
    "# #     'CIESM',\n",
    "# #     'GFDL-CM4',\n",
    "# #     'MRI-ESM2-0']  \n",
    "\n",
    "# list_models = ['MIROC6', 'CESM2', 'CanESM5', 'AWI-ESM-1-1-LR', 'MPI-ESM1-2-LR',\n",
    "#        'UKESM1-0-LL', 'HadGEM3-GC31-LL', 'CNRM-CM6-1', 'CNRM-ESM2-1',\n",
    "#        'IPSL-CM6A-LR', 'IPSL-CM5A2-INCA']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b62978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## experiment\n",
    "# experiment_id = ['historical']\n",
    "\n",
    "# ## time resolution\n",
    "# t_res = ['day', 'CFday']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540777e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## search for variables, models, ...\n",
    "# cat = col.search(variable_id=variable_id, source_id=list_models, experiment_id=experiment_id, table_id = t_res, member_id=['r1i1p1f1'])\n",
    "# # cat.df\n",
    "# ## show the CMIP6 models found in pandas Dataframe \n",
    "# cat.df['source_id'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5c4b0a",
   "metadata": {},
   "source": [
    "## Create dictionary from the list of datasets we found\n",
    "Load the found datasets into xarray dataset containers using intake-esmâs `to_dataset_dict()` function, which yields a Python dictionary.\n",
    "\n",
    "> **_NOTE:_** This step may take several minutes so be patient!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550968bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dset_dict = cat.to_dataset_dict(zarr_kwargs={'use_cftime':True,})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8585e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dset_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febae699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # list all merged datasets and show coordinates\n",
    "# for keys, ds in dset_dict.items():\n",
    "#     print('{}: {}'.format(keys, list(ds.dims)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9cced3",
   "metadata": {},
   "source": [
    "## Calendar\n",
    "Not all models in CMIP6 use the same calendar. Hence we double check the time axis. Later, when we regrid to the same horizontal resolution (<a href=\"#regrid_hz\">Regrid CMIP6 data</a>) we will assign the same calendars for each model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e425fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # metadata of the historical run:\n",
    "# _d2 = pd.Series([\"calendar\",\n",
    "#                  \"branch_time_in_parent\", #\"parent_activity_id\", \"parent_experiment_id\",\t\"parent_mip_era\",\n",
    "#                  \"parent_source_id\",#\"parent_sub_experiment_id\", \n",
    "#                  \"parent_time_units\",# \"parent_variant_label\"\n",
    "#                   ])\n",
    "# _d2 = pd.DataFrame(_d2).rename(columns={0:'index'})\n",
    "# for i in dset_dict.keys():\n",
    "#     _data = []\n",
    "#     _names =[]\n",
    "#     _data.append(dset_dict[i].time.to_index().calendar)\n",
    "#     for k, v in dset_dict[i].attrs.items():\n",
    "        \n",
    "#         if 'parent_time_units' in k or 'branch_time_in_parent' in k or 'parent_source_id' in k:\n",
    "#             _data.append(v)\n",
    "#             _names.append(k)\n",
    "#     _d2 = pd.concat([_d2,   pd.Series(_data)], axis=1)\n",
    "\n",
    "# _d2.dropna(how='all', axis=1, inplace=True)\n",
    "# _d2 = _d2.set_index('index')\n",
    "# _d2.columns = _d2.loc['parent_source_id']\n",
    "# _d2.drop('parent_source_id').T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdde1726",
   "metadata": {},
   "source": [
    "## Show attributes and individual identifier\n",
    "NorESM2-MM is going to be the reference model for the horizontal grid. The `xarray` datasets inside `dset_dict` can be extracted as any value in a Python dictionary.\n",
    "\n",
    "The dictonary key for NorESM2-MM is: **CMIP.NCC.NorESM2-MM.historical.Amon.gn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e045126e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if variable_id[0] == 'lwp':\n",
    "#     ds = dset_dict['CMIP.NCC.NorESM2-MM.historical.AERmon.gn']\n",
    "# else:\n",
    "#     ds = dset_dict['CMIP.NCC.NorESM2-MM.historical.Amon.gn']\n",
    "    \n",
    "\n",
    "## attributes of the xarray dataset \n",
    "# ds[variable_id[0]].attrs, ds.attrs['tracking_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b6cbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dset_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2311f9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for model in dset_dict.keys():\n",
    "#     for keys in dset_dict[model].keys():\n",
    "#         print(model, keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a37c88",
   "metadata": {},
   "source": [
    "## Assign attributes to the variables\n",
    "\n",
    "We will assign the attributes to the variables as in ERA5 to make CMIP6 and ERA5 variables comperable.\n",
    "\n",
    "* [`cli`](http://clipc-services.ceda.ac.uk/dreq/u/dd916e3e2eca18cda5d9f81749d0c91c.html) and [`clw`](http://clipc-services.ceda.ac.uk/dreq/u/86b2b3318a73839edfafa9d46864aadc.html) in **kg kg-1** $\\rightarrow$ Multiply by **1000** to get **g kg-1**\n",
    "* [`clivi`](http://clipc-services.ceda.ac.uk/dreq/u/73c496f5669cc122cf1cddfe4df2a27a.html) and [`lwp`](http://clipc-services.ceda.ac.uk/dreq/u/e6b31a1928879fcd3c92fe7b592f070e.html) in **kg m-2** $\\rightarrow$ Multiply by **1000** to get **g m-2**\n",
    "* [`pr`](http://clipc-services.ceda.ac.uk/dreq/u/62f26742cf240c1b5169a5cd511196b6.html) and [`prsn`](http://clipc-services.ceda.ac.uk/dreq/u/051919eddec810e292c883205c944ceb.html) in **kg m-2 s-1** $\\rightarrow$ Multiply by **86400** to get **mm day-1**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670a98a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for var_id in variable_id:\n",
    "#     for keys in dset_dict.keys():\n",
    "#         if var_id == 'cli' or var_id == 'clw' or var_id == 'lwp' or var_id  == 'clivi':\n",
    "#             dset_dict[keys][var_id] = dset_dict[keys][var_id]*1000\n",
    "#             if var_id == 'cli':\n",
    "#                 dset_dict[keys][var_id].attrs = {'units': 'g kg-1', 'long_name': 'Mass Fraction of Cloud Ice', 'standard_name': 'mass_fraction_of_cloud_ice_in_air', 'comment': 'Includes both large-scale and convective cloud. This is calculated as the mass of cloud ice in the grid cell divided by the mass of air (including the water in all phases) in the grid cell. It includes precipitating hydrometeors ONLY if the precipitating hydrometeors affect the calculation of radiative transfer in model.', 'cell_methods': 'area: time: mean (interval: 5 minutes)', 'cell_measures': 'area: areacella',}\n",
    "#             if var_id == 'clw':\n",
    "#                 dset_dict[keys][var_id].attrs = {'units': 'g kg-1', 'long_name': 'Mass Fraction of Cloud Liquid Water', 'standard_name': 'mass_fraction_of_cloud_liquid_water_in_air', 'comment': 'Includes both large-scale and convective cloud. Calculate as the mass of cloud liquid water in the grid cell divided by the mass of air (including the water in all phases) in the grid cells. Precipitating hydrometeors are included ONLY if the precipitating hydrometeors affect the calculation of radiative transfer in model.', 'cell_methods': 'area: time: mean', 'cell_measures': 'area: areacella'}\n",
    "#             if var_id == 'clivi':\n",
    "#                 dset_dict[keys][var_id].attrs = {'units': 'g m-2', 'long_name': 'Ice Water Path', 'comment': 'mass of ice water in the column divided by the area of the column (not just the area of the cloudy portion of the column). Includes precipitating frozen hydrometeors ONLY if the precipitating hydrometeor affects the calculation of radiative transfer in model.', 'cell_methods': 'area: time: mean', 'cell_measures': 'area: areacella'} \n",
    "#             if var_id == 'lwp':\n",
    "#                 dset_dict[keys][var_id].attrs = {'units': 'g m-2', 'long_name': 'Liquid Water Path', 'comment': 'The total mass of liquid water in cloud per unit area.', 'cell_methods': 'area: time: mean', 'cell_measures': 'area: areacella'}\n",
    "\n",
    "#         if var_id == 'pr' or var_id == 'prsn':\n",
    "#             try: \n",
    "#                 dset_dict[keys][var_id] = dset_dict[keys][var_id]*86400\n",
    "#                 if var_id == 'pr':\n",
    "#                     dset_dict[keys][var_id].attrs = {'units': 'mm day-1', 'long_name': 'Precipitation', 'comment': 'includes both liquid and solid phases','cell_methods': 'area: time: mean', 'cell_measures': 'area: areacella'}\n",
    "#                 if var_id == 'prsn':\n",
    "#                     dset_dict[keys][var_id].attrs = {'units': 'mm day-1', 'long_name': 'Snowfall', 'comment': 'At surface; includes precipitation of all forms of water in the solid phase', 'cell_methods': 'area: time: mean', 'cell_measures': 'area: areacella'}\n",
    "#             except KeyError:\n",
    "#                 continue\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688d8d63",
   "metadata": {},
   "source": [
    " ## Interpolate from CMIP6 hybrid sigma-pressure levels to ERA5 isobaric pressure levels\n",
    "\n",
    "The vertical variables in the CMIP6 models are in hybrid sigma-pressure levels. Hence the vertical variable in the xarray datasets in `dset_dict` will be calculated by using the [GeoCAT-comb](https://geocat-comp.readthedocs.io/en/latest/index.html#) function to [interpolate data from hybrid-sigma levels to isobaric levels](https://geocat-comp.readthedocs.io/en/latest/user_api/generated/geocat.comp.interpolation.interp_hybrid_to_pressure.html#geocat.comp.interpolation.interp_hybrid_to_pressure).\n",
    "\n",
    "The GeoCAT-comb function takes the following input:\n",
    "* `data`:   Multidimensional data array, which holds hybrid-sigma levels and has a lev_dim coordinate.\n",
    "* `ps`:     A multi-dimensional array of surface pressures (Pa), same time/space shape as data. Not all variables include the surface pressure, hence we will search the `Pangeo.io` catalog to find the surface pressure associated with the model. \n",
    "* `hyam`:     One-dimensional arrays containing the hybrid A coefficients. Must have the same dimension size as the lev_dim dimension of data.\n",
    "* `hybm`:     One-dimensional arrays containing the hybrid B coefficients. Must have the same dimension size as the lev_dim dimension of data.\n",
    "* `p0`:       Scalar numeric value equal to surface reference pressure (Pa). Defaults to 100000 Pa.\n",
    "* `new_levels`: A one-dimensional array of output pressure levels (Pa). We will use the pressure of `air_temperature` (19 levels).\n",
    "\n",
    "\n",
    "$$ P(i,j,k) = hyam(k) p0 + hybm(k) ps(i,j)$$\n",
    "\n",
    "```\n",
    "import geocat\n",
    "\n",
    "geocat.comp.interpolation.interp_hybrid_to_pressure(data      =ds['variable'], \n",
    "                                                    ps        =ds['ps'], \n",
    "                                                    hyam      =ds['a'], \n",
    "                                                    hybm      =ds['b'], \n",
    "                                                    p0        =ds['p0'], \n",
    "                                                    new_levels=ds['plev'])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0fbe2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load the levels from the ERA5 file and transfer to Pa\n",
    "# era_level = (xr.open_dataset('/scratch/franzihe/input/ERA5/monthly_means/0.25deg/clwc_Amon_ERA5_198501_198912.nc')['level'])*100\n",
    "\n",
    "# if (('clw' or 'cli' or 'ta') in variable_id) == True:\n",
    "#     # if var_id == 'clw' or var_id == 'cli':\n",
    "#     for keys in dset_dict.keys():\n",
    "#         # rename cmip pressure level for atmospheric temperature\n",
    "#         dset_dict[keys] = dset_dict[keys].rename({'plev':'clev'})\n",
    "#         # interpolate from hybrid to pressure levels for clw and cli\n",
    "#         for var_id in [variable_id[variable_id.index('clw')], variable_id[variable_id.index('cli')]]:\n",
    "\n",
    "#             if ('ps' in list(dset_dict[keys].keys())) == False:     # valid for models which don't provide ps in the clw or cli dataset\n",
    "#                 model = keys.split('.')[2]\n",
    "#                 ds_ps = col.search(source_id=model, table_id = ['Amon', ], experiment_id=['historical'], variable_id=['ps','p0', ], member_id=['r1i1p1f1']).to_dataset_dict(zarr_kwargs={'use_cftime':True,}, )\n",
    "#                 dset_dict[keys].update(ds_ps[keys], )\n",
    "#                 dset_dict[keys]['ps'] = dset_dict[keys]['ps'].isel(member_id = 0)\n",
    "                \n",
    "                \n",
    "#             # Rename datasets with different naming convention for constant A\n",
    "#             if ('a' in list(dset_dict[keys].keys())) == False:\n",
    "#                 dset_dict[keys] = dset_dict[keys].rename({'ap':'a', 'ap_bnds': 'a_bnds'})\n",
    "                    \n",
    "                    \n",
    "#             # Convert the model level to isobaric levels\n",
    "#             #### a, b, ps, p0\n",
    "#             if ('a' in list(dset_dict[keys].keys())) == True and ('b' in list(dset_dict[keys].keys())) == True and ('p0' in list(dset_dict[keys].keys())) == True and ('ps' in list(dset_dict[keys].keys())) == True:\n",
    "#                 dset_dict[keys]['{}_interp'.format(var_id)] = gc.interpolation.interp_hybrid_to_pressure(data=dset_dict[keys][var_id].isel(member_id = 0), \n",
    "#                                                                                                                 ps=dset_dict[keys]['ps'],hyam=dset_dict[keys]['a'],\n",
    "#                                                                                                                 hybm=dset_dict[keys]['b'],\n",
    "#                                                                                                                 p0=dset_dict[keys]['p0'].values, \n",
    "#                                                                                                                 new_levels=(era_level).values, )\n",
    "#                 # # remove the variables needed for the calculation of the isobaric levels. If this step is not performed, \n",
    "#                 # # the horizontal regridding will not be possible.\n",
    "#                 # dset_dict[keys] = dset_dict[keys].drop(('a', 'p0', 'b', 'ps', 'a_bnds', 'b_bnds', var_id))\n",
    "#                 dset_dict[keys] = dset_dict[keys].drop((var_id))\n",
    "#                 dset_dict[keys] = dset_dict[keys].rename({'{}_interp'.format(var_id):var_id})\n",
    "\n",
    "#             #### a, b, ps\n",
    "#             elif ('a' in list(dset_dict[keys].keys())) == True and ('b' in list(dset_dict[keys].keys())) == True and ('ps' in list(dset_dict[keys].keys())) == True and ('p0' in list(dset_dict[keys].keys())) == False:\n",
    "#                 dset_dict[keys]['{}_interp'.format(var_id)] = gc.interpolation.interp_hybrid_to_pressure(data=dset_dict[keys][var_id].isel(member_id=0), \n",
    "#                                                                                                                 ps=dset_dict[keys]['ps'],\n",
    "#                                                                                                                 hyam=dset_dict[keys]['a'], \n",
    "#                                                                                                                 hybm=dset_dict[keys]['b'],\n",
    "#                                                                                                                 p0=100000.0,\n",
    "#                                                                                                                 new_levels=(era_level).values, )\n",
    "#                 # dset_dict[keys] = dset_dict[keys].drop(('a', 'b', 'ps', 'a_bnds', 'b_bnds', var_id))\n",
    "#                 dset_dict[keys] = dset_dict[keys].drop((var_id))\n",
    "#                 dset_dict[keys] = dset_dict[keys].rename({'{}_interp'.format(var_id):var_id})\n",
    "        \n",
    "#         # remove the variables needed for the calculation of the isobaric levels. If this step is not performed, \n",
    "#         # the horizontal regridding will not be possible.\n",
    "#         if ('p0' in list(dset_dict[keys].keys())) == True:\n",
    "#             dset_dict[keys] = dset_dict[keys].drop(('a', 'p0', 'b', 'ps', 'a_bnds', 'b_bnds',))\n",
    "#         elif ('p0' in list(dset_dict[keys].keys())) == False:\n",
    "#             dset_dict[keys] = dset_dict[keys].drop(('a', 'b', 'ps', 'a_bnds', 'b_bnds',))\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d66e138",
   "metadata": {},
   "source": [
    "## Regrid CMIP6 data to NorESM2-MM grid <a id='regrid_hz'></a>\n",
    "\n",
    "We want to conduct statistical analysis at the annual and seasonal timescales to determine the biases in cloud phase and precipitation (liquid and solid) in the CMIP6 models. At the moment we have all historical data from the CMIP6 models. For this, we will have to extract the 30-year period between 1985 and 2014.\n",
    "\n",
    "$\\rightarrow$ Define a start and end year.\n",
    "\n",
    "The CMIP6 high resolution models have approximately a nominal resolution of 100km. But not all have identical grid spacing. Hence we will make use of the python package `xesmf` and the documentation on [decreasing resolution](https://xesmf.readthedocs.io/en/latest/notebooks/Compare_algorithms.html#Decreasing-resolution), [Limitations and warnings](https://xesmf.readthedocs.io/en/latest/notebooks/Masking.html?highlight=conservative#Limitations-and-warnings). \n",
    "\n",
    "NorESM2-MM will be the reference grid since we want to compare the models to the ERA5 data. The ERA5 data has a nominal resolution of 0.25deg and has been regridded to the same horizontal resolution as the NorESM2-MM in [the ERA5 Jupyter Notebook](../ERA5/ERA5_1985-2014.ipynb). \n",
    "\n",
    "$\\rightarrow$ Define NorESM2-MM as the reference grid `ds_out`.\n",
    "\n",
    "Create a new Python dictionary (`ds_gridded_dict`) with the regridded CMIP6 `xarray` datasets between 1985 an 2014. Save each regridded model to a `netcdf`, locally. \n",
    "\n",
    "> **_NOTE:_** This step may take several minutes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13589926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# starty = 1985; endy = 2014\n",
    "# year_range = range(starty, endy+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf06ee5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_out = dset_dict['CMIP.NCC.NorESM2-MM.historical.Amon.gn']\n",
    "# ds_in  = dset_dict[model]\n",
    "\n",
    "# import xesmf\n",
    "\n",
    "# # Regridder data\n",
    "# regridder = xesmf.Regridder(ds_in, ds_out, \"conservative\")\n",
    "\n",
    "# # Apply regridder to data\n",
    "#  # the entire dataset can be processed at once\n",
    "# ds_in_regrid = regridder(ds_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e1c933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create dictionary for reggridded data\n",
    "# ds_gridded_dict = dict()\n",
    "\n",
    "# # Read in the output grid from NorESM\n",
    "# if variable_id[0] == 'lwp':\n",
    "#     ds_out = dset_dict['CMIP.NCC.NorESM2-MM.historical.AERmon.gn'].isel(member_id = 0)\n",
    "# else:\n",
    "#     ds_out = dset_dict['CMIP.NCC.NorESM2-MM.historical.Amon.gn'].isel(member_id = 0)\n",
    "# ds_out = ds_out.sel(time = ds_out.time.dt.year.isin(year_range)).squeeze()\n",
    "\n",
    "\n",
    "# counter = 0\n",
    "\n",
    "# for keys in dset_dict.keys():\n",
    "#     # select only models which have atmospheric monthly values\n",
    "#     amon = keys.split('.')[-2]\n",
    "#     if amon == 'Amon' or amon == 'AERmon': \n",
    "#         # select model name \n",
    "#         model = keys.split('.')[2]\n",
    "        \n",
    "#         # select where data should be saved\n",
    "#         if len(variable_id) > 1:\n",
    "#             filename = '{}_Amon_1deg_{}01_{}12.nc'.format(variable_id, starty, endy)\n",
    "#         elif len([variable_id]) == 1:\n",
    "#             filename = '{}_Amon_1deg_{}01_{}12.nc'.format(variable_id[0], starty, endy)\n",
    "#         # filename = '{}_Amon_1deg_{}01_{}12.nc'.format(variable_id[0], starty, endy)\n",
    "#         savepath = '/scratch/franzihe/output/CMIP6_hist/1deg/{}/'.format(model)\n",
    "#         nc_out = savepath + filename\n",
    "#         files = glob(nc_out)\n",
    "        \n",
    "#         # Input data from CMIP6 model to be regridded\n",
    "#         ds_in = dset_dict[keys].isel(member_id = 0)\n",
    "#         ds_in = ds_in.sel(time = ds_in.time.dt.year.isin(year_range)).squeeze()\n",
    "            \n",
    "#         # common time grid\n",
    "#         ds_in['time'] = ds_out['time']\n",
    "            \n",
    "\n",
    "#         # Regrid data\n",
    "#         ds_in_regrid = fct.regrid_data(ds_in, ds_out)\n",
    "\n",
    "\n",
    "#         # Shift the longitude from 0-->360 to -180-->180 and sort by longitude and time\n",
    "#         ds_in_regrid = ds_in_regrid.assign_coords(lon=(((ds_in_regrid.lon + 180) % 360) - 180)).sortby('lon').sortby('time')\n",
    "#         ds_in_regrid = ds_in_regrid.reset_coords(names=['time_bnds', ], drop=True)\n",
    "            \n",
    "            \n",
    "#         # create dataset with all models\n",
    "#         ds_gridded_dict[model] = ds_in_regrid\n",
    "\n",
    "#         # if nc_out in files:\n",
    "#         #     print('{} is downloaded'.format(nc_out))\n",
    "#         #     counter += 1\n",
    "#         #     print('Have regridded in total: {:} files'.format(str(counter)))\n",
    "#         # else:    \n",
    "#             # Save to netcdf file\n",
    "#         ds_in_regrid.to_netcdf(nc_out)\n",
    "#         print('file written: {}'.format(nc_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d3e3d0",
   "metadata": {},
   "source": [
    "## Connect all models into one Dataset with new coordinate 'model'\n",
    "\n",
    "We will create a `xarray.Dataset` with all CMIP6 models, after the interpolation to the same horizonal (and vertical) resolution. This step will make the next steps easier, as we will not need the the full dictonary key and can just use the model name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa03d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # not needed if regridding works\n",
    "# ds_gridded_dict = dict()\n",
    "# for model in list_models:\n",
    "#     ds_gridded_dict[model] = xr.open_dataset('/scratch/franzihe/output/CMIP6_hist/1deg/{}/{}_Amon_1deg_198501_201412.nc'.format(model,variable_id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c2aa10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # expand the model with nan array where missing variables\n",
    "# for model in ds_gridded_dict.keys():\n",
    "#     for var in variable_id:\n",
    "#         # print(var, model)\n",
    "#         if (var in ds_gridded_dict[model].variables) == False:\n",
    "#             print(model, var)\n",
    "#             if var == 'pr' or var == 'prsn':\n",
    "#                 ds_gridded_dict[model][var] = xr.DataArray(data=da.full(shape = (ds_gridded_dict[model]['time'].shape[0], \n",
    "#                                                                                  ds_gridded_dict[model]['lat'].shape[0], \n",
    "#                                                                                  ds_gridded_dict[model]['lon'].shape[0]), \n",
    "#                                                             fill_value=np.nan, \n",
    "#                                                             chunks=(120, 721, 1440/2)), dims = [ds_gridded_dict[model].time.dims[0], ds_gridded_dict[model].lat.dims[0], ds_gridded_dict[model].lon.dims[0]], coords=[ds_gridded_dict[model].time.values, ds_gridded_dict[model].lat.values, ds_gridded_dict[model].lon.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01e6664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _ds = list(ds_gridded_dict.values())\n",
    "# _coord = list(ds_gridded_dict.keys())\n",
    "# ds_cmip = xr.concat(objs=_ds, dim=_coord, coords=\"all\").rename({'concat_dim':'model'})\n",
    "# ds_cmip = ds_cmip.drop('bnds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd920c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_cmip = xr.open_mfdataset('/scratch/franzihe/output/CMIP6_hist/1deg/*.nc')\n",
    "# ds_cmip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f61689",
   "metadata": {},
   "source": [
    "## Supercooled liquid water fraction\n",
    "\n",
    "$$SLF = \\frac{\\text{cloud liquid water content}}{\\text{cloud liquid water content} + \\text{cloud ice water content}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3359cbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_cmip['SLF'] = ds_cmip['clw']/(ds_cmip['cli'] + ds_cmip['clw'])\n",
    "# ds_cmip['SLF'].attrs = {'units': '', 'long_name':'Super cooled liquid water fraction'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb73cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ds_cmip['SLF'] = fct.set4D_latitude_values_nan(ds_cmip['SLF'], 45, -45)\n",
    "\n",
    "# # find only SLF where snowfall\n",
    "# ds_cmip['SLF_sf'] = ds_cmip['SLF'].where((ds_cmip['prsn'] * 3600/86400) >= 0.01)\n",
    "# ds_cmip['SLF_sf'] = ds_cmip['SLF_sf'].sortby('latitude', ascending=True)\n",
    "# ds_cmip['SLF_sf'] = ds_cmip['SLF_sf'].groupby('time.season').mean(('time', 'longitude'), skipna=True)\n",
    "\n",
    "# fig, axsm = plt.subplots(2,2, sharex=True, sharey=True, figsize=[15, 7.5])\n",
    "# for ax, sea in zip(axsm.flatten(), ds_cmip['SLF_sf'].season):\n",
    "#     cf = ds_cmip['SLF_sf'].sel(season=sea).plot(ax=ax,x='latitude', y='level', cmap=cm.tokyo_r, levels=np.arange(0,1.1,0.1), ylim= [1000, 1], yincrease=False, add_colorbar=False)\n",
    "#     ax.grid()\n",
    "#     ax.set(title='season = {}'.format(sea.values), ylabel='Pressure [hPa]', xlabel='Latitude')\n",
    "    \n",
    "\n",
    "# fig.suptitle('ERA5 ({} - {})'.format(starty, endy), fontweight=\"bold\")\n",
    "# cbaxes = fig.add_axes([0.1, 0.0, 0.8, 0.025])\n",
    "# cbar = plt.colorbar(cf, cax=cbaxes, shrink=0.5, orientation='horizontal', label='Supercooled liquid water fraction')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8618c33",
   "metadata": {},
   "source": [
    "## Statistics\n",
    "For variables:\n",
    "- Snowfall [sf]\n",
    "- Total column cloud liquid, supercooled liqid, and rain water [tclslrw]\n",
    "- Total column cloud ice, snow water [tcisw]\n",
    "- 2m-Temperature [2t]\n",
    "\n",
    "1. Find where liquid water path is $\\ge$ 5 g m-2 \n",
    "2. Find where snowfall is $\\ge$ 0.01mm h-1\n",
    "3. Find where 2m-temperature $\\le$ 0 $^o$ C "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43706002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def set4D_latitude_values_nan(array, upper_lat, lower_lat):\n",
    "#     array[:,:,int(array['lat'].where((array['lat'] > lower_lat) & (array['lat'] < upper_lat)).argmin('lat')) : int(array['lat'].where((array['lat'] > lower_lat) & (array['lat'] < upper_lat)).argmax('lat')), :] = xr.DataArray(data=da.full(shape=(array[:,:,int(array['lat'].where((array['lat']>lower_lat) & (array['lat']<upper_lat)).argmin('lat')) :\\\n",
    "#                                            int(array['lat'].where((array['lat']>lower_lat) & (array['lat']<upper_lat)).argmax('lat')), :]).shape,\n",
    "#                           fill_value=np.nan),\n",
    "#              dims=(array[:,:,int(array['lat'].where((array['lat']>lower_lat) & (array['lat']<upper_lat)).argmin('lat')) :\\\n",
    "#                              int(array['lat'].where((array['lat']>lower_lat) & (array['lat']<upper_lat)).argmax('lat')), :]).dims,\n",
    "#              coords=(array[:,:,int(array['lat'].where((array['lat']>lower_lat) & (array['lat']<upper_lat)).argmin('lat')) :\\\n",
    "#                                int(array['lat'].where((array['lat']>lower_lat) & (array['lat']<upper_lat)).argmax('lat')), :]).coords)\n",
    "\n",
    "#     return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8963545b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_cmip['prsn']  = set4D_latitude_values_nan(ds_cmip['prsn'], 45, -45)\n",
    "# ds_cmip['lwp']   = set4D_latitude_values_nan(ds_cmip['lwp'], 45, -45)\n",
    "# ds_cmip['clivi'] = set4D_latitude_values_nan(ds_cmip['clivi'], 45, -45)\n",
    "# ds_cmip['tas']   = set4D_latitude_values_nan(ds_cmip['tas'], 45, -45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d96ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1. find where LWP >=5 gm-2\n",
    "# sf = ds_cmip['prsn'].where(ds_cmip['lwp']>=5)\n",
    "# lwp = ds_cmip['lwp'].where(ds_cmip['lwp']>=5)\n",
    "# iwp = ds_cmip['clivi'].where(ds_cmip['lwp']>=5)\n",
    "# t2 = ds_cmip['tas'].where(ds_cmip['tas']>=5)\n",
    "\n",
    "# # 2. find where snowfall >= 0.01mmh-1\n",
    "# unit_sf = ds_cmip['prsn']*(3600/86400)\n",
    "# sf = sf.where(unit_sf>=0.01)\n",
    "# lwp = lwp.where(unit_sf>=0.01)\n",
    "# iwp = iwp.where(unit_sf>=0.01)\n",
    "# t2 = t2.where(unit_sf>=0.01)\n",
    "\n",
    "# # 3. find where 2m-temperature <= 0C\n",
    "# sf = sf.where(ds_cmip['tas']<=273.15)\n",
    "# lwp = lwp.where(ds_cmip['tas']<=273.15)\n",
    "# iwp = iwp.where(ds_cmip['tas']<=273.15)\n",
    "# t2 = t2.where(ds_cmip['tas']<=273.15)\n",
    "\n",
    "# # count occurences per season\n",
    "# sf_count = sf.groupby('time.season').count(dim='time',keep_attrs=True)\n",
    "# lwp_count = lwp.groupby('time.season').count(dim='time',keep_attrs=True)\n",
    "# iwp_count = iwp.groupby('time.season').count(dim='time',keep_attrs=True)\n",
    "# t2_count = t2.groupby('time.season').count(dim='time',keep_attrs=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035683a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plt_seasonal_NH_SH(variable,levels,cbar_label,plt_title):\n",
    "\n",
    "#     f, axsm = plt.subplots(nrows=2,ncols=4,figsize =[10,7], subplot_kw={'projection': ccrs.NorthPolarStereo(central_longitude=0.0,globe=None)})\n",
    "\n",
    "#     for ax, season in zip(axsm.flatten()[:4], variable.season):\n",
    "#         ax.add_feature(cy.feature.COASTLINE, alpha=0.5)\n",
    "#         ax.set_extent([-180, 180, 90, 45], ccrs.PlateCarree())\n",
    "#         ax.set(title ='season = {}'.format(season.values))\n",
    "#         gl = ax.gridlines(draw_labels=True)\n",
    "#         gl.top_labels   = False\n",
    "#         gl.right_labels = False\n",
    "#         variable.sel(season=season).plot(ax=ax, transform=ccrs.PlateCarree(), add_colorbar=False,\n",
    "#                             cmap=cm.hawaii_r, levels=levels)\n",
    "\n",
    "#     for ax, i, season in zip(axsm.flatten()[4:], np.arange(5,9), variable.season):\n",
    "#         ax.remove()\n",
    "#         ax = f.add_subplot(2,4,i, projection=ccrs.SouthPolarStereo(central_longitude=0.0, globe=None))\n",
    "#         ax.add_feature(cy.feature.COASTLINE, alpha=0.5)\n",
    "#         ax.set_extent([-180, 180, -90, -45], ccrs.PlateCarree())\n",
    "#         ax.set(title ='season = {}'.format(season.values))\n",
    "#         gl = ax.gridlines(draw_labels=True)\n",
    "#         gl.top_labels   = False\n",
    "#         gl.right_labels = False\n",
    "#         cf = variable.sel(season=season).plot(ax=ax, transform=ccrs.PlateCarree(), add_colorbar=False,\n",
    "#                             cmap=cm.hawaii_r, levels=levels)\n",
    "\n",
    "#     cbaxes = f.add_axes([1.0125, 0.025, 0.025, 0.9])\n",
    "#     cbar = plt.colorbar(cf, cax=cbaxes, shrink=0.5,extend='max', orientation='vertical', label=cbar_label)\n",
    "#     f.suptitle(plt_title, fontweight=\"bold\");\n",
    "#     plt.tight_layout(pad=0., w_pad=0., h_pad=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed21dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for model in sf_count['model']:\n",
    "\n",
    "#     # snowfall event occurance\n",
    "#     plt_seasonal_NH_SH((sf_count.where(sf_count>0.)).sel(model=model),np.arange(0,100,10),cbar_label ='{} frequency (month)',plt_title='{} {} ({} - {}) Count of months where sf >= 7.2mm'.format(model.values,sf_count.where(sf_count>0.).attrs['long_name'], starty,endy))\n",
    "\n",
    "#     # snowfall divided by ice water path\n",
    "#     sf_iwp = ((sf/(iwp)).groupby('time.season').mean('time', keep_attrs=True, skipna=True))\n",
    "#     plt_seasonal_NH_SH(sf_iwp.sel(model=model),np.arange(0,0.0475,0.0025),cbar_label='precipitation efficency ()',plt_title='{} Snowfall/IWP ({} - {}) where sf >= 7.2mm'.format(model.values,starty,endy))\n",
    "\n",
    "#     # snowfall divided by the sum of ice and liquid water path\n",
    "#     sf_iwp_lwp = ((sf/(lwp+iwp)).groupby('time.season').mean('time', keep_attrs=True, skipna=True))\n",
    "#     plt_seasonal_NH_SH(sf_iwp_lwp.sel(model=model), np.arange(0,0.0475,0.0025),cbar_label='precipiation efficency ()', plt_title='{} Snowfall/(IWP + LWP) ({} {}) where sf >= 7.2mm'.format(model.values,starty,endy))\n",
    "\n",
    "#     # liquid water path\n",
    "#     lwp_season = lwp.groupby('time.season').mean(dim='time',keep_attrs=True, skipna=True)\n",
    "#     plt_seasonal_NH_SH(lwp_season.sel(model=model), np.arange(0,360, 10),cbar_label='{} ({})'.format(lwp_season.attrs['long_name'], lwp_season.attrs['units']),plt_title='{} {} ({} - {}) Count of months where sf >= 7.2mm'.format(model.values,lwp_season.attrs['long_name'], starty,endy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c261d11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cbfb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _iwp = iwp.groupby('time.season').mean(('time', ), keep_attrs=True, skipna=True)\n",
    "# _lwp = lwp.groupby('time.season').mean(('time', ), keep_attrs=True, skipna=True)\n",
    "# _sf = sf.groupby('time.season').mean(('time', ), keep_attrs=True, skipna=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e402fead",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 'EC-Earth3-AerChem'\n",
    "# 'NorESM2-MM'\n",
    "# 'GFDL-ESM4'\n",
    "# 'TaiESM1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8dd3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _iwp.min().values, _iwp.max().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6acd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _sf.min().values, _sf.max().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34e09ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from matplotlib.colors import LogNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7a1a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plt_2dhist_iwp_sf(_iwp, _sf,model ):\n",
    "#     f, axsm = plt.subplots(nrows=2,ncols=4,figsize =[10,5], sharex=True, sharey=True)\n",
    "#     cmap = cm.batlow\n",
    "#     # levels = np.arange(0.1,65000,5000)\n",
    "#     # norm = BoundaryNorm(levels, ncolors=cmap.N, )\n",
    "#     norm = LogNorm(vmin=1, vmax=65000)\n",
    "\n",
    "\n",
    "\n",
    "#     for ax, season in zip(axsm.flatten()[:4], _iwp.season):\n",
    "#         Z, xedges, yedges = np.histogram2d((_iwp.where(_iwp['lat'] >=45).sel(season=season).values.flatten()), \n",
    "#                                         (_sf.where(_sf['lat'] >=45).sel(season=season).values.flatten()), \n",
    "#                                         bins=[54, 25], \n",
    "#                                         range=[[5,545],[0, 25]])   \n",
    "\n",
    "#         im = ax.pcolormesh(xedges, yedges, Z.transpose(), cmap=cmap,norm=norm,)\n",
    "#         # cbar = f.colorbar(im, ax=ax,)\n",
    "#         ax.set(title =r'lat$\\geq 45^\\circ$N; season = {}'.format(season.values))\n",
    "#         ax.grid()\n",
    "\n",
    "#     for ax, season in zip(axsm.flatten()[4:], _iwp.season):\n",
    "#         Z, xedges, yedges = np.histogram2d((_iwp.where(_iwp['lat'] <=-45).sel(season=season).values.flatten()), \n",
    "#                                         (_sf.where(_sf['lat'] <=-45).sel(season=season).values.flatten()), \n",
    "#                                         bins=[54, 25], \n",
    "#                                         range=[[5,545],[0, 25]])   \n",
    "\n",
    "#         im = ax.pcolormesh(xedges, yedges, Z.transpose(), cmap=cmap,norm=norm,)\n",
    "#         # cbar = f.colorbar(im, ax=ax, )\n",
    "#         ax.set(title =r'lat$\\leq-45^\\circ$S; season = {}'.format(season.values))\n",
    "        \n",
    "#         ax.set_xlabel('Ice Water Path ({})'.format(_iwp.attrs['units']))\n",
    "#         ax.grid()\n",
    "        \n",
    "#     axsm.flatten()[0].set_ylabel('{} ({})'.format(_sf.attrs['long_name'], _sf.attrs['units']))\n",
    "#     axsm.flatten()[4].set_ylabel('{} ({})'.format(_sf.attrs['long_name'], _sf.attrs['units']))\n",
    "\n",
    "\n",
    "#     cbaxes = f.add_axes([1.0125, 0.025, 0.025, 0.9])\n",
    "#     cbar = plt.colorbar(im, cax=cbaxes, shrink=0.5, orientation='vertical', label='frequency')\n",
    "#     f.suptitle('{} ({} - {}) Months where sf >= 7.2mm'.format(model, starty,endy), fontweight=\"bold\");\n",
    "#     plt.tight_layout(pad=0.5, w_pad=0.5, h_pad=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af28755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = ['EC-Earth3-AerChem', 'NorESM2-MM', 'GFDL-ESM4', 'TaiESM1']\n",
    "# for model in model:\n",
    "#     plt_2dhist_iwp_sf(_iwp.sel(model=model), _sf.sel(model='NorESM2-MM'), model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee55e9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32122631",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcf0a61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4af80cc0",
   "metadata": {},
   "source": [
    "## Find mixed-phase clouds\n",
    "Calculate the IWC/LWC statistics given by the values. Setting the value to 0.5 will find the level in the atmosphere where IWC and LWC are 50/50. Setting it to a value of 0.7 will find the level where IWC is 70% while LWC is 30%.\n",
    "\n",
    "1. find the fraction of IWC to LWC \n",
    "$$fraction = \\frac{IWC}{IWC + LWC}$$\n",
    "2. find the nearest value to given IWC-fraction\n",
    "3. find atmospheric pressure levels where IWC/LWC fraction occurs\n",
    "4. find the index of the first atmospheric pressure level\n",
    "5. use the index to select variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3310bf",
   "metadata": {},
   "source": [
    "## Create dictionary from the list of datasets we want to use for the IWC/LWC statistics\n",
    "Calculate the IWC/LWC statistics given by the values. Setting the value to 0.5 will find the level in the atmosphere where IWC and LWC are 50/50. Setting it to a value of 0.7 will find the level where IWC is 70% while LWC is 30%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf305f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _ds = list(dset_dict2.values())\n",
    "# _coord = list(dset_dict2.keys())\n",
    "# ds_cmip_1deg = xr.concat(objs=_ds, dim=_coord, coords=\"all\").rename({'concat_dim':'stat'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ec6513",
   "metadata": {},
   "source": [
    "# 3. Exploratory Data Analysis <a id='exploratory'></a>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07569b8",
   "metadata": {},
   "source": [
    "## Create seasonal mean of all regridded models\n",
    "...and plot seasonal mean of each individual model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7133bdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for var_id in list(ds_cmip.keys()):\n",
    "#     ds_cmip = fct.seasonal_mean_std(ds_cmip, var_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309b2273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # extract the variable name similar to ERA5 for plotting\n",
    "# var = fct.to_era_variable[variable_id[variable_id.index('prsn')]]\n",
    "\n",
    "# for model in ds_cmip.model.values:\n",
    "#     fct.plt_spatial_seasonal_mean(ds_cmip[variable_id[variable_id.index('prsn')]+'_season_mean'].sel(model=model), var, title='{} MEAN ({} - {})'.format(model,starty, endy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13221e8b",
   "metadata": {},
   "source": [
    "## Create model mean/spread of seasonal mean of all regridded models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f08410d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_cmip[variable_id[variable_id.index('prsn')]+'_season_model_mean'] = ds_cmip[variable_id[variable_id.index('prsn')]+'_season_mean'].mean('model', keep_attrs=True, skipna = True)\n",
    "# ds_cmip[variable_id[variable_id.index('prsn')]+'_season_model_std']  = ds_cmip[variable_id[variable_id.index('prsn')]+'_season_mean'].std('model', keep_attrs=True, skipna = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e90a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axs, im = fct.plt_spatial_seasonal_mean(ds_cmip[variable_id[variable_id.index('prsn')]+'_season_model_mean'], var, add_colorbar=False, title='CMIP6 - high resolution (1985 - 2014)')\n",
    "\n",
    "# fig.subplots_adjust(right=0.8)\n",
    "# cbar_ax = fig.add_axes([1, 0.15, 0.025, 0.7])\n",
    "# cb = fig.colorbar(im, cax=cbar_ax, orientation=\"vertical\", fraction=0.046, pad=0.04)\n",
    "# cb.set_label(label='MEAN - {}'.format(fct.plt_dict[var][fct.plt_dict['header'].index('label')]), weight='bold')\n",
    "\n",
    "# plt.tight_layout()\n",
    "\n",
    "\n",
    "# axs[2].text(1,-0.12, ds_cmip.model.values.tolist()[0:5], size=12, ha=\"center\", \n",
    "#          transform=axs[2].transAxes, bbox ={'facecolor':'green',\n",
    "#                 'alpha':0.6,\n",
    "#                 'pad':5})\n",
    "# if len(ds_cmip.model.values.tolist()) > 4:\n",
    "#     axs[2].text(1,-0.25, ds_cmip.model.values.tolist()[5:10], size=12, ha=\"center\", \n",
    "#             transform=axs[2].transAxes, bbox ={'facecolor':'green',\n",
    "#                     'alpha':0.6,\n",
    "#                     'pad':5})\n",
    "# if len(ds_cmip.model.values.tolist()) > 10:\n",
    "#     axs[2].text(1,-0.38, ds_cmip.model.values.tolist()[10:-1], size=12, ha=\"center\", \n",
    "#             transform=axs[2].transAxes, bbox ={'facecolor':'green',\n",
    "#                     'alpha':0.6,\n",
    "#                     'pad':5})\n",
    "    \n",
    "\n",
    "# # save figure to png\n",
    "# figdir = '/uio/kant/geo-metos-u1/franzihe/Documents/Figures/CMIP6/'\n",
    "# figname = '{}_season_mean_1deg_{}_{}.png'.format(variable_id[0], starty, endy)\n",
    "# plt.savefig(figdir + figname, format = 'png', bbox_inches = 'tight', transparent = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996c1702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axs, im = fct.plt_spatial_seasonal_mean(ds_cmip[variable_id[variable_id.index('prsn')]+'_season_model_mean'], var, add_colorbar=False, title='CMIP6 - high resolution (1985 - 2014)')\n",
    "\n",
    "# fig.subplots_adjust(right=0.8)\n",
    "# cbar_ax = fig.add_axes([1, 0.15, 0.025, 0.7])\n",
    "# cb = fig.colorbar(im, cax=cbar_ax, orientation=\"vertical\", fraction=0.046, pad=0.04)\n",
    "# cb.set_label(label='MEAN - {}'.format(fct.plt_dict[var][fct.plt_dict['header'].index('label')]), weight='bold')\n",
    "\n",
    "\n",
    "\n",
    "# for ax, i in zip(axs, ds_cmip[variable_id[variable_id.index('prsn')]+'_season_model_std'].season):\n",
    "#     sm = ds_cmip[variable_id[variable_id.index('prsn')]+'_season_model_std'].sel(season=i).plot.contour(ax=ax, transform=ccrs.PlateCarree(), \n",
    "#                                                                       robust=True,\n",
    "#                                                                       vmin = fct.plt_dict[var][fct.plt_dict['header'].index('vmin_std')], \n",
    "#                                                                       vmax = fct.plt_dict[var][fct.plt_dict['header'].index('vmax_std')],\n",
    "#                                                                        levels = 6,\n",
    "#                                                                       cmap=cm.lajolla,\n",
    "#                                                                       add_colorbar=False)\n",
    "    \n",
    "# cbar_ax = fig.add_axes([1.10, 0.15, 0.025, 0.7])\n",
    "# sb = fig.colorbar(sm, cax=cbar_ax, orientation=\"vertical\", fraction=0.046, pad=0.04)\n",
    "# sb.set_label(label='STD - {}'.format(fct.plt_dict[var][fct.plt_dict['header'].index('label')]), weight='bold')\n",
    "\n",
    "\n",
    "# plt.tight_layout()\n",
    "\n",
    "\n",
    "# axs[2].text(1,-0.12, ds_cmip.model.values.tolist()[0:5], size=12, ha=\"center\", \n",
    "#          transform=axs[2].transAxes, bbox ={'facecolor':'green',\n",
    "#                 'alpha':0.6,\n",
    "#                 'pad':5})\n",
    "# if len(ds_cmip.model.values.tolist()) > 4:\n",
    "#     axs[2].text(1,-0.25, ds_cmip.model.values.tolist()[5:10], size=12, ha=\"center\", \n",
    "#             transform=axs[2].transAxes, bbox ={'facecolor':'green',\n",
    "#                     'alpha':0.6,\n",
    "#                     'pad':5})\n",
    "# if len(ds_cmip.model.values.tolist()) > 10:\n",
    "#     axs[2].text(1,-0.38, ds_cmip.model.values.tolist()[10:-1], size=12, ha=\"center\", \n",
    "#             transform=axs[2].transAxes, bbox ={'facecolor':'green',\n",
    "#                     'alpha':0.6,\n",
    "#                     'pad':5})\n",
    "# # save figure to png\n",
    "# figdir = '/uio/kant/geo-metos-u1/franzihe/Documents/Figures/CMIP6/'\n",
    "# figname = '{}_season_mean_std_1deg_{}_{}.png'.format(variable_id[0], starty, endy)\n",
    "# plt.savefig(figdir + figname, format = 'png', bbox_inches = 'tight', transparent = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b2fd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save to netcdf\n",
    "# filename = '{}_1deg_{}01_{}12.nc'.format(variable_id[0], starty, endy)\n",
    "# savepath = '/scratch/franzihe/output/CMIP6_hist/1deg/'\n",
    "# nc_out = savepath + filename\n",
    "# files = glob(nc_out)\n",
    "\n",
    "# counter = 0 \n",
    "# # Save to netcdf file\n",
    "# if nc_out in files:\n",
    "# #     print('{} is downloaded'.format(nc_out))\n",
    "# #     counter += 1\n",
    "# #     print('Have saved in total: {:} files'.format(str(counter)))\n",
    "# # else:\n",
    "#     ds_cmip.to_netcdf(nc_out)\n",
    "#     print('file written: .{}'.format(nc_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3add6210",
   "metadata": {},
   "source": [
    "# References <a id='references'></a>\n",
    "\n",
    "\n",
    "[1] Zelinka, M. D., Myers, T. A., McCoy, D. T., Po-Chedley, S., Caldwell, P. M., Ceppi, P., et al. (2020). Causes of higher climate sensitivity in CMIP6 models. Geophysical Research Letters, 47, e2019GL085782. https://doi-org.ezproxy.uio.no/10.1029/2019GL085782 \n",
    "\n",
    "[2] Bjordal, J., Storelvmo, T., AlterskjÃ¦r, K. et al. Equilibrium climate sensitivity above 5âÂ°C plausible due to state-dependent cloud feedback. Nat. Geosci. 13, 718â721 (2020). https://doi-org.ezproxy.uio.no/10.1038/s41561-020-00649-1 \n",
    "\n",
    "[3] Wu, T., Lu, Y., Fang, Y., Xin, X., Li, L., Li, W., Jie, W., Zhang, J., Liu, Y., Zhang, L., Zhang, F., Zhang, Y., Wu, F., Li, J., Chu, M., Wang, Z., Shi, X., Liu, X., Wei, M., Huang, A., Zhang, Y., and Liu, X.: The Beijing Climate Center Climate System Model (BCC-CSM): the main progress from CMIP5 to CMIP6 , Geosci. Model Dev., 12, 1573â1600, https://doi.org/10.5194/gmd-12-1573-2019, 2019. \n",
    "\n",
    "[4] Lee, W.-L., Wang, Y.-C., Shiu, C.-J., Tsai, I., Tu, C.-Y., Lan, Y.-Y., Chen, J.-P., Pan, H.-L., and Hsu, H.-H.: Taiwan Earth System Model Version 1: description and evaluation of mean state, Geosci. Model Dev., 13, 3887â3904, https://doi.org/10.5194/gmd-13-3887-2020, 2020. \n",
    "\n",
    "[5] Bian HE, Yongqiang YU, Qing BAO, Pengfei LIN, Hailong LIU, Jinxiao LI, Lei WANG, Yimin LIU, Guoxiong WU, Kangjun CHEN, Yuyang GUO, Shuwen ZHAO, Xiaoqi ZHANG, Mirong SONG & Jinbo XIE (2020) CAS FGOALS-f3-L model dataset descriptions for CMIP6 DECK experiments, Atmospheric and Oceanic Science Letters, 13:6, 582-588, DOI: 10.1080/16742834.2020.1778419 \n",
    "\n",
    "[6] Cherchi, A., Fogli, P. G., Lovato, T., Peano, D., Iovino, D., Gualdi, S., et al. (2019). Global mean climate and main patterns of variability in the CMCC-CM2 coupled model. Journal of Advances in Modeling Earth Systems, 11, 185â 209. https://doi-org.ezproxy.uio.no/10.1029/2018MS001369 \n",
    "\n",
    "[7] van Noije, T., Bergman, T., Le Sager, P., O'Donnell, D., Makkonen, R., GonÃ§alves-Ageitos, M., DÃ¶scher, R., Fladrich, U., von Hardenberg, J., Keskinen, J.-P., Korhonen, H., Laakso, A., Myriokefalitakis, S., Ollinaho, P., PÃ©rez GarcÃ­a-Pando, C., Reerink, T., SchrÃ¶dner, R., Wyser, K., and Yang, S.: EC-Earth3-AerChem: a global climate model with interactive aerosols and atmospheric chemistry participating in CMIP6 , Geosci. Model Dev., 14, 5637â5668, https://doi.org/10.5194/gmd-14-5637-2021, 2021. \n",
    "\n",
    "[8] Golaz, J.-C., Caldwell, P. M., Van Roekel, L. P., Petersen, M. R., Tang, Q., Wolfe, J. D., et al. (2019). The DOE E3SM coupled model version 1: Overview and evaluation at standard resolution. Journal of Advances in Modeling Earth Systems, 11, 2089â 2129. https://doi-org.ezproxy.uio.no/10.1029/2018MS001603 \n",
    "\n",
    "[9] Burrows, S. M., Maltrud, M., Yang, X., Zhu, Q., Jeffery, N., Shi, X., et al. (2020). The DOE E3SM v1.1 biogeochemistry configuration: Description and simulated ecosystem-climate responses to historical changes in forcing. Journal of Advances in Modeling Earth Systems, 12, e2019MS001766. https://doi-org.ezproxy.uio.no/10.1029/2019MS001766 \n",
    "\n",
    "[10] MÃ¼ller, W. A., Jungclaus, J. H., Mauritsen, T., Baehr, J., Bittner, M., Budich, R., et al. (2018). A higher-resolution version of the Max Planck Institute Earth System Model (MPI-ESM1.2-HR). Journal of Advances in Modeling Earth Systems, 10, 1383â 1413. https://doi-org.ezproxy.uio.no/10.1029/2017MS001217 \n",
    "\n",
    "[11] Yukimoto, S., H. Kawai, T. Koshiro, N. Oshima, K. Yoshida, S. Urakawa, H. Tsujino, M. Deushi, T. Tanaka, M. Hosaka, S. Yabu, H. Yoshimura, E. Shindo, R. Mizuta, A. Obata, Y. Adachi, and M. Ishii, 2019: The Meteorological Research Institute Earth System Model version 2.0, MRI-ESM2.0: Description and basic evaluation of the physical component. J. Meteor. Soc. Japan, 97, 931â965, doi:10.2151/jmsj.2019-051.\n",
    "\n",
    "[12] Seland, Ã., Bentsen, M., OliviÃ©, D., Toniazzo, T., Gjermundsen, A., Graff, L. S., Debernard, J. B., Gupta, A. K., He, Y.-C., KirkevÃ¥g, A., Schwinger, J., Tjiputra, J., Aas, K. S., Bethke, I., Fan, Y., Griesfeller, J., Grini, A., Guo, C., Ilicak, M., Karset, I. H. H., Landgren, O., Liakka, J., Moseid, K. O., Nummelin, A., Spensberger, C., Tang, H., Zhang, Z., Heinze, C., Iversen, T., and Schulz, M.: Overview of the Norwegian Earth System Model (NorESM2) and key climate response of CMIP6 DECK, historical, and scenario simulations, Geosci. Model Dev., 13, 6165â6200, https://doi.org/10.5194/gmd-13-6165-2020, 2020. \n",
    "\n",
    "[13] Held, I. M., Guo, H., Adcroft, A., Dunne, J. P., Horowitz, L. W., Krasting, J., et al. (2019). Structure and performance of GFDL's CM4.0 climate model. Journal of Advances in Modeling Earth Systems, 11, 3691â 3727. https://doi-org.ezproxy.uio.no/10.1029/2019MS001829 \n",
    "\n",
    "[14] Dunne, J. P., Horowitz, L. W., Adcroft, A. J., Ginoux, P., Held, I. M., John, J. G., et al. (2020). The GFDL Earth System Model Version 4.1 (GFDL-ESM 4.1): Overall coupled model description and simulation characteristics. Journal of Advances in Modeling Earth Systems, 12, e2019MS002015. https://doi-org.ezproxy.uio.no/10.1029/2019MS002015 \n",
    "\n",
    "[15] Park, S., Shin, J., Kim, S., Oh, E., & Kim, Y. (2019). Global Climate Simulated by the Seoul National University Atmosphere Model Version 0 with a Unified Convection Scheme (SAM0-UNICON), Journal of Climate, 32(10), 2917-2949. Retrieved Jan 12, 2022, from https://journals-ametsoc-org.ezproxy.uio.no/view/journals/clim/32/10/jcli-d-18-0796.1.xml\n",
    "\n",
    "[16] Lin, Y., Huang, X., Liang, Y., Qin, Y., Xu, S., & Huang, W., et al. (2020). Community Integrated Earth System Model (CIESM): Description and evaluation. Journal of Advances in Modeling Earth Systems, 12, e2019MS002036. https://doi-org.ezproxy.uio.no/10.1029/2019MS002036 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9188a63c",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1zb0LHvipx8JOXLLrCxzYToJM7eNK4eaw\"  height=\"100\" />\n",
    "<img src=\"https://reliance.rohub.org/static/media/Reliance-logo.433dc2e9.png\"  height=\"100\" />\n",
    "\n",
    "<img src=\"https://www.uio.no/vrtx/decorating/resources/dist/src2/images/footer/uio-logo-en.svg\"  height=\"100\" />\n",
    "<img src=\"https://erc.europa.eu/sites/default/files/logo_0.png\"  height=\"100\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecfbcf9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7f24944080b15318569c1ef785be98f8dd5a0531d3a23558ab9e7edab213d92e"
  },
  "kernelspec": {
   "display_name": "Python 3.10.6 ('globalsnow': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "7f24944080b15318569c1ef785be98f8dd5a0531d3a23558ab9e7edab213d92e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
