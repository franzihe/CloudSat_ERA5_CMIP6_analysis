{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export PYTHONPATH=\"${PYTHONPATH}:/uio/kant/geo-geofag-u1/franzihe/Documents/Python/globalsnow/CloudSat_ERA5_CMIP6_analysis/utils/\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example with CloudSat \n",
    "\n",
    "\n",
    "# Table of Contents\n",
    "<ul>\n",
    "<li><a href=\"#introduction\">1. Introduction</a></li>\n",
    "<li><a href=\"#data_wrangling\">2. Data Wrangling</a></li>\n",
    "<li><a href=\"#exploratory\">3. Exploratory Data Analysis</a></li>\n",
    "<li><a href=\"#conclusion\">4. Conclusion</a></li>\n",
    "<li><a href=\"#references\">5. References</a></li>\n",
    "</ul>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction <a id='introduction'></a>\n",
    "Cloud feedbacks are a major contributor to the spread of climate sensitivity in global climate models (GCMs) ([Zelinka et al. (2020)](https://doi-org.ezproxy.uio.no/10.1029/2019GL085782)]). Among the most poorly understood cloud feedbacks is the one associated with the cloud phase, which is expected to be modified with climate change ([Bjordal et al. (2020)](https://doi-org.ezproxy.uio.no/10.1038/s41561-020-00649-1)). Cloud phase bias, in addition, has significant implications for the simulation of radiative properties and glacier and ice sheet mass balances in climate models. \n",
    "\n",
    "In this context, this work aims to expand our knowledge on how the representation of the cloud phase affects snow formation in GCMs. Better understanding this aspect is necessary to develop climate models further and improve future climate predictions. \n",
    "\n",
    "* Load ERA5 data previously downloaded locally via [Jupyter Notebook - download ERA5](https://github.com/franzihe/download_ERA5)\n",
    "* find clouds: liquid-only, ice-only, mixed-phase\n",
    "* Regridd the ERA5 variables to the same horizontal resolution as high-resolution CMIP6 models with [`xesmf`](https://xesmf.readthedocs.io/en/latest/)\n",
    "* Calculate and plot the seasonal mean of the variable\n",
    "\n",
    "**Questions**\n",
    "* How is the cloud phase and snowfall varying between 1985 and 2014?\n",
    "\n",
    "\n",
    "> **_NOTE:_** We answer questions related to the comparison of CMIP models to ERA5 in another [Jupyter Notebook](../CMIP6_ERA5_CloudSat/plt_seasonal_mean.ipynb)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Wrangling <a id='data_wrangling'></a>\n",
    "\n",
    "This study will compare surface snowfall, ice, and liquid water content from the Coupled Model Intercomparison Project Phase 6 ([CMIP6](https://esgf-node.llnl.gov/projects/cmip6/)) climate models (accessed through [Pangeo](https://pangeo.io/)) to the CloudSat data from **2007 to 2010**, excluding September 2008 and December 2009 (insufficient CALIOP data and CloudSat battery failure, see McIllhattan et al. 2017). We conduct statistical analysis at the annual and seasonal timescales to determine the biases in cloud phase and precipitation (liquid and solid) in the CMIP6 models and their potential connection between them. The CMIP6 data analysis can be found in the [Jupyter Notebook for CMIP6](../cmip/CMIP6_hr_1985-2014.ipynb).\n",
    "\n",
    "- Time period: 2007 to 2010\n",
    "- horizonal nominal resolution: ~250km\n",
    "- time resolution: monthly data \n",
    "- Variables:\n",
    "\n",
    "| shortname     |             Long name                                       |      Units    |  levels |\n",
    "| ------------- |:-----------------------------------------------------------:| -------------:|--------:|\n",
    "| n_obs         |Totoal number of observations ('satellite profiles'), all-sky|| |\n",
    "| n_cld         |Total number of cloudy observations                          |            |  |\n",
    "| n_lcc         |Total number of observations with supercooled liquid cloud (any 'mixed' or 'liquid' flag within profile)       |       |   |\n",
    "| lcc_freq      |n_lcc / n_obs       |       |   |\n",
    "| n_sf_all      |all profiles with confident snowfall (sf >0)          |       |   |\n",
    "| n_sf_all_snow |all profiles with confident snowfall above 0.01 mm/h               |       |   |\n",
    "| n_sf_lcc      |all profiles with supercooled liquid cloud and confident snowfall (sf >0) | |\n",
    "| n_sf_lcc_snow | all profiles with supercooled liquid cloud and confident snowfall above 0.01 mm/h | |\n",
    "| sf_avg_all    | average surface snowfall (mm/h) corresponding to n_sf_all | |\n",
    "| sf_avg_all_snow | average surface snowfall (mm/h) corresponding to n_sf_all_snow | |\n",
    "| sf_avg_lcc    | average surface snowfall (mm/h) corresponding to n_sf_lcc ||\n",
    "| sf_avg_lcc_snow | average surface snowfall (mm/h) corresponding to n_sf_lcc_snow ||\n",
    "| lcc_sf_freq    | n_sf_lcc_snow / n_lcc ||"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organize my data\n",
    "\n",
    "- Define a prefix for my project (you may need to adjust it for your own usage on your infrastructure).\n",
    "    - input folder where all the data used as input to my Jupyter Notebook is stored (and eventually shared)\n",
    "    - output folder where all the results to keep are stored\n",
    "    - tool folder where all the tools\n",
    "\n",
    "The CloudSat data is located in the folder `/input/cloudsat/` and have the format `cc_sf_lcc_{year}_{month}.nc`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mimi.uio.no\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import sys\n",
    "import socket\n",
    "hostname = socket.gethostname()\n",
    "\n",
    "abs_path = str(pathlib.Path(hostname).parent.absolute())\n",
    "WORKDIR = abs_path[:- (len(abs_path.split('/')[-2] + abs_path.split('/')[-1])+1)]\n",
    "\n",
    "\n",
    "if \"mimi\" in hostname:\n",
    "    print(hostname)\n",
    "    DATA_DIR = \"/scratch/franzihe/\"\n",
    "    # FIG_DIR = \"/uio/kant/geo-geofag-u1/franzihe/Documents/Figures/ERA5/\"\n",
    "    FIG_DIR = \"/uio/kant/geo-geofag-u1/franzihe/Documents/Python/globalsnow/CloudSat_ERA5_CMIP6_analysis/Figures/CloudSat/\"\n",
    "elif \"glefsekaldt\" in hostname: \n",
    "    DATA_DIR = \"/home/franzihe/Data/\"\n",
    "    FIG_DIR = \"/home/franzihe/Documents/Figures/CloudSat/\"\n",
    "\n",
    "INPUT_DATA_DIR = os.path.join(DATA_DIR, 'input')\n",
    "OUTPUT_DATA_DIR = os.path.join(DATA_DIR, 'output')\n",
    "UTILS_DIR = os.path.join(WORKDIR, 'utils')\n",
    "\n",
    "sys.path.append(UTILS_DIR)\n",
    "# make figure directory\n",
    "try:\n",
    "    os.mkdir(FIG_DIR)\n",
    "except OSError:\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import python packages\n",
    "- `Python` environment requirements: file [requirements_globalsnow.txt](../../requirements_globalsnow.txt) \n",
    "- load `python` packages from [imports.py](../../utils/imports.py)\n",
    "- load `functions` from [functions.py](../../utils/functions.py)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.core.options.set_options at 0x7f1430365e40>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# supress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # don't output warnings\n",
    "\n",
    "# import packages\n",
    "from imports import(xr, intake, ccrs, cy, plt, glob, cm, fct, np, da, LogNorm, pd, datetime, xe)\n",
    "xr.set_options(display_style='html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open ERA5 variables\n",
    "Get the data requried for the analysis. Beforehand we downloaded the monthly averaged data on single levels and pressure levels via the Climate Data Store (CDS) infrastructure. The github repository [Download ERA5](https://github.com/franzihe/download_ERA5) gives examples on how to download the data from the CDS. We use the Jupyter Notebooks [download_Amon_single_level](https://github.com/franzihe/download_ERA5/blob/main/download_Amon_single_level.ipynb) and [download_Amon_pressure_level](https://github.com/franzihe/download_ERA5/blob/main/download_Amon_pressure_level.ipynb). Both, download the monthly means for the variables mentioned above between 1985 and 2014.\n",
    "\n",
    "> **_NOTE:_** To download from CDS a user has to have a CDS user account, please create the account [here](https://cds.climate.copernicus.eu/user/register).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs_in = os.path.join(INPUT_DATA_DIR, 'cloudsat/')\n",
    "cmip_in = os.path.join(INPUT_DATA_DIR, 'cmip6_hist/single_model')\n",
    "\n",
    "cs_out = os.path.join(OUTPUT_DATA_DIR, 'CS_ERA5_CMIP6/')\n",
    "# make output data directory\n",
    "try:\n",
    "    os.mkdir(cs_out)\n",
    "except OSError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # needed for regridding ERA5 data to CMIP6 grids\n",
    "# cmip_in = os.path.join(INPUT_DATA_DIR, 'cmip6_hist/single_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the moment we have downloaded 4 years (2007 - 2010) for CloudSat. We define start and end year to ensure to only extract the 4-year period between 2007 and 2010.\n",
    "\n",
    "$\\rightarrow$ Define a start and end year\n",
    "\n",
    "We will load all available variables into one xarray dataset with `xarray.open_mfdataset(file)` and select the time range [by name](https://xarray.pydata.org/en/stable/user-guide/indexing.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rsync -av --progress login.nird-lmd.sigma2.no:/projects/NS9600K/tcarlsen/18_Global_Snow_Study/output_files/sf_lcc_monthly/ /scratch/franzihe/input/cloudsat/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "starty = 2007\n",
    "endy = 2010\n",
    "year_range = range(starty, endy + 1)\n",
    "\n",
    "\n",
    "# Uncomment the line below if you want to search for files matching the pattern in the directory specified by `cs_in`\n",
    "# cs_file_in = glob(f'{cs_in}/*_cs5_*.nc')\n",
    "# cs_file_in = sorted(glob(f'{cs_in}/40NS/*_cs5_*.nc'))\n",
    "# cs_file_in = []\n",
    "# cs_file_in.extend(sorted(glob(f'{cs_in}/sf_lcc_monthly/cc_sf_lcc_2007_01.nc')))\n",
    "# cs_file_in.extend(sorted(glob(f'{cs_in}/sf_lcc_monthly{input_file}/cc_sf_lcc_2007_01{input_file}.nc')))\n",
    "\n",
    "# ds_cs = xr.open_mfdataset(cs_file_in, )\n",
    "# Uncomment the line below if you want to rename the variable `t2m` to `2t`\n",
    "# ds_cs = ds_cs.rename_vars({'t2m': '2t'})\n",
    "\n",
    "# ds_cs = ds_cs.sel(time=ds_cs.time.dt.year.isin(year_range)).squeeze()\n",
    "\n",
    "# ds_cs = ds_cs.assign_coords(lon=(((ds_cs.lon + 180) % 360) - 180)).sortby(['lon', 'time'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_time_dim_and_attributes(ds, ds_cs, year, month):\n",
    "    # add time dimension\n",
    "    ds = ds.expand_dims(time=pd.to_datetime([f'{year}-{month}']))\n",
    "            \n",
    "    # #assign attributes to variables\n",
    "    ds['n_obs'] = ds['n_obs'].assign_attrs({'long_name':'Total number of observations (\"satellite profiles\"), all-sky', 'units': ''})\n",
    "    ds['n_cld'] = ds['n_cld'].assign_attrs({'long_name':'Total number of cloudy observations', 'units': ''})\n",
    "    ds['n_lcc'] = ds['n_lcc'].assign_attrs({'long_name':'Total number of observations with supercooled liquid cloud (any \"mixed\" or \"liquid\" flag within profile)', 'units': ''})\n",
    "    ds['lcc_freq'] = ds['lcc_freq'].assign_attrs({'long_name':'n_lcc / n_obs', 'units': ''})\n",
    "    ds['n_sf_all'] = ds['n_sf_all'].assign_attrs({'long_name':'all profiles with confident snowfall (sf >0)', 'units': ''})\n",
    "    ds['n_sf_all_snow'] = ds['n_sf_all_snow'].assign_attrs({'long_name':'all profiles with confident snowfall above 0.01 mm/h', 'units': ''})\n",
    "    ds['n_sf_lcc'] = ds['n_sf_lcc'].assign_attrs({'long_name':'all profiles with supercooled liquid cloud and confident snowfall (sf >0)', 'units': ''})\n",
    "    ds['n_sf_lcc_snow'] = ds['n_sf_lcc_snow'].assign_attrs({'long_name':'all profiles with supercooled liquid cloud and confident snowfall above 0.01 mm/h', 'units': ''})\n",
    "    ds['sf_avg_all'] = ds['sf_avg_all'].assign_attrs({'long_name':'average surface snowfall (mm/h) corresponding to n_sf_all', 'units': '(mm h-1)'})\n",
    "    ds['sf_avg_all_snow'] = ds['sf_avg_all_snow'].assign_attrs({'long_name':'average surface snowfall (mm/h) corresponding to n_sf_all_snow', 'units': '(mm h-1)'})\n",
    "    ds['sf_avg_lcc'] = ds['sf_avg_lcc'].assign_attrs({'long_name':'average surface snowfall (mm/h) corresponding to n_sf_lcc', 'units': '(mm h-1)'})\n",
    "    ds['sf_avg_lcc_snow'] = ds['sf_avg_lcc_snow'].assign_attrs({'long_name':'average surface snowfall (mm/h) corresponding to n_sf_lcc_snow',\n",
    "                                                                'units': '(mm h-1)'})\n",
    "    ds['lcc_sf_freq'] = ds['lcc_sf_freq'].assign_attrs({'long_name':'n_sf_lcc_snow / n_lcc', 'units': ''})\n",
    "    \n",
    "    ds_cs = xr.merge([ds_cs, ds])\n",
    "    \n",
    "    return(ds_cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datetime64(\"2008-09\") and datetime64(\"2009-12\") not found in axis\n"
     ]
    }
   ],
   "source": [
    "# ds_cs       = xr.Dataset()\n",
    "# ds_cs_orig  = xr.Dataset()\n",
    "ds_cs_all = xr.Dataset()\n",
    "ds_cs_orig_all = xr.Dataset()\n",
    "\n",
    "\n",
    "for year in year_range:\n",
    "    for month in np.arange(1,13):\n",
    "        # print(f'{year}-{month}-16')\n",
    "        # cs_file_in = []\n",
    "        # cs_orig_in = []\n",
    "        cs_file_in_all = []\n",
    "        cs_orig_all = []\n",
    "        \n",
    "        if month < 10:\n",
    "            # cs_file_in.extend(sorted(glob(f'{cs_in}/sf_lcc_monthly/cc_sf_lcc_{year}_0{month}.nc')))\n",
    "            # cs_orig_in.extend(sorted(glob(f'{cs_in}/sf_lcc_monthly_no_T_thres/cc_sf_lcc_{year}_0{month}_no_T_thres.nc')))\n",
    "            cs_file_in_all.extend(sorted(glob(f'{cs_in}/sf_lcc_monthly_T_thres_all/cc_sf_lcc_{year}_0{month}_T_thres_all.nc')))\n",
    "            cs_orig_all.extend(sorted(glob(f'{cs_in}/sf_lcc_monthly_no_T_thres_all/cc_sf_lcc_{year}_0{month}_no_T_thres_all.nc')))\n",
    "        else:\n",
    "            # cs_file_in.extend(sorted(glob(f'{cs_in}/sf_lcc_monthly/cc_sf_lcc_{year}_{month}.nc')))\n",
    "            # cs_orig_in.extend(sorted(glob(f'{cs_in}/sf_lcc_monthly_no_T_thres/cc_sf_lcc_{year}_{month}_no_T_thres.nc')))\n",
    "            cs_file_in_all.extend(sorted(glob(f'{cs_in}/sf_lcc_monthly_T_thres_all/cc_sf_lcc_{year}_{month}_T_thres_all.nc')))\n",
    "            cs_orig_all.extend(sorted(glob(f'{cs_in}/sf_lcc_monthly_no_T_thres_all/cc_sf_lcc_{year}_{month}_no_T_thres_all.nc')))\n",
    "        # if len(cs_file_in) != 0:\n",
    "        #     ds      = xr.open_mfdataset(cs_file_in, )\n",
    "        #     ds_cs       = add_time_dim_and_attributes(ds, ds_cs)\n",
    "        # if len(cs_orig_in) != 0:\n",
    "        #     ds_orig = xr.open_mfdataset(cs_orig_in)\n",
    "        #     ds_cs_orig  = add_time_dim_and_attributes(ds_orig, ds_cs_orig)\n",
    "        if len(cs_file_in_all) != 0:\n",
    "            ds_all = xr.open_mfdataset(cs_file_in_all)\n",
    "            ds_cs_all = add_time_dim_and_attributes(ds_all, ds_cs_all, year, month)\n",
    "        if len(cs_orig_all) != 0:\n",
    "            ds_orig_all = xr.open_mfdataset(cs_orig_all)\n",
    "            ds_cs_orig_all = add_time_dim_and_attributes(ds_orig_all, ds_cs_orig_all, year, month)\n",
    "            \n",
    "\n",
    "try: \n",
    "    # ds_cs = ds_cs.drop([np.datetime64('2008-09'), np.datetime64('2009-12')], dim='time')  \n",
    "    # ds_cs_orig = ds_cs_orig.drop([np.datetime64('2008-09'), np.datetime64('2009-12')], dim='time') \n",
    "    ds_cs_all = ds_cs_all.drop([np.datetime64('2008-09'), np.datetime64('2009-12')], dim='time') \n",
    "    ds_cs_orig_all = ds_cs_orig_all.drop([np.datetime64('2008-09'), np.datetime64('2009-12')], dim='time')\n",
    "except:\n",
    "    print('datetime64(\"2008-09\") and datetime64(\"2009-12\") not found in axis') \n",
    "         \n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change attributes matching CMIP6 data\n",
    "\n",
    "We will assign the attributes to the variables to make CMIP6, ERA5, and CloudSat variables comperable.\n",
    "\n",
    "* [`snowfall_rate_sf`] is in **mm h-1** $\\rightarrow$  multiply by **1000kg / 1000 m3** to get **kg m-2 h-1** .\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this optimized version, you define a dictionary conversion_factors that maps each variable ID (var_id) \n",
    "# to its corresponding conversion factor and attributes. The code then iterates over the keys of the dataset \n",
    "# ds_cs and checks if the variable ID exists in the conversion_factors dictionary. If it does, it retrieves \n",
    "# the conversion factor and attributes for that variable and performs the necessary operations.\n",
    "\n",
    "conversion_factors = {\n",
    "    'sf_avg_all': {'factor': 1000./1000., 'attrs': {'units': 'kg m-2 h-1', 'long_name': 'average surface snowfall corresponding to n_sf_all'}},\n",
    "    'sf_avg_all_snow': {'factor': 1000./1000., 'attrs': {'units': 'kg m-2 h-1', 'long_name': 'average surface snowfall corresponding to n_sf_all_snow'}},\n",
    "    'sf_avg_lcc': {'factor': 1000./1000., 'attrs': {'units': 'kg m-2 h-1', 'long_name': 'average surface snowfall corresponding to n_sf_lcc'}},\n",
    "    'sf_avg_lcc_snow': {'factor': 1000./1000., 'attrs': {'units': 'kg m-2 h-1', 'long_name':'average surface snowfall corresponding to n_sf_lcc_snow'}},\n",
    "}\n",
    "\n",
    "\n",
    "for var_id in ds_cs_all.keys():\n",
    "    if var_id in conversion_factors:\n",
    "        factor = conversion_factors[var_id]['factor']\n",
    "        attrs = conversion_factors[var_id]['attrs']\n",
    "\n",
    "        # ds_cs[var_id] = ds_cs[var_id] * factor\n",
    "        # ds_cs[var_id].attrs = attrs\n",
    "        # ds_cs[var_id] = ds_cs[var_id].where(ds_cs[var_id] >= 0., other=np.nan)\n",
    "        \n",
    "        # ds_cs_orig[var_id] = ds_cs_orig[var_id] * factor\n",
    "        # ds_cs_orig[var_id].attrs = attrs\n",
    "        # ds_cs_orig[var_id] = ds_cs_orig[var_id].where(ds_cs_orig[var_id] >= 0., other=np.nan)\n",
    "        \n",
    "        ds_cs_all[var_id] = ds_cs_all[var_id] * factor\n",
    "        ds_cs_all[var_id].attrs = attrs\n",
    "        ds_cs_all[var_id] = (ds_cs_all[var_id].where(ds_cs_all[var_id] >= 0., other=np.nan)).astype(np.float64())\n",
    "        \n",
    "        ds_cs_orig_all[var_id] = ds_cs_orig_all[var_id] * factor\n",
    "        ds_cs_orig_all[var_id].attrs = attrs\n",
    "        ds_cs_orig_all[var_id] = (ds_cs_orig_all[var_id].where(ds_cs_orig_all[var_id] >= 0., other=np.nan)).astype(np.float64())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lcc_freq  \n",
      "n_obs  \n",
      "n_cld  \n",
      "n_lcc  \n",
      "n_sf_all  \n",
      "n_sf_all_snow  \n",
      "n_sf_lcc  \n",
      "n_sf_lcc_snow  \n",
      "lcc_sf_freq  \n",
      "sf_avg_all kg m-2 h-1 kg m-2 h-1\n",
      "sf_avg_all_snow kg m-2 h-1 kg m-2 h-1\n",
      "sf_avg_lcc kg m-2 h-1 kg m-2 h-1\n",
      "sf_avg_lcc_snow kg m-2 h-1 kg m-2 h-1\n"
     ]
    }
   ],
   "source": [
    "for var_id in ds_cs_all.keys():\n",
    "        print(var_id, ds_cs_all[var_id].attrs['units'], ds_cs_orig_all[var_id].attrs['units'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regrid to 500km nominal resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lcc_freq True\n",
      "n_obs True\n",
      "n_cld True\n",
      "n_lcc True\n",
      "n_sf_all True\n",
      "n_sf_all_snow True\n",
      "n_sf_lcc True\n",
      "n_sf_lcc_snow True\n",
      "lcc_sf_freq True\n",
      "sf_avg_all True\n",
      "sf_avg_all_snow True\n",
      "sf_avg_lcc True\n",
      "sf_avg_lcc_snow True\n",
      "lcc_freq True\n",
      "n_obs True\n",
      "n_cld True\n",
      "n_lcc True\n",
      "n_sf_all True\n",
      "n_sf_all_snow True\n",
      "n_sf_lcc True\n",
      "n_sf_lcc_snow True\n",
      "lcc_sf_freq True\n",
      "sf_avg_all True\n",
      "sf_avg_all_snow True\n",
      "sf_avg_lcc True\n",
      "sf_avg_lcc_snow True\n"
     ]
    }
   ],
   "source": [
    "model = 'IPSL-CM5A2-INCA'\n",
    "variable_id = ['areacella']\n",
    "cmip_file_in = glob(f'{cmip_in}/{model}/{variable_id[0]}_fx_*{model}*.nc')\n",
    "\n",
    "ds_cmip = xr.open_dataset(cmip_file_in[0], )#drop_variables=variable_id[0])\n",
    "\n",
    "# Shift longitude to be from -180 to 180\n",
    "ds_cmip = ds_cmip.assign_coords(lon=(((ds_cmip['lon'] + 180) % 360) - 180)).sortby('lon')\n",
    "\n",
    "# cs_in_grid      = fct.regrid_data(ds_cs, ds_cmip)\n",
    "# cs_in_grid_orig = fct.regrid_data(ds_cs_orig, ds_cmip)\n",
    "cs_in_grid_all = fct.regrid_data(ds_cs_all, ds_cmip)\n",
    "cs_in_grid_orig_all = fct.regrid_data(ds_cs_orig_all, ds_cmip)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save CloudSat file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file written: /scratch/franzihe/output/CS_ERA5_CMIP6/orig/cloudsat_250_orig_200701_201012.nc\n",
      "file written: /scratch/franzihe/output/CS_ERA5_CMIP6/lcc_sf/cloudsat_250_lcc_sf_200701_201012.nc\n",
      "file written: /scratch/franzihe/output/CS_ERA5_CMIP6/lcc_2t/cloudsat_250_lcc_2t_200701_201012.nc\n",
      "file written: /scratch/franzihe/output/CS_ERA5_CMIP6/2t/cloudsat_250_2t_200701_201012.nc\n",
      "file written: /scratch/franzihe/output/CS_ERA5_CMIP6/lcc_2t_sf/cloudsat_250_lcc_2t_sf_200701_201012.nc\n",
      "file written: /scratch/franzihe/output/CS_ERA5_CMIP6/lcc_2t_days/cloudsat_250_lcc_2t_days_200701_201012.nc\n"
     ]
    }
   ],
   "source": [
    "model = 'IPSL-CM6A-LR'\n",
    "variable_id = ['areacella']\n",
    "cmip_file_in = glob(f'{cmip_in}/{model}/{variable_id[0]}_fx_*{model}*.nc')\n",
    "\n",
    "ds_cmip = xr.open_mfdataset(cmip_file_in)\n",
    "\n",
    "# Shift longitude to be from -180 to 180\n",
    "ds_cmip = ds_cmip.assign_coords(lon=(((ds_cmip['lon'] + 180) % 360) - 180)).sortby('lon')\n",
    "SH = ds_cmip.sel(lat=slice(-90,-45))\n",
    "NH = ds_cmip.sel(lat=slice(45,90))\n",
    "ds_cmip = xr.concat([SH, NH], 'lat')\n",
    "\n",
    "# SH = ds_cs.sel(lat=slice(-90,-45))\n",
    "# NH = ds_cs.sel(lat=slice(45,90))     \n",
    "# ds_cs = xr.concat([SH, NH], 'lat')\n",
    "\n",
    "# SH = ds_cs_orig.sel(lat=slice(-90,-45))\n",
    "# NH = ds_cs_orig.sel(lat=slice(45,90))     \n",
    "# ds_cs_orig = xr.concat([SH, NH], 'lat')\n",
    "SH = ds_cs_all.sel(lat=slice(-90,-45))\n",
    "NH = ds_cs_all.sel(lat=slice(45,90))     \n",
    "ds_cs_all = xr.concat([SH, NH], 'lat')\n",
    "\n",
    "SH = ds_cs_orig_all.sel(lat=slice(-90,-45))\n",
    "NH = ds_cs_orig_all.sel(lat=slice(45,90))\n",
    "ds_cs_orig_all = xr.concat([SH, NH], 'lat')\n",
    "\n",
    "ds_dict= {'orig': xr.merge([ds_cs_orig_all['n_lcc'], ds_cs_orig_all['n_cld'], ds_cs_orig_all['n_obs'], ds_cmip]),\n",
    "          'lcc_sf': xr.merge([ds_cs_orig_all['n_sf_lcc_snow'], ds_cmip]),\n",
    "        #   'orig': xr.merge([ds_cs_orig['n_lcc'], ds_cs_orig['n_cld'], ds_cs_orig['n_obs'], ds_cmip]),\n",
    "        #   'lcc_sf': xr.merge([ds_cs_orig['n_sf_lcc_snow'], ds_cmip]),\n",
    "          'lcc_2t': xr.merge([ds_cs_all['n_lcc'], ds_cmip]),\n",
    "          '2t' : xr.merge([ds_cs_all['n_cld'], ds_cs_all['n_obs'], ds_cmip]),\n",
    "          'lcc_2t_sf': xr.merge([ds_cs_all['n_sf_lcc_snow'], ds_cmip]),\n",
    "          'lcc_2t_days': xr.merge([ds_cs_all['sf_avg_lcc_snow'], ds_cmip])}\n",
    "\n",
    "starty = 2007; endy = 2010\n",
    "counter = 0\n",
    "for stats, dataset in ds_dict.items():\n",
    "    # print(stats, dataset)\n",
    "    filename = f'cloudsat_250_{stats}_{starty}01_{endy}12.nc'\n",
    "    cs_file_out = os.path.join(cs_out, f'{stats}/{filename}')\n",
    "    \n",
    "    if os.path.exists(cs_file_out):\n",
    "        dataset.to_netcdf(cs_file_out)\n",
    "        print('file written: {}'.format(cs_file_out))\n",
    "            #     print(f'{cs_file_out} is downloaded')\n",
    "            #     counter += 1\n",
    "            #     print(f'Have regridded in total: {counter} files')\n",
    "    else:\n",
    "        dataset.to_netcdf(cs_file_out)\n",
    "        print('file written: {}'.format(cs_file_out))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file written: /scratch/franzihe/output/CS_ERA5_CMIP6/orig/cloudsat_500_orig_200701_201012.nc\n",
      "file written: /scratch/franzihe/output/CS_ERA5_CMIP6/lcc_sf/cloudsat_500_lcc_sf_200701_201012.nc\n",
      "file written: /scratch/franzihe/output/CS_ERA5_CMIP6/lcc_2t/cloudsat_500_lcc_2t_200701_201012.nc\n",
      "file written: /scratch/franzihe/output/CS_ERA5_CMIP6/2t/cloudsat_500_2t_200701_201012.nc\n",
      "file written: /scratch/franzihe/output/CS_ERA5_CMIP6/lcc_2t_sf/cloudsat_500_lcc_2t_sf_200701_201012.nc\n",
      "file written: /scratch/franzihe/output/CS_ERA5_CMIP6/lcc_2t_days/cloudsat_500_lcc_2t_days_200701_201012.nc\n"
     ]
    }
   ],
   "source": [
    "model = 'IPSL-CM5A2-INCA'\n",
    "variable_id = ['areacella']\n",
    "cmip_file_in = glob(f'{cmip_in}/{model}/{variable_id[0]}_fx_*{model}*.nc')\n",
    "\n",
    "ds_cmip = xr.open_mfdataset(cmip_file_in)\n",
    "\n",
    "ds_cmip = ds_cmip.assign_coords(lon=(((ds_cmip['lon'] + 180) % 360) - 180)).sortby('lon')\n",
    "\n",
    "\n",
    "SH = ds_cmip.sel(lat=slice(-90,-45))\n",
    "NH = ds_cmip.sel(lat=slice(45,90))\n",
    "ds_cmip = xr.concat([SH, NH], 'lat')\n",
    "\n",
    "# SH = cs_in_grid.sel(lat=slice(-90,-45))\n",
    "# NH = cs_in_grid.sel(lat=slice(45,90))\n",
    "# cs_in_grid = xr.concat([SH, NH], 'lat')\n",
    "\n",
    "# SH = cs_in_grid_orig.sel(lat=slice(-90,-45))\n",
    "# NH = cs_in_grid_orig.sel(lat=slice(45,90))\n",
    "# cs_in_grid_orig = xr.concat([SH, NH], 'lat')\n",
    "\n",
    "SH = cs_in_grid_all.sel(lat=slice(-90,-45))\n",
    "NH = cs_in_grid_all.sel(lat=slice(45,90))\n",
    "cs_in_grid_all = xr.concat([SH, NH], 'lat')\n",
    "\n",
    "SH = cs_in_grid_orig_all.sel(lat=slice(-90,-45))\n",
    "NH = cs_in_grid_orig_all.sel(lat=slice(45,90))\n",
    "cs_in_grid_orig_all = xr.concat([SH, NH], 'lat')\n",
    "\n",
    "ds_dict= {'orig': xr.merge([cs_in_grid_orig_all['n_lcc'], cs_in_grid_orig_all['n_cld'], cs_in_grid_orig_all['n_obs'], ds_cmip]),\n",
    "          'lcc_sf': xr.merge([cs_in_grid_orig_all['n_sf_lcc_snow'], ds_cmip]),\n",
    "        #   'orig': xr.merge([cs_in_grid_orig['n_lcc'], cs_in_grid_orig['n_cld'], cs_in_grid_orig['n_obs'], ds_cmip]),\n",
    "        #   'lcc_sf': xr.merge([cs_in_grid_orig['n_sf_lcc_snow'], ds_cmip]),\n",
    "          'lcc_2t': xr.merge([cs_in_grid_all['n_lcc'], ds_cmip]),\n",
    "          '2t' : xr.merge([cs_in_grid_all['n_cld'], cs_in_grid_all['n_obs'],ds_cmip]),\n",
    "          'lcc_2t_sf': xr.merge([cs_in_grid_all['n_sf_lcc_snow'], ds_cmip]),\n",
    "          'lcc_2t_days': xr.merge([cs_in_grid_all['sf_avg_lcc_snow'], ds_cmip])}\n",
    "\n",
    "starty = 2007; endy = 2010\n",
    "counter = 0\n",
    "for stats, dataset in ds_dict.items():\n",
    "    # print(stats, dataset)\n",
    "    filename = f'cloudsat_500_{stats}_{starty}01_{endy}12.nc'\n",
    "    cs_file_out = os.path.join(cs_out, f'{stats}/{filename}')\n",
    "    \n",
    "    if os.path.exists(cs_file_out):\n",
    "        dataset.to_netcdf(cs_file_out)\n",
    "        print('file written: {}'.format(cs_file_out))\n",
    "            #     print(f'{cs_file_out} is downloaded')\n",
    "            #     counter += 1\n",
    "            #     print(f'Have regridded in total: {counter} files')\n",
    "    else:\n",
    "        dataset.to_netcdf(cs_file_out)\n",
    "        print('file written: {}'.format(cs_file_out))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1zb0LHvipx8JOXLLrCxzYToJM7eNK4eaw\"  height=\"100\" />\n",
    "<img src=\"https://reliance.rohub.org/static/media/Reliance-logo.433dc2e9.png\"  height=\"100\" />\n",
    "\n",
    "<img src=\"https://www.uio.no/vrtx/decorating/resources/dist/src2/images/footer/uio-logo-en.svg\"  height=\"100\" />\n",
    "<img src=\"https://erc.europa.eu/sites/default/files/logo_0.png\"  height=\"100\" />\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7f24944080b15318569c1ef785be98f8dd5a0531d3a23558ab9e7edab213d92e"
  },
  "kernelspec": {
   "display_name": "Python 3.10.6 ('globalsnow': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7f24944080b15318569c1ef785be98f8dd5a0531d3a23558ab9e7edab213d92e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
