{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of CMIP6, ERA5, and CloudSat\n",
    "\n",
    "\n",
    "# Table of Contents\n",
    "<ul>\n",
    "<li><a href=\"#introduction\">1. Introduction</a></li>\n",
    "<li><a href=\"#data_wrangling\">2. Data Wrangling</a></li>\n",
    "<li><a href=\"#exploratory\">3. Exploratory Data Analysis</a></li>\n",
    "<li><a href=\"#conclusion\">4. Conclusion</a></li>\n",
    "<li><a href=\"#references\">5. References</a></li>\n",
    "</ul>\n",
    "\n",
    "# 1. Introduction <a id='introduction'></a>\n",
    "\n",
    "\n",
    "**Questions**\n",
    "* How is the cloud phase and snowfall \n",
    "\n",
    "\n",
    "> **_NOTE:_** .\n",
    "\n",
    "# 2. Data Wrangling <a id='data_wrangling'></a>\n",
    "\n",
    "\n",
    "## Organize my data\n",
    "\n",
    "- Define a prefix for my project (you may need to adjust it for your own usage on your infrastructure).\n",
    "    - input folder where all the data used as input to my Jupyter Notebook is stored (and eventually shared)\n",
    "    - output folder where all the results to keep are stored\n",
    "    - tool folder where all the tools\n",
    "\n",
    "The ERA5 0.25deg data is located in the folder `\\scratch\\franzihe\\`, CloudSat at ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "lwp_threshold = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mimi.uio.no\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import sys\n",
    "import socket\n",
    "hostname = socket.gethostname()\n",
    "\n",
    "abs_path = str(pathlib.Path(hostname).parent.absolute())\n",
    "WORKDIR = abs_path[:- (len(abs_path.split('/')[-2] + abs_path.split('/')[-1])+1)]\n",
    "\n",
    "\n",
    "if \"mimi\" in hostname:\n",
    "    print(hostname)\n",
    "    DATA_DIR = \"/mn/vann/franzihe/\"\n",
    "    # FIG_DIR = \"/uio/kant/geo-geofag-u1/franzihe/Documents/Figures/ERA5/\"\n",
    "    FIG_DIR = f\"/uio/kant/geo-geofag-u1/franzihe/Documents/Python/globalsnow/CloudSat_ERA5_CMIP6_analysis/Figures/CS_ERA5_CMIP6_hourly_{lwp_threshold}/\"\n",
    "elif \"glefsekaldt\" in hostname: \n",
    "    DATA_DIR = \"/home/franzihe/Data/\"\n",
    "    FIG_DIR = \"/home/franzihe/Documents/Figures/ERA5/\"\n",
    "\n",
    "INPUT_DATA_DIR = os.path.join(DATA_DIR, 'input')\n",
    "OUTPUT_DATA_DIR = os.path.join(DATA_DIR, 'output')\n",
    "UTILS_DIR = os.path.join(WORKDIR, 'utils')\n",
    "FIG_DIR_mci = os.path.join(FIG_DIR, 'McIlhattan/')\n",
    "\n",
    "sys.path.append(UTILS_DIR)\n",
    "# make figure directory\n",
    "try:\n",
    "    os.mkdir(FIG_DIR)\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    os.mkdir(FIG_DIR_mci)\n",
    "except OSError:\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import python packages\n",
    "- `Python` environment requirements: file [requirements_globalsnow.txt](../../requirements_globalsnow.txt) \n",
    "- load `python` packages from [imports.py](../../utils/imports.py)\n",
    "- load `functions` from [functions.py](../../utils/functions.py)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.core.options.set_options at 0x7f034012f220>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# supress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # don't output warnings\n",
    "\n",
    "# import packages\n",
    "from imports import(xr, ccrs, cy, plt, glob, cm, fct, np, pd, add_cyclic_point)\n",
    "# from matplotlib.lines import Line2D\n",
    "# from matplotlib.patches import Patch\n",
    "# from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "xr.set_options(display_style='html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open variables\n",
    "Get the data requried for the analysis. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mn/vann/franzihe/output/CS_ERA5_CMIP6'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat_in = os.path.join(OUTPUT_DATA_DIR, 'CS_ERA5_CMIP6')\n",
    "dat_in\n",
    "# make output data directory\n",
    "# try:\n",
    "#     os.mkdir(dat_out)\n",
    "# except OSError:\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variable_id = ['tas', 'prsn', 'pr', 'lwp', 'clivi', 'areacella']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define list of models\n",
    "list_models = [#'cloudsat_250',\n",
    "               'cloudsat_500',\n",
    "               # 'era_30',\n",
    "               # 'era_250',\n",
    "               'era_500',\n",
    "               # 'cmip_250',\n",
    "               'cmip_500',\n",
    "               # 'MIROC6', \n",
    "               # 'CanESM5', \n",
    "               # 'AWI-ESM-1-1-LR', \n",
    "               # 'MPI-ESM1-2-LR', \n",
    "               # 'UKESM1-0-LL', \n",
    "               # 'HadGEM3-GC31-LL',\n",
    "               # 'CNRM-CM6-1',\n",
    "               # 'CNRM-ESM2-1',\n",
    "               # 'IPSL-CM6A-LR',\n",
    "               # 'IPSL-CM5A2-INCA'\n",
    "            ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty dictionaries to store the Xarray datasets for different variables:\n",
    "variables = ['orig', '2t', 'lcc', 'lcc_2t', 'lcc_sf', 'lcc_2t_days', 'lcc_2t_sf', ]\n",
    "ds = {var: {} for var in variables}\n",
    "ds_mci = {var: {} for var in variables}\n",
    "ds_hourly = {var: {} for var in variables}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for model in list_models:\n",
    "    \n",
    "#     for var in variables:\n",
    "#         if model == 'cloudsat_250' or model == 'cloudsat_500' or var == 'orig' or var == '2t':\n",
    "#             file_pattern = f'{dat_in}/{var}/{model}_{var}*.nc'\n",
    "#         else:\n",
    "#             file_pattern = f'{dat_in}/{lwp_threshold}_{var}/{model}_{lwp_threshold}_{var}*.nc'\n",
    "#         # print(file_pattern)\n",
    "#         files = sorted(glob(file_pattern))\n",
    "#         # print(files)\n",
    "#         for file in files:\n",
    "#             _ds = xr.open_mfdataset(file)\n",
    "#             # [var][model]\n",
    "#             ds[var][model] = xr.Dataset()\n",
    "#             # ds_mci[var][model] = xr.Dataset()\n",
    "#             # make the data cyclic going from -180 to 180\n",
    "#             for var_id in _ds.keys():\n",
    "#                 data = _ds[var_id]\n",
    "                \n",
    "#                 if 'lon' in _ds[var_id].dims and (data['lon'][0] != data['lon'][-1]*(-1)):\n",
    "#                     lon = _ds.coords['lon']\n",
    "#                     lon_idx = data.dims.index('lon')\n",
    "#                     wrap_data, wrap_lon = add_cyclic_point(data, coord=lon, axis=lon_idx)\n",
    "                    \n",
    "#                     if len(wrap_data.shape) == 2:\n",
    "#                         ds[var][model][var_id] = xr.DataArray(data = wrap_data, coords=dict(lat=data['lat'],\n",
    "#                                                                                             lon=np.append(data['lon'].values, data['lon'][0].values*(-1))))\n",
    "                    \n",
    "#                     if len(wrap_data.shape) == 3:\n",
    "#                         if 'time' in data.dims:\n",
    "#                             ds[var][model][var_id] = xr.DataArray(data = wrap_data, coords=dict(time=data['time'],\n",
    "#                                                                                                 lat=data['lat'],\n",
    "#                                                                                                 lon=np.append(data['lon'].values, data['lon'][0].values*(-1))))\n",
    "#                         elif 'model' in data.dims:\n",
    "#                             ds[var][model][var_id] = xr.DataArray(data = wrap_data, coords=dict(lat=data['lat'],\n",
    "#                                                                                                 lon=np.append(data['lon'].values, data['lon'][0].values*(-1)),\n",
    "#                                                                                                 model=data['model']), \n",
    "#                                                                   )\n",
    "#                     if len(wrap_data.shape) == 4:\n",
    "#                         ds[var][model][var_id] = xr.DataArray(data = wrap_data, coords=dict(time=data['time'],\n",
    "#                                                                                             lat=data['lat'],\n",
    "#                                                                                             lon=np.append(data['lon'].values, data['lon'][0].values*(-1)),\n",
    "#                                                                                             model=data['model']))\n",
    "                        \n",
    "#                 else:\n",
    "#                     ds[var][model][var_id] = data\n",
    "                    \n",
    "#                 ds[var][model][var_id].attrs = data.attrs\n",
    "                \n",
    "#             # ds_mci[var][model] = xr.concat([ds[var][model].sel(lat=slice(-90,-66.91)),\n",
    "#             #                                 ds[var][model].sel(lat=slice(66.91,90))], dim ='lat')\n",
    "#             # ds_mci[var][model] = ds[var][model].where(np.logical_and((ds[var][model].lat<=66.91),(ds[var][model].lat>=66.91)), other=np.nan )\n",
    "            \n",
    "# # Access the datasets using  ds[var][model]\n",
    "# # For example:\n",
    "# # lcc_2t_days_dataset = ds['lcc_2t_days']['era_30']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in list_models:\n",
    "# for model in list_models[1:2]:\n",
    "\n",
    "    \n",
    "    for var in variables:\n",
    "        if model == 'cloudsat_250' or model == 'cloudsat_500':\n",
    "            file_pattern = f'{dat_in}/{var}/{model}_{var}*.nc'\n",
    "        elif model == 'era_30' or model == 'era_250' or model == 'era_500':\n",
    "            if var == 'orig' or var == '2t':\n",
    "                file_pattern = f'{dat_in}_hourly/{var}/{model}_{var}*.nc'\n",
    "            else:\n",
    "                file_pattern = f'{dat_in}_hourly/{lwp_threshold}_{var}/{model}_{lwp_threshold}_{var}*.nc'\n",
    "        else:\n",
    "            if var == 'orig' or var == '2t':\n",
    "                file_pattern = f'{dat_in}/{var}/{model}_{var}*.nc'\n",
    "            else:\n",
    "                file_pattern = f'{dat_in}/{lwp_threshold}_{var}/{model}_{lwp_threshold}_{var}*.nc'\n",
    "                \n",
    "                \n",
    "        files = sorted(glob(file_pattern))\n",
    "        # print(files)\n",
    "        for file in files:\n",
    "            _ds_hourly = xr.open_mfdataset(file)\n",
    "            # [var][model]\n",
    "            ds_hourly[var][model] = xr.Dataset()\n",
    "            # ds_hourly_mci[var][model] = xr.Dataset()\n",
    "            # make the data cyclic going from -180 to 180\n",
    "            for var_id in _ds_hourly.keys():\n",
    "                data = _ds_hourly[var_id]\n",
    "                \n",
    "                if 'lon' in _ds_hourly[var_id].dims and (data['lon'][0] != data['lon'][-1]*(-1)):\n",
    "                    lon = _ds_hourly.coords['lon']\n",
    "                    lon_idx = data.dims.index('lon')\n",
    "                    wrap_data, wrap_lon = add_cyclic_point(data, coord=lon, axis=lon_idx)\n",
    "                    \n",
    "                    if len(wrap_data.shape) == 2:\n",
    "                        ds_hourly[var][model][var_id] = xr.DataArray(data = wrap_data, coords=dict(lat=data['lat'],\n",
    "                                                                                            lon=np.append(data['lon'].values, data['lon'][0].values*(-1))))\n",
    "                    \n",
    "                    if len(wrap_data.shape) == 3:\n",
    "                        if 'time' in data.dims:\n",
    "                            ds_hourly[var][model][var_id] = xr.DataArray(data = wrap_data, coords=dict(time=data['time'],\n",
    "                                                                                                lat=data['lat'],\n",
    "                                                                                                lon=np.append(data['lon'].values, data['lon'][0].values*(-1))))\n",
    "                        elif 'model' in data.dims:\n",
    "                            ds_hourly[var][model][var_id] = xr.DataArray(data = wrap_data, coords=dict(lat=data['lat'],\n",
    "                                                                                                lon=np.append(data['lon'].values, data['lon'][0].values*(-1)),\n",
    "                                                                                                model=data['model']), \n",
    "                                                                  )\n",
    "                    if len(wrap_data.shape) == 4:\n",
    "                        ds_hourly[var][model][var_id] = xr.DataArray(data = wrap_data, coords=dict(time=data['time'],\n",
    "                                                                                            lat=data['lat'],\n",
    "                                                                                            lon=np.append(data['lon'].values, data['lon'][0].values*(-1)),\n",
    "                                                                                            model=data['model']))\n",
    "                        \n",
    "                else:\n",
    "                    ds_hourly[var][model][var_id] = data\n",
    "                    \n",
    "                ds_hourly[var][model][var_id].attrs = data.attrs\n",
    "                \n",
    "            # ds_hourly_mci[var][model] = xr.concat([ds_hourly[var][model].sel(lat=slice(-90,-66.91)),\n",
    "            #                                 ds_hourly[var][model].sel(lat=slice(66.91,90))], dim ='lat')\n",
    "            # ds_hourly_mci[var][model] = ds_hourly[var][model].where(np.logical_and((ds_hourly[var][model].lat<=66.91),(ds_hourly[var][model].lat>=66.91)), other=np.nan )\n",
    "            \n",
    "# Access the datasets using  ds_hourly[var][model]\n",
    "# For example:\n",
    "# lcc_2t_days_dataset = ds_hourly['lcc_2t_days']['era_30']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['cloudsat_500', 'era_500', 'cmip_500'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_hourly['orig'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for var in ds.keys():\n",
    "#     for model in ['cloudsat_250', 'cloudsat_500', 'cmip_250', 'cmip_500']:\n",
    "#         # ds_hourly[var][model] = xr.Dataset()\n",
    "#         try: \n",
    "#             ds_hourly[var][model] = ds[var][model]\n",
    "#         except KeyError:\n",
    "#             # print(var, model)\n",
    "#             continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_label = {\n",
    "     # 'lcc_wo_snow': {'cb_label':'FsLCC (%)', 'levels':np.arange(0,110,10), 'vmin': 0, 'vmax':100, 'diff_levels':np.arange(-30,35,5), 'diff_vmin':-30, 'diff_vmax':30},\n",
    "#      'lcc_w_snow':  {'cb_label':'FoS in sLCCs (%)', 'levels':np.arange(0,110,10), 'vmin': 0, 'vmax':100, 'diff_levels':np.arange(-60,65,5), 'diff_vmin':-60, 'diff_vmax':60},\n",
    "#      'sf_eff':      {'cb_label':'SE in sLCCs (h$^{-1}$)', 'levels':np.arange(0,5.5,.5), 'vmin':0, 'vmax':5, 'diff_levels':np.arange(-1.2,1.4,.2), 'diff_vmin':-1.2, 'diff_vmax':1.2}#'Relative snowfall efficiency (h$^{-1}$)'\n",
    "     \n",
    "     'FLCC' : {'cb_label':'FLCC (%)',             'levels':np.arange(0,105.,5.), 'vmin':0, 'vmax': 100.,   'diff_levels':np.arange(-100,110,10),   'diff_vmin':-100, 'diff_vmax':100},\n",
    "     'FsLCC': {'cb_label':'FsLCC (%)',            'levels':np.arange(0,105.,5.), 'vmin':0, 'vmax': 100,   'diff_levels':np.arange(-100,110,10),   'diff_vmin':-100, 'diff_vmax':100},\n",
    "     # # 'FoP'  : {'cb_label':'FoP in LCCs (%)',      'levels':np.arange(0,105.,5.), 'vmin':0, 'vmax': 100,   'diff_levels':np.arange(-100,110,10),   'diff_vmin':-100, 'diff_vmax':100},\n",
    "     # 'FoS'  : {'cb_label':'FoS in sLCCs (%)',     'levels':np.arange(0,105.,5.), 'vmin':0, 'vmax': 100,   'diff_levels':np.arange(-100,110,10),   'diff_vmin':-100, 'diff_vmax':100},\n",
    "     # 'sf_eff': {'cb_label':'SE in sLCCs (h$^{-1}$)','levels':np.arange(0,5.5,.5), 'vmin':0, 'vmax': 5,   'diff_levels':np.arange(-1.2,1.4,.2),   'diff_vmin':-1.2, 'diff_vmax':1.2},\n",
    "     # # 'pr_eff': {'cb_label':'PE in sLCCs (h$^{-1}$)', 'levels':np.arange(0,550.,50.), 'vmin':0, 'vmax':500,   'diff_levels':np.arange(-120,140,20),   'diff_vmin':-120, 'diff_vmax':120},\n",
    "     'FLCC-FsLCC': {'cb_label':'FLCC (%), FsLCC (%)',  'levels':np.arange(0,105.,5.), 'vmin':0, 'vmax': 100,   'diff_levels':np.arange(-100,110,10),   'diff_vmin':-100, 'diff_vmax':100}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig dict_keys([])\n",
      "2t dict_keys([])\n",
      "lcc dict_keys([])\n",
      "lcc_2t dict_keys([])\n",
      "lcc_sf dict_keys([])\n",
      "lcc_2t_days dict_keys([])\n",
      "lcc_2t_sf dict_keys([])\n"
     ]
    }
   ],
   "source": [
    "for var in ds.keys():\n",
    "    print(var, ds[var].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratios = fct.get_ratios_dict(list_models, ds,seasons='normal')\n",
    "ratios_hourly = fct.get_ratios_dict(list_models, ds_hourly,seasons='normal')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for variables in ratios_hourly['cloudsat_500'].variables:\n",
    "    for var_name in dict_label.keys():\n",
    "        if var_name in variables:\n",
    "            # ratios = fct.get_only_valid_values(ratios, '500', variables)\n",
    "            ratios_hourly = fct.get_only_valid_values(ratios_hourly, '500', variables)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Calculate weighted averages\n",
    "\n",
    "# for model in ratios.keys():\n",
    "#     weights = ds['orig'][model]['areacella']\n",
    "#     for vars in ratios[model].keys():\n",
    "#         ratios[model][vars+'_mean'], ratios[model][vars+'_std'], ratios[model][vars+'_stats'] = fct.weighted_average(ratios[model][vars], weights)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in ratios_hourly.keys():\n",
    "    weights_hourly = ds_hourly['orig'][model]['areacella']\n",
    "    for vars in ratios_hourly[model].keys():\n",
    "        ratios_hourly[model][vars+'_mean'], ratios_hourly[model][vars+'_std'], ratios_hourly[model][vars+'_stats'] = fct.weighted_average(ratios_hourly[model][vars], weights_hourly)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratios_hourly['500'] = xr.concat([ratios_hourly['cloudsat_500'].assign_coords(coords={'model':'CloudSat'}),\n",
    "                                  ratios_hourly['era_500'].assign_coords(coords={'model':'ERA5'}),\n",
    "                                  ratios_hourly['cmip_500']], dim=(\"model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dir_hourly = os.path.join(OUTPUT_DATA_DIR, 'CS_ERA5_CMIP6_hourly/ratios_500/')\n",
    "try:\n",
    "    os.mkdir(file_dir_hourly)\n",
    "except OSError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var_name in dict_label.keys():\n",
    "    file_name = f'{var_name}_LWP{lwp_threshold}_2007_2010.nc'\n",
    "    (ratios_hourly['500'][[f'{var_name}_season', f'{var_name}_season_cs_mean',\n",
    "                           f'{var_name}_month_mean', f'{var_name}_year_mean', f'{var_name}_month_cs_mean', f'{var_name}_year_cs_mean',\n",
    "                           f'{var_name}_month_years_mean', f'{var_name}_year_years_mean', f'{var_name}_season_mean',\n",
    "                           f'{var_name}_month',f'{var_name}_year',f'{var_name}_year_cs_mean',\n",
    "                           ]]).to_netcdf(path=os.path.join(file_dir_hourly, file_name), format=\"NETCDF4\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lwp_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "globalsnow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "01a47c9201b0e71806f8317dc994d10479d7bb1c7bfa2fc7a59a724dd50a1c8a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
