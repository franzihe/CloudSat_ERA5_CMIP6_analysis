{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of CMIP6, ERA5, and CloudSat\n",
    "\n",
    "\n",
    "# Table of Contents\n",
    "<ul>\n",
    "<li><a href=\"#introduction\">1. Introduction</a></li>\n",
    "<li><a href=\"#data_wrangling\">2. Data Wrangling</a></li>\n",
    "<li><a href=\"#exploratory\">3. Exploratory Data Analysis</a></li>\n",
    "<li><a href=\"#conclusion\">4. Conclusion</a></li>\n",
    "<li><a href=\"#references\">5. References</a></li>\n",
    "</ul>\n",
    "\n",
    "# 1. Introduction <a id='introduction'></a>\n",
    "\n",
    "\n",
    "**Questions**\n",
    "* How is the cloud phase and snowfall \n",
    "\n",
    "\n",
    "> **_NOTE:_** .\n",
    "\n",
    "# 2. Data Wrangling <a id='data_wrangling'></a>\n",
    "\n",
    "\n",
    "## Organize my data\n",
    "\n",
    "- Define a prefix for my project (you may need to adjust it for your own usage on your infrastructure).\n",
    "    - input folder where all the data used as input to my Jupyter Notebook is stored (and eventually shared)\n",
    "    - output folder where all the results to keep are stored\n",
    "    - tool folder where all the tools\n",
    "\n",
    "The ERA5 0.25deg data is located in the folder `\\scratch\\franzihe\\`, CloudSat at ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import sys\n",
    "import socket\n",
    "hostname = socket.gethostname()\n",
    "\n",
    "abs_path = str(pathlib.Path(hostname).parent.absolute())\n",
    "WORKDIR = abs_path[:- (len(abs_path.split('/')[-2] + abs_path.split('/')[-1])+1)]\n",
    "\n",
    "\n",
    "if \"mimi\" in hostname:\n",
    "    print(hostname)\n",
    "    DATA_DIR = \"/scratch/franzihe/\"\n",
    "    # FIG_DIR = \"/uio/kant/geo-geofag-u1/franzihe/Documents/Figures/ERA5/\"\n",
    "    FIG_DIR = \"/uio/kant/geo-geofag-u1/franzihe/Documents/Python/globalsnow/CloudSat_ERA5_CMIP6_analysis/Figures/CS_ERA5_CMIP6_season_McI/\"\n",
    "elif \"glefsekaldt\" in hostname: \n",
    "    DATA_DIR = \"/home/franzihe/Data/\"\n",
    "    FIG_DIR = \"/home/franzihe/Documents/Figures/ERA5/\"\n",
    "\n",
    "INPUT_DATA_DIR = os.path.join(DATA_DIR, 'input')\n",
    "OUTPUT_DATA_DIR = os.path.join(DATA_DIR, 'output')\n",
    "UTILS_DIR = os.path.join(WORKDIR, 'utils')\n",
    "\n",
    "sys.path.append(UTILS_DIR)\n",
    "# make figure directory\n",
    "try:\n",
    "    os.mkdir(FIG_DIR)\n",
    "except OSError:\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import python packages\n",
    "- `Python` environment requirements: file [requirements_globalsnow.txt](../../requirements_globalsnow.txt) \n",
    "- load `python` packages from [imports.py](../../utils/imports.py)\n",
    "- load `functions` from [functions.py](../../utils/functions.py)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# supress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # don't output warnings\n",
    "\n",
    "# import packages\n",
    "from imports import(xr, ccrs, cy, plt, glob, cm, fct, np, pd, Line2D, Patch, r2_score, LinearSegmentedColormap, BoundaryNorm)\n",
    "# from matplotlib.lines import Line2D\n",
    "# from matplotlib.patches import Patch\n",
    "# from sklearn.metrics import r2_score\n",
    "from cartopy.util import add_cyclic_point\n",
    "\n",
    "\n",
    "xr.set_options(display_style='html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open variables\n",
    "Get the data requried for the analysis. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_in = os.path.join(OUTPUT_DATA_DIR, 'CS_ERA5_CMIP6')\n",
    "\n",
    "# make output data directory\n",
    "# try:\n",
    "#     os.mkdir(dat_out)\n",
    "# except OSError:\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variable_id = ['tas', 'prsn', 'pr', 'lwp', 'clivi', 'areacella']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define list of models\n",
    "list_models = ['cloudsat_250',\n",
    "               'cloudsat_500',\n",
    "               'era_30',\n",
    "               'era_250',\n",
    "               'era_500',\n",
    "               'cmip_250',\n",
    "               'cmip_500',\n",
    "               'MIROC6', \n",
    "               'CanESM5', \n",
    "               'AWI-ESM-1-1-LR', \n",
    "               'MPI-ESM1-2-LR', \n",
    "               'UKESM1-0-LL', \n",
    "               'HadGEM3-GC31-LL',\n",
    "               'CNRM-CM6-1',\n",
    "               'CNRM-ESM2-1',\n",
    "               'IPSL-CM6A-LR',\n",
    "               'IPSL-CM5A2-INCA'\n",
    "            ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty dictionaries to store the Xarray datasets for different variables:\n",
    "variables = ['orig', '2t', 'lcc', 'lcc_2t', 'lcc_sf', 'lcc_2t_days', 'lcc_2t_sf', ]\n",
    "ds = {var: {} for var in variables}\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in list_models:\n",
    "    \n",
    "    for var in variables:\n",
    "        file_pattern = f'{dat_in}/{var}/{model}_*{var}*.nc'\n",
    "        files = sorted(glob(file_pattern))\n",
    "        for file in files:\n",
    "            _ds = xr.open_mfdataset(file)\n",
    "            # [var][model]\n",
    "            ds[var][model] = xr.Dataset()\n",
    "            # make the data cyclic going from -180 to 180\n",
    "            for var_id in _ds.keys():\n",
    "                data = _ds[var_id]\n",
    "                \n",
    "                if 'lon' in _ds[var_id].dims and (data['lon'][0] != data['lon'][-1]*(-1)):\n",
    "                    lon = _ds.coords['lon']\n",
    "                    lon_idx = data.dims.index('lon')\n",
    "                    wrap_data, wrap_lon = add_cyclic_point(data, coord=lon, axis=lon_idx)\n",
    "                    \n",
    "                    if len(wrap_data.shape) == 2:\n",
    "                        ds[var][model][var_id] = xr.DataArray(data = wrap_data, coords=dict(lat=data['lat'],\n",
    "                                                                                            lon=np.append(data['lon'].values, data['lon'][0].values*(-1))))\n",
    "                    \n",
    "                    if len(wrap_data.shape) == 3:\n",
    "                        if 'time' in data.dims:\n",
    "                            ds[var][model][var_id] = xr.DataArray(data = wrap_data, coords=dict(time=data['time'],\n",
    "                                                                                                lat=data['lat'],\n",
    "                                                                                                lon=np.append(data['lon'].values, data['lon'][0].values*(-1))))\n",
    "                        elif 'model' in data.dims:\n",
    "                            ds[var][model][var_id] = xr.DataArray(data = wrap_data, coords=dict(lat=data['lat'],\n",
    "                                                                                                lon=np.append(data['lon'].values, data['lon'][0].values*(-1)),\n",
    "                                                                                                model=data['model']), \n",
    "                                                                  )\n",
    "                    if len(wrap_data.shape) == 4:\n",
    "                        ds[var][model][var_id] = xr.DataArray(data = wrap_data, coords=dict(time=data['time'],\n",
    "                                                                                            lat=data['lat'],\n",
    "                                                                                            lon=np.append(data['lon'].values, data['lon'][0].values*(-1)),\n",
    "                                                                                            model=data['model']))\n",
    "                        \n",
    "                else:\n",
    "                    ds[var][model][var_id] = data\n",
    "                    \n",
    "                ds[var][model][var_id].attrs = data.attrs\n",
    "                        \n",
    "\n",
    "            \n",
    "# Access the datasets using  ds[var][model]\n",
    "# For example:\n",
    "# lcc_2t_days_dataset = ds['lcc_2t_days']['era_30']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calc_stats_model_variation(ds):\n",
    "#     # calculate the model variations\n",
    "    \n",
    "#     quantiles = (ds.quantile([0.25, 0.5, 0.75], dim='model', skipna=True, keep_attrs=False)).squeeze()\n",
    "#     iqr = quantiles.sel(quantile=0.75) - quantiles.sel(quantile=0.25)\n",
    "#     max_val = (quantiles.sel(quantile=0.75) +1.5*iqr).assign_coords({'quantile':'min'})\n",
    "#     min_val = (quantiles.sel(quantile=0.25) +1.5*iqr).assign_coords({'quantile':'max'})\n",
    "#     mean = ds.mean(dim='model', skipna=True, keep_attrs=True).assign_coords({'quantile':'mean'})\n",
    "#     std = ds.std(dim='model', skipna=True, keep_attrs=True).assign_coords({'quantile':'std'})\n",
    "#     stats = xr.concat([max_val, quantiles, min_val, mean], dim='quantile')\n",
    "    \n",
    "#     stats['areacella'] = stats['areacella'].isel(quantile=0).squeeze()\n",
    "#     stats['days_season'] = stats['days_season'].isel(quantile=0, lat=0).squeeze()\n",
    "#     stats['days_month'] = stats['days_month'].isel(quantile=0, lat=0).squeeze()\n",
    "    \n",
    "#     return(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key in ds.keys():\n",
    "#     ds[key]['cmip_250_stats'] = calc_stats_model_variation(ds[key]['cmip_250'])\n",
    "#     ds[key]['cmip_500_stats'] = calc_stats_model_variation(ds[key]['cmip_500'])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_stats(data, weights, lat_slice):\n",
    "        weighted_data = data.sel(lat=lat_slice).weighted(weights)\n",
    "        mean = weighted_data.mean(('lat', 'lon'), skipna=True, keep_attrs=False)\n",
    "        std = weighted_data.std(('lat', 'lon'), skipna=True, keep_attrs=False)\n",
    "        if 'quantile' not in data.coords:\n",
    "                \n",
    "                quantiles = weighted_data.quantile([0.25, 0.5, 0.75], dim=('lat', 'lon'), skipna=True, keep_attrs=False)\n",
    "                iqr = quantiles.sel(quantile=0.75) - quantiles.sel(quantile=0.25)\n",
    "                max_val = (quantiles.sel(quantile=0.75) + 1.5 * iqr).assign_coords({'quantile': 'min'})\n",
    "                min_val = (quantiles.sel(quantile=0.25) - 1.5 * iqr).assign_coords({'quantile': 'max'})\n",
    "                stats = xr.concat([max_val, quantiles, min_val], dim='quantile')\n",
    "                \n",
    "        else:\n",
    "                stats = xr.DataArray()\n",
    "        \n",
    "        return mean, std, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_average(data, weights):\n",
    "\n",
    "    lat_north = slice(45, 90)\n",
    "    lat_south = slice(-90, -45)\n",
    "    \n",
    "    NH_mean, NH_std, NH_stats = calculate_stats(data, weights, lat_north)\n",
    "    SH_mean, SH_std, SH_stats = calculate_stats(data, weights, lat_south)\n",
    "    \n",
    "    mean = xr.concat([NH_mean, SH_mean], pd.Index(['NH', 'SH'], name=\"hemisphere\"))\n",
    "    std = xr.concat([NH_std, SH_std], pd.Index(['NH', 'SH'], name=\"hemisphere\"))\n",
    "    stats = xr.concat([NH_stats, SH_stats], pd.Index(['NH', 'SH'], name='hemisphere'))\n",
    "    \n",
    "    return mean, std, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ratios_season_month(var1, var2, weights, stats, out_var,var3=None ):\n",
    "    \n",
    "    ratios = xr.Dataset()\n",
    "    \n",
    "    if var3 == None:\n",
    "        var3 = xr.DataArray()\n",
    "    \n",
    "    # fseason = var1.groupby('time.season')\n",
    "    \n",
    "    fseason = {'NDJ':xr.concat([var1.sel(time=fct.is_season(var1['time.month'], 11,12)),\n",
    "                                var1.sel(time=fct.is_season(var1['time.month'], 1,1))], dim='time'),\n",
    "               'MJJ':var1.sel(time=fct.is_season(var1['time.month'], 5,7)),\n",
    "               'FMA':var1.sel(time=fct.is_season(var1['time.month'], 2,4)), \n",
    "               'ASO':var1.sel(time=fct.is_season(var1['time.month'], 8,10))}\n",
    "    \n",
    "    fmonth = var1.groupby('time.month')\n",
    "    \n",
    "    if 'time' in var2.dims:\n",
    "        # nseason = var2.groupby('time.season')\n",
    "        nseason = {'NDJ':xr.concat([var2.sel(time=fct.is_season(var2['time.month'], 11, 12)),\n",
    "                                    var2.sel(time=fct.is_season(var2['time.month'], 1, 1))], dim='time'),\n",
    "                   'MJJ':var2.sel(time=fct.is_season(var2['time.month'], 5, 7)),\n",
    "                   'FMA':var2.sel(time=fct.is_season(var2['time.month'], 2, 4)),\n",
    "                   'ASO':var2.sel(time=fct.is_season(var2['time.month'], 8, 10))}\n",
    "        nmonth = var2.groupby('time.month')\n",
    "\n",
    "    if stats == 'count':\n",
    "        # per season\n",
    "        # if 'season' in var2.dims:\n",
    "        #     ratios[out_var+'_season'] = fseason.count(dim='time', keep_attrs=False) / var2\n",
    "            \n",
    "        if 'time' in var2.dims:\n",
    "            ratios[out_var+'_season'] = xr.concat([fseason['NDJ'].count(dim='time', keep_attrs=False) / nseason['NDJ'].count(dim='time', keep_attrs=False),\n",
    "                                                   fseason['MJJ'].count(dim='time', keep_attrs=False) / nseason['MJJ'].count(dim='time', keep_attrs=False),\n",
    "                                                   fseason['FMA'].count(dim='time', keep_attrs=False) / nseason['FMA'].count(dim='time', keep_attrs=False),\n",
    "                                                   fseason['ASO'].count(dim='time', keep_attrs=False) / nseason['ASO'].count(dim='time', keep_attrs=False)], \n",
    "                                                  dim='season')\n",
    "            ratios[out_var+'_season'] = ratios[out_var+'_season'].assign_coords({'season':['NDJ', 'MJJ', 'FMA', 'ASO']})\n",
    "            \n",
    "            \n",
    "            ratios[out_var+'_month'] = fmonth.count(dim='time', keep_attrs=False) / nmonth.count(dim='time', keep_attrs=False)\n",
    "            # all years\n",
    "            ratios[out_var+'_year'] = var1.count(dim='time', keep_attrs=False) / var2.count(dim='time', keep_attrs=False)\n",
    "        \n",
    "            \n",
    "        # per month\n",
    "        if 'month' in var3.dims:\n",
    "            ratios[out_var+'_month'] = fmonth.count(dim='time', keep_attrs=False) / var3\n",
    "        elif 'time' in var3.dims:\n",
    "            # all years\n",
    "            ratios[out_var+'_year'] = var1.count(dim='time', keep_attrs=False) / var3.count(dim='time', keep_attrs=False)\n",
    "    \n",
    "    elif stats == 'sum':\n",
    "        # per season\n",
    "        ratios[out_var +'_season'] = xr.concat([fseason['NDJ'].sum(dim='time', skipna=True, keep_attrs=False) / nseason['NDJ'].sum(dim='time', skipna=True, keep_attrs=False),\n",
    "                                                          fseason['MJJ'].sum(dim='time', skipna=True, keep_attrs=False) / nseason['MJJ'].sum(dim='time', skipna=True, keep_attrs=False),\n",
    "                                                          fseason['FMA'].sum(dim='time', skipna=True, keep_attrs=False) / nseason['FMA'].sum(dim='time', skipna=True, keep_attrs=False),\n",
    "                                                          fseason['ASO'].sum(dim='time', skipna=True, keep_attrs=False) / nseason['ASO'].sum(dim='time', skipna=True, keep_attrs=False),],\n",
    "                                                  dim='season',)\n",
    "        ratios[out_var+'_season'] = ratios[out_var+'_season'].assign_coords({'season':['NDJ', 'MJJ', 'FMA', 'ASO']})\n",
    "        # per month\n",
    "        ratios[out_var +'_month']  = (fmonth.sum(dim='time', skipna=True, keep_attrs=False)) / nmonth.sum(dim='time', skipna=True, keep_attrs=False)\n",
    "        # all years\n",
    "        ratios[out_var +'_year'] = (var1.sum(dim='time', skipna=True, keep_attrs=False)) / (var2.sum(dim='time', skipna=True, keep_attrs=False))\n",
    "\n",
    "    elif stats == 'mean':\n",
    "        # if out_var == 'sf_eff':\n",
    "        # per season\n",
    "        ratios[out_var+'_season'] = xr.concat([(xr.concat([(var1/var2).sel(time=fct.is_season(var1['time.month'], 11,12)),\n",
    "                                                                     (var1/var2).sel(time=fct.is_season(var1['time.month'], 1,1))], dim='time')).mean(dim='time', skipna=True, keep_attrs=False),\n",
    "                                                         ((var1/var2).sel(time=fct.is_season(var1['time.month'], 5,7))).mean(dim='time', skipna=True, keep_attrs=False),\n",
    "                                                         ((var1/var2).sel(time=fct.is_season(var1['time.month'], 2,4))).mean(dim='time', skipna=True, keep_attrs=False),\n",
    "                                                         ((var1/var2).sel(time=fct.is_season(var1['time.month'], 8,10))).mean(dim='time', skipna=True, keep_attrs=False)],\n",
    "                                                 dim='season')\n",
    "        ratios[out_var+'_season'] = ratios[out_var+'_season'].assign_coords({'season':['NDJ', 'MJJ', 'FMA', 'ASO']})\n",
    "        # per month\n",
    "        ratios[out_var+'_month'] = (var1/var2).groupby('time.month').mean(dim='time', skipna=True, keep_attrs=False)\n",
    "        \n",
    "        \n",
    "        \n",
    "        ratios[out_var+'_year'] = (var1/var2).mean(dim='time', skipna=True, keep_attrs=False)\n",
    "    \n",
    "    ratios[out_var +'_season'] = ratios[out_var +'_season'].where(ratios[out_var +'_season'] != 0., other = np.nan)\n",
    "    ratios[out_var +'_month'] = ratios[out_var +'_month'].where(ratios[out_var +'_month'] != 0., other = np.nan)\n",
    "    ratios[out_var +'_year'] = ratios[out_var +'_year'].where(ratios[out_var +'_year'] != 0., other = np.nan)\n",
    "    \n",
    "    if out_var != 'sf_eff':\n",
    "        ratios[out_var +'_season'] = ratios[out_var +'_season']*100\n",
    "        ratios[out_var +'_month'] = ratios[out_var +'_month']*100\n",
    "        ratios[out_var +'_year'] = ratios[out_var +'_year']*100\n",
    "    # for vars in ratios.keys():\n",
    "    #     ratios[vars+'_mean'], ratios[vars+'_std'], ratios[vars+'_stats'] = weighted_average(ratios[vars], weights)\n",
    "\n",
    "    return(ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratios = {}\n",
    "for model in list_models:\n",
    "    if model == 'cloudsat_250' or model == 'cloudsat_500':\n",
    "        ds['lcc_2t_days'][model]['lwp'] = xr.DataArray(np.nan, coords=ds['lcc_2t_days'][model]['sf_avg_lcc_snow'].coords, dims=ds['lcc_2t_days'][model]['sf_avg_lcc_snow'].dims)\n",
    "        ratios[model] = xr.merge(objects = [\n",
    "            # get_ratios_season_month(var1 = ds['lcc_2t'][model]['n_lcc'], var2 = ds['2t'][model]['n_cld'], stats = 'sum', out_var = 'lcc_wo_snow', weights = ds['2t'][model]['areacella']),\n",
    "            get_ratios_season_month(var1 = ds['orig'][model]['n_lcc'], var2 = ds['orig'][model]['n_obs'],stats ='sum', out_var = 'FLCC', weights = ds['orig'][model]['areacella']),\n",
    "            get_ratios_season_month(var1 = ds['lcc_2t'][model]['n_lcc'], var2 = ds['orig'][model]['n_obs'], stats = 'sum', out_var = 'FsLCC', weights = ds['orig'][model]['areacella']), #out_var = 'lcc_wo_snow', weights = ds['2t'][model]['areacella']),\n",
    "            get_ratios_season_month(var1 = ds['lcc_sf'][model]['n_sf_lcc_snow'], var2 = ds['orig'][model]['n_lcc'], stats = 'sum', out_var = 'FoP', weights = ds['orig'][model]['areacella']),\n",
    "            get_ratios_season_month(var1 = ds['lcc_2t_sf'][model]['n_sf_lcc_snow'], var2 = ds['lcc_2t'][model]['n_lcc'], stats='sum', out_var = 'FoS', weights=ds['orig'][model]['areacella']), #out_var='lcc_w_snow', weights=ds['lcc_2t'][model]['areacella']),\n",
    "            get_ratios_season_month(var1 = ds['lcc_2t_days'][model]['sf_avg_lcc_snow'], var2 = ds['lcc_2t_days'][model]['lwp'], stats = 'mean', out_var = 'sf_eff', weights = ds['orig'][model]['areacella'])\n",
    "        ])\n",
    "    else:\n",
    "        ratios[model] = xr.merge(objects=[\n",
    "            # get_ratios_season_month(var1=ds['lcc_2t'][model]['lwp'], var2=ds['2t'][model]['twp'].where(ds['2t'][model]['twp']>0.), stats='count', out_var='lcc_wo_snow', weights=ds['2t'][model]['areacella']),              # relative frequency of liquid containing clouds in relation to when there is a cloud\n",
    "            # get_ratios_season_month(var1=ds['lcc_2t'][model]['lwp'], var2=ds['2t'][model]['tas'], stats='count', out_var='lcc_wo_snow', weights=ds['2t'][model]['areacella']), # sLCC frequency compared to all observations when T<0C\n",
    "            ## use of 'tas' in var2 as this has values everywhere where data is valid, while 'lwp' or 'prsn' might not have values\n",
    "            get_ratios_season_month(var1=ds['lcc'][model]['lwp'], var2=ds['orig'][model]['tas'], stats='count', out_var='FLCC', weights=ds['orig'][model]['areacella']),\n",
    "            get_ratios_season_month(var1=ds['lcc_2t'][model]['lwp'], var2=ds['orig'][model]['tas'], stats='count', out_var='FsLCC', weights=ds['orig'][model]['areacella']),#out_var='lcc_wo_snow', weights=ds['2t'][model]['areacella']), # sLCC frequency compared to all observations when T<0C\n",
    "            get_ratios_season_month(var1=ds['lcc_sf'][model]['prsn'], var2=ds['orig'][model]['tas'], stats='count', out_var='FoP', weights=ds['orig'][model]['areacella']),\n",
    "            get_ratios_season_month(var1=ds['lcc_2t_sf'][model]['prsn'], var2=ds['lcc_2t'][model]['lwp'], stats='count', out_var='FoS', weights=ds['orig'][model]['areacella']), #out_var='lcc_w_snow', weights=ds['lcc_2t'][model]['areacella']),   # relative frequency of snowfall from liquid containing clouds\n",
    "            get_ratios_season_month(var1=ds['lcc_2t_days'][model]['prsn'], var2=ds['lcc_2t_days'][model]['lwp'], stats='mean', out_var='sf_eff', weights=ds['orig'][model]['areacella']),      # relative snowfall (precipitation) efficency\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {\n",
    "     # 'lcc_wo_snow': {'cb_label':'FsLCC (%)', 'levels':np.arange(0,110,10), 'vmin': 0, 'vmax':100, 'diff_levels':np.arange(-30,35,5), 'diff_vmin':-30, 'diff_vmax':30},\n",
    "#      'lcc_w_snow':  {'cb_label':'FoS in sLCCs (%)', 'levels':np.arange(0,110,10), 'vmin': 0, 'vmax':100, 'diff_levels':np.arange(-60,65,5), 'diff_vmin':-60, 'diff_vmax':60},\n",
    "#      'sf_eff':      {'cb_label':'SE in sLCCs (h$^{-1}$)', 'levels':np.arange(0,5.5,.5), 'vmin':0, 'vmax':5, 'diff_levels':np.arange(-1.2,1.4,.2), 'diff_vmin':-1.2, 'diff_vmax':1.2}#'Relative snowfall efficiency (h$^{-1}$)'\n",
    "     \n",
    "     'FLCC' : {'cb_label':'FLCC (%)',             'levels':np.arange(0,110,10), 'vmin':0, 'vmax': 100,   'diff_levels':np.arange(-100,110,10),   'diff_vmin':-100, 'diff_vmax':100},\n",
    "     'FsLCC': {'cb_label':'FsLCC (%)',            'levels':np.arange(0,110,10), 'vmin':0, 'vmax': 100,   'diff_levels':np.arange(-100,110,10),   'diff_vmin':-100, 'diff_vmax':100},\n",
    "     'FoP'  : {'cb_label':'FoP in LCCs (%)',      'levels':np.arange(0,110,10), 'vmin':0, 'vmax': 100,   'diff_levels':np.arange(-100,110,10),   'diff_vmin':-100, 'diff_vmax':100},\n",
    "     'FoS'  : {'cb_label':'FoS in sLCCs (%)',     'levels':np.arange(0,110,10), 'vmin':0, 'vmax': 100,   'diff_levels':np.arange(-100,110,10),   'diff_vmin':-100, 'diff_vmax':100},\n",
    "     'sf_eff': {'cb_label':'SE in sLCCs (h$^{-1}$)','levels':np.arange(0,5.5,.5), 'vmin':0, 'vmax': 5,   'diff_levels':np.arange(-1.2,1.4,.2),   'diff_vmin':-1.2, 'diff_vmax':1.2} }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_only_valid_values(ratios, res, out_var):\n",
    "    for time in ['season', 'month', 'year']:\n",
    "        # for out_var, time in product(d.keys(), times):\n",
    "        #     for cs_key, era_key, cmip_key in zip(cloudsat_keys, era_keys, cmip_keys):\n",
    "        cs_key = f'cloudsat_{res}'\n",
    "        era_key = f'era_{res}'\n",
    "        cmip_key = f'cmip_{res}'\n",
    "        \n",
    "        v1_250 = ratios[cs_key][f'{out_var}_{time}']\n",
    "        v1_250 = v1_250.where(v1_250 != 0., other = np.nan)\n",
    "        \n",
    "        v2_250 = ratios[era_key][f'{out_var}_{time}']\n",
    "        v2_250 = v2_250.where(v2_250 != 0., other = np.nan)\n",
    "\n",
    "        \n",
    "        v3_250 = ratios[cmip_key][f'{out_var}_{time}']\n",
    "        v3_250 = v3_250.where(v3_250 != 0., other = np.nan)\n",
    "\n",
    "        v1_era_250 = v1_250.copy()\n",
    "        v1_era_250 = v1_era_250.where(~np.isnan(v2_250))  \n",
    "        ratios[cs_key][f'{out_var}_{time}_era'] = v1_era_250\n",
    "\n",
    "        v1_cmip_250 = v1_250.copy()\n",
    "        v1_cmip_250 = v1_cmip_250.where(~np.isnan(v3_250))\n",
    "        v1_cmip_250 = v1_cmip_250.mean('model',skipna=True)\n",
    "        ratios[cs_key][f'{out_var}_{time}_cmip'] = v1_cmip_250\n",
    "\n",
    "        v2_250_cs = v2_250.copy()\n",
    "        v2_250_cs = v2_250_cs.where(~np.isnan(v1_250))\n",
    "        ratios[era_key][f'{out_var}_{time}_cs'] = v2_250_cs\n",
    "\n",
    "        v3_250_cs = v3_250.copy()\n",
    "        v3_250_cs = v3_250_cs.where(~np.isnan(v1_250))\n",
    "        ratios[cmip_key][f'{out_var}_{time}_cs'] = v3_250_cs\n",
    "            \n",
    "        if out_var == 'sf_eff':\n",
    "            v1_cmip_250 = v2_250.copy()\n",
    "            v1_cmip_250 = v1_cmip_250.where(~np.isnan(v3_250))\n",
    "            v1_cmip_250 = v1_cmip_250.mean('model', skipna=True)\n",
    "            ratios[era_key][f'{out_var}_{time}_cmip'] = v1_cmip_250\n",
    "            \n",
    "            v2_250_era = v2_250.copy()\n",
    "            v2_250_era = v2_250_era.where(~np.isnan(v2_250))\n",
    "            ratios[era_key][f'{out_var}_{time}_era'] = v2_250_era\n",
    "            \n",
    "            v2_250_cmip = v3_250.copy()\n",
    "            v2_250_cmip = v2_250_cmip.where(~np.isnan(v3_250))\n",
    "            ratios[cmip_key][f'{out_var}_{time}_cmip'] = v2_250_cmip\n",
    "            \n",
    "            v3_250_era = v3_250.copy()\n",
    "            v3_250_era = v3_250_era.where(~np.isnan(v2_250))\n",
    "            ratios[cmip_key][f'{out_var}_{time}_era'] = v3_250_era\n",
    "            \n",
    "\n",
    "    return (ratios)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var_name in d.keys():\n",
    "    ratios = get_only_valid_values(ratios, '250', var_name)\n",
    "    ratios = get_only_valid_values(ratios, '500', var_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate weighted averages\n",
    "\n",
    "for model in ratios.keys():\n",
    "    weights = ds['2t'][model]['areacella']\n",
    "    for vars in ratios[model].keys():\n",
    "        ratios[model][vars+'_mean'], ratios[model][vars+'_std'], ratios[model][vars+'_stats'] = weighted_average(ratios[model][vars], weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_label = ['a)', 'b)', 'c)', 'd)', 'e)', 'f)', 'g)', 'h)', 'i)', 'j)', 'k)', 'l)', 'm)', 'n)', 'o)', 'p)', 'q)', 'r)', 's)', 't)', 'u)', 'v)', 'w)', 'x)', 'y)', 'z)',\n",
    "             'aa)', 'bb)', 'cc)', 'dd)', 'ee)', 'ff)', 'gg)', 'hh)', 'ii)', 'jj)', 'kk)', 'll)', 'mm)', 'nn)', 'oo)', 'pp)', 'qq)', 'rr)', 'ss)', 'tt)', 'uu)', 'vv)', 'ww)', 'xx)', 'yy)', 'zz)']\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer from both of us: you calculate the mean value for each grid cell right? Just calculate the standard deviation over all models per grid cell as well (sigma), then devide sigma by the number of models N, that is your standard error SE = sigma/sqrt(N), your confidence interval CI is CI = 1.96*SE (for 95% confidence level), or CI = 2.58*SE (for 99% confidence level). So if you take the difference with ERA, just check if this difference is larger then that confidence interval CI to find significant pixels\n",
    "3:18\n",
    "Some background: https://towardsdatascience.com/confidence-intervals-explained-simply-for-data-scientists-8354a6e2266b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_spatial_season(hemisphere,ds, var_name, dict_label,fig_dir, lat_extent):\n",
    "  if var_name != 'sf_eff':\n",
    "    # val1      = ds['era_500'][var_name + '_season']\n",
    "    val1      = ds['cloudsat_500'][var_name + '_season']\n",
    "    val1      = val1.where(val1 != 0., other=np.nan)\n",
    "    val1      = val1.where(~np.isnan(val1))\n",
    "    # val1_mean = ds['era_500'][var_name + '' +'_season_mean']\n",
    "    val1_mean = ds['cloudsat_500'][var_name + '' + '_season_mean']\n",
    "    \n",
    "    \n",
    "    # val1_500  = ds['cloudsat_250'][var_name + '_season_era']\n",
    "    # val1_500  = val1_500.where(val1_500 != 0., other=np.nan)\n",
    "    # val1_500  = val1_500.where(~np.isnan(val1_500))\n",
    "    # val1_500_mean = ds['cloudsat_250'][var_name + '' +'_season_era_mean']\n",
    "    \n",
    "    val1_500  = ds['cloudsat_500'][var_name + '_season_cmip']\n",
    "    val1_500  = val1_500.where(val1_500 != 0., other=np.nan)\n",
    "    val1_500  = val1_500.where(~np.isnan(val1_500))\n",
    "    val1_500_mean = ds['cloudsat_500'][var_name + '' +'_season_cmip_mean']\n",
    "\n",
    "\n",
    "    val2_500      = ds['era_500'][var_name + '_season_cs']\n",
    "    val2_500 = val2_500.where(val2_500 != 0., other=np.nan)\n",
    "    val2_500 = val2_500.where(~np.isnan(val2_500))\n",
    "    val2_500_mean = ds['era_500'][var_name + '' + '_season_cs_mean']\n",
    "    \n",
    "    diff_era  = val1_500.where(~np.isnan(val2_500)) - val2_500.where(~np.isnan(val1_500))\n",
    "    diff_era_mean = val1_500_mean - val2_500_mean\n",
    "    \n",
    "    val3_500      = ds['cmip_500'][var_name + '_season_cs']\n",
    "    # val3_500 = val3_500.where(val3_500 != 0., other=np.nan)\n",
    "    # val3_500 = val3_500.where(~np.isnan(val3_500))\n",
    "    val3_500_mean = ds['cmip_500'][var_name + '' +'_season_cs_mean']\n",
    "    \n",
    "    model_labels = ['CloudSat (500km)', 'CloudSat - ERA5 (500km)', 'CloudSat - CMIP6$_{mean}$ (500km)', ]\n",
    "  if var_name == 'sf_eff':\n",
    "    val1      = ds['era_500'][var_name + '_season']\n",
    "    val1      = val1.where(val1 != 0., other=np.nan)\n",
    "    val1      = val1.where(~np.isnan(val1))\n",
    "    val1_mean = ds['era_500'][var_name + '' +'_season_mean']\n",
    "  \n",
    "    val1_500  = ds['era_500'][var_name + '_season_cmip']\n",
    "    val1_500  = val1_500.where(val1_500 != 0., other=np.nan)\n",
    "    val1_500  = val1_500.where(~np.isnan(val1_500))\n",
    "    val1_500_mean = ds['era_500'][var_name + '' +'_season_mean']\n",
    "    \n",
    "    val2_500 = ds['cmip_500'][var_name + '_season']\n",
    "    val2_500 = val2_500.where(val2_500 != 0., other = np.nan)\n",
    "    val2_500 = val2_500.where(~np.isnan(val2_500))\n",
    "    val2_500_mean = ds['cmip_500'][var_name + '' + '_season_era_mean']\n",
    "    \n",
    "    diff_era  = val2_500\n",
    "    diff_era_mean = val2_500_mean\n",
    "    \n",
    "    if 'model' in diff_era.coords:\n",
    "      # create model mean\n",
    "      diff_era = diff_era.mean('model', skipna=True, keep_attrs=False)\n",
    "      diff_era = diff_era.where(diff_era != 0., other=np.nan)\n",
    "      diff_era = diff_era.where(~np.isnan(diff_era))\n",
    "      diff_era_mean = diff_era_mean.mean('model',skipna=True, keep_attrs=False)\n",
    "    \n",
    "    # diff_era  = val1_500.where(~np.isnan(val2_500)) - val2_500.where(~np.isnan(val1_500))\n",
    "    # diff_era_mean = val1_500_mean - val2_500_mean\n",
    "    \n",
    "    val3_500      = ds['cmip_500'][var_name + '_season_era']\n",
    "    val3_500 = val3_500.where(val3_500 != 0., other=np.nan)\n",
    "    # val3_500 = val3_500.where(~np.isnan(val3_500))\n",
    "    val3_500_mean = ds['cmip_500'][var_name + '' +'_season_era_mean']\n",
    "    \n",
    "  \n",
    "    model_labels = ['ERA5 (500 km)', 'CMIP6$_{mean}$ (500km)', 'ERA5 - CMIP6$_{mean}$ (500km)', ]\n",
    "  \n",
    "  # if 'model' in val2_500.coords:\n",
    "  #   val2_500      = val2_500.mean('model',skipna=True, keep_attrs=False)\n",
    "  #   val2_500 = val2_500.where(val2_500 != 0., other=np.nan)\n",
    "  #   val2_500 = val2_500.where(~np.isnan(val2_500))\n",
    "  #   val2_500_mean = val2_500_mean.mean('model',skipna=True, keep_attrs=False)\n",
    "  \n",
    "  diff_cmip = val1_500.where(~np.isnan(val3_500)) - val3_500.where(~np.isnan(val1_500))\n",
    "  diff_cmip_mean = val1_500_mean - val3_500_mean\n",
    "  if 'model' in diff_cmip.coords:\n",
    "    \n",
    "    # create model mean\n",
    "    diff_cmip = diff_cmip.mean('model', skipna=True, keep_attrs=False)\n",
    "    diff_cmip = diff_cmip.where(diff_cmip != 0., other=np.nan)\n",
    "    diff_cmip = diff_cmip.where(~np.isnan(diff_cmip))\n",
    "    diff_cmip_mean = diff_cmip_mean.mean('model',skipna=True, keep_attrs=False)\n",
    "    \n",
    "\n",
    "    # create model std\n",
    "    #  https://towardsdatascience.com/confidence-intervals-explained-simply-for-data-scientists-8354a6e2266b\n",
    "    std = val3_500.std(dim='model',skipna=True, keep_attrs=False)\n",
    "    std = std.where(std != 0., other =np.nan)\n",
    "    std = std.where(~np.isnan(std))\n",
    "    # calculate statistic significance dependend on the model spread\n",
    "    SE = std/np.sqrt(len(val3_500['model'])) # standard error\n",
    "    CI = (SE) * 1.96 # for 95% confidence level, or CI=2.58*SE for 99% confidence level\n",
    "  \n",
    "  \n",
    "  \n",
    "\n",
    "  list1 = [val1, diff_era, diff_cmip]\n",
    "           #(val1_500 - val2_500.where(~np.isnan(val1_500))), \n",
    "           \n",
    "\n",
    "  list_glob = [val1_mean, diff_era_mean, diff_cmip_mean\n",
    "               #val1_500_mean - val2_500_mean, \n",
    "               ]\n",
    "  projections = {'NH': ccrs.NorthPolarStereo(central_longitude=0.0, globe=None),\n",
    "                'SH': ccrs.SouthPolarStereo(central_longitude=0.0, globe=None)}\n",
    "\n",
    "  projection = projections[hemisphere]\n",
    "  density=4\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "  f, axsm = plt.subplots(nrows=3, ncols=4, subplot_kw={'projection': projection}, figsize=[12, 9], sharex=True, sharey=True)\n",
    "  coast = cy.feature.NaturalEarthFeature(category='physical', scale='110m', facecolor='none', name='coastline')\n",
    "\n",
    "  rows = model_labels\n",
    "  props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "\n",
    "\n",
    "  for ax, row in zip(axsm[:, 0], rows):\n",
    "      ax.text(-0.07, 0.55, row, va='bottom', ha='center', rotation='vertical', rotation_mode='anchor', transform=ax.transAxes, fontweight='bold')\n",
    "\n",
    "  for ax, k in zip(axsm.flatten(), range(len(fig_label))):\n",
    "      if hemisphere == 'NH':\n",
    "          ax.set_extent([-180, 180, 90, lat_extent], ccrs.PlateCarree())\n",
    "      elif hemisphere == 'SH':\n",
    "          ax.set_extent([-180, 180, -90, -1*(lat_extent)], ccrs.PlateCarree())\n",
    "      ax.add_feature(coast, alpha=0.5)\n",
    "      gl = ax.gridlines(draw_labels=True)\n",
    "      gl.top_labels = False\n",
    "      gl.right_labels = False\n",
    "      ax.text(0.05, 0.95, f'{fig_label[k]}', fontweight='bold', horizontalalignment='left', verticalalignment='top', transform=ax.transAxes)\n",
    "\n",
    "\n",
    "  for ax, k, season in zip(axsm.flatten()[:4], range(len(fig_label)), val1.season):\n",
    "      ax.set(title=f'season = {season.values}')\n",
    "\n",
    "  # for i, (value,mean) in enumerate(zip([\"CS_30\",\"CS_30 - ERA_30\" ,\"CS_250 - CMIP_250\", \"CS_500 - CMIP_500\"],\n",
    "  #                               [\"CS_30_glob\", \"CS_30 - ERA_30_glob\" ,\"CS_250 - CMIP_250_glob\", \"CS_500 - CMIP_500_glob\"])):\n",
    "  for i, (value,hemi_glob) in enumerate(zip(list1,list_glob)):\n",
    "    if i == 0:\n",
    "      cmap = cm.hawaii_r\n",
    "      \n",
    "      cb_label = dict_label['cb_label']\n",
    "      levels = dict_label['levels']\n",
    "      vmin = dict_label['vmin'] \n",
    "      vmax = dict_label['vmax']\n",
    "      \n",
    "      if var_name != 'sf_eff':\n",
    "        cbaxes = f.add_axes([0.92, 0.65, 0.0125, 0.225])\n",
    "      if var_name == 'sf_eff':\n",
    "        cbaxes = f.add_axes([0.92, 0.4, 0.0125, 0.45])\n",
    "    elif i == 1:\n",
    "      sub_title = \"\"\n",
    "      if var_name != 'sf_eff':\n",
    "        cmap = cm.bam\n",
    "        levels = dict_label['diff_levels']\n",
    "        vmin = dict_label['diff_vmin']\n",
    "        vmax = dict_label['diff_vmax']\n",
    "      if var_name == 'sf_eff':\n",
    "        cmap = cm.hawaii_r\n",
    "        levels = dict_label['levels']\n",
    "        vmin = dict_label['vmin']\n",
    "        vmax = dict_label['vmax']\n",
    "    elif i == 2:\n",
    "      cmap = cm.bam\n",
    "      sub_title = \"\"\n",
    "      levels = dict_label['diff_levels']\n",
    "      vmin = dict_label['diff_vmin']\n",
    "      vmax = dict_label['diff_vmax']\n",
    "      if var_name != 'sf_eff':\n",
    "        cbaxes = f.add_axes([0.92, 0.13, 0.0125, 0.45])\n",
    "        cb_label = 'CloudSat - Model (%)'\n",
    "      if var_name == 'sf_eff':\n",
    "        cbaxes = f.add_axes([0.92, 0.13, 0.0125, 0.2])\n",
    "        cb_label = 'ERA5 - Model (h$^{-1}$)'\n",
    "             \n",
    "    for ax, season in zip(axsm.flatten()[i*4: (i+1)*4+1], val1.season):\n",
    "      #   print(i, value,hemi_glob, season.values)\n",
    "        if i == 0:\n",
    "          sub_title =f'season = {season.values}'\n",
    "\n",
    "        val = value.sel(lat=slice(45,90)) if hemisphere == 'NH' else value.sel(lat=slice(-90,-45))\n",
    "        cf = (val.where(~np.isnan(val))).sel(season=season).plot(ax=ax, transform=ccrs.PlateCarree(), extend =None, add_colorbar=False, cmap=cmap, levels=levels, vmin=vmin, vmax=vmax)\n",
    "        \n",
    "        val2 = hemi_glob.sel(hemisphere=hemisphere, season=season).values.round(2)\n",
    "        if var_name != 'sf_eff':\n",
    "          ax.text(0.05, 0.125, f'{val2:.0f}%',transform=ax.transAxes, fontsize=11, verticalalignment='top',bbox=props)\n",
    "        if var_name == 'sf_eff':\n",
    "          ax.text(0.05, 0.125, f'{val2:.1f}'+'h$^{-1}$',transform=ax.transAxes, fontsize=11, verticalalignment='top',bbox=props)\n",
    "        ax.set_title(sub_title)\n",
    "        \n",
    "        # plot statistic not significant \n",
    "        # take the difference with ERA, just check if this difference is lower then that confidence interval CI to find insignificant pixels\n",
    "          \n",
    "        if i == 2:\n",
    "          diff = diff_cmip.sel(lat=slice(45,90)) if hemisphere == 'NH' else value.sel(lat=slice(-90,-45))\n",
    "          (diff.where(np.abs(diff) < CI)).sel(season=season).plot.contourf(ax=ax, transform=ccrs.PlateCarree(), colors='none', hatches=[density*'/',density*'/'],add_colorbar=False,)\n",
    "          ax.set_title('')\n",
    "        \n",
    "    if i == 0 or i == 2:\n",
    "        cbar = plt.colorbar(cf, cax=cbaxes, shrink=0.5,extend=None, orientation='vertical', label=cb_label)\n",
    "    \n",
    "    if i == 2:\n",
    "      s = f.subplotpars\n",
    "      # bb = [s.left+.45, s.top - 0.79, (s.right - s.left), 0.05]\n",
    "      bb = [0.95, 0.09, 0.0125, 0.05]\n",
    "      axsm.flatten()[i].legend([Patch(facecolor='none', edgecolor='k', hatch=density*'/', label='CI < 95%')\n",
    "        ],\n",
    "        ['CI < 95%'],\n",
    "        bbox_to_anchor=bb,loc=8,ncol=1,borderaxespad=0,fancybox=True,bbox_transform=f.transFigure,\n",
    "    )\n",
    "        \n",
    "        \n",
    "  plt.tight_layout(pad=0., w_pad=0., h_pad=0.)  \n",
    "\n",
    "  figname = f'{var_name}_season_{hemisphere}_2007_2010.png'\n",
    "  plt.savefig(fig_dir + figname, format='png', bbox_inches='tight', transparent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var_name in d.keys():\n",
    "    for hemisphere in ['NH', 'SH']:\n",
    "        plt_spatial_season(hemisphere, ratios, var_name, d[var_name],FIG_DIR, 45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_monthly_model_variation(ds_dict, var_name, dict_label,fig_dir):\n",
    "    colors = cm.hawaii(range(0, 256, int(256 / 3) + 1))\n",
    "\n",
    "    \n",
    "    \n",
    "    f, axsm = plt.subplots(nrows=1, ncols=2, sharex=True, sharey=True, figsize=[12, 5])\n",
    "    ax = axsm.flat\n",
    "\n",
    "    # bp= [[], []]\n",
    "    bp= [[],]\n",
    "    for i, hemisphere in enumerate(['NH', 'SH']):\n",
    "        # ax[i].hlines(0., 0.25, 12.75, colors='k')\n",
    "        ax[i].grid(True)\n",
    "        cs_data = xr.concat([ds_dict['cloudsat_500'][var_name + '_month_mean'].sel(hemisphere=hemisphere),\n",
    "                             ds_dict['cloudsat_500'][var_name + '_year_mean'].sel(hemisphere=hemisphere).assign_coords(coords={'month':13})], dim='month')\n",
    "        ax[i].scatter(x=np.arange(1,14), y=cs_data, color='k', marker='o',s=50)\n",
    "        \n",
    "        # era_data = ds_dict['era_30'][var_name + '_month_mean'].sel(hemisphere=hemisphere,)\n",
    "        cmip_key = 'cmip_500'\n",
    "        if var_name != 'sf_eff':\n",
    "            era_data = xr.concat([ds_dict['era_500'][var_name + '_month_cs_mean'].sel(hemisphere=hemisphere,),\n",
    "                                ds_dict['era_500'][var_name + '_year_cs_mean'].sel(hemisphere=hemisphere,).assign_coords(coords={'month':13})], dim='month')\n",
    "            cmip_data = xr.concat([ds_dict[cmip_key][var_name + '_month_cs_mean'],\n",
    "                               ds_dict[cmip_key][var_name + '_year_cs_mean'].assign_coords(coords={'month':'years'})], dim='month')\n",
    "        if var_name == 'sf_eff':\n",
    "            era_data = xr.concat([ds_dict['era_500'][var_name + '_month_cmip_mean'].sel(hemisphere=hemisphere,),\n",
    "                                ds_dict['era_500'][var_name + '_year_cmip_mean'].sel(hemisphere=hemisphere,).assign_coords(coords={'month':13})], dim='month')\n",
    "            cmip_data = xr.concat([ds_dict[cmip_key][var_name + '_month_era_mean'],\n",
    "                               ds_dict[cmip_key][var_name + '_year_era_mean'].assign_coords(coords={'month':'years'})], dim='month')\n",
    "        # era_data.plot.line(ax=ax[i],x='month',color=colors[0],marker='o', linestyle=None)#,linewidth=1.5 linestyle=(0, (1, 1)))\n",
    "        ax[i].scatter(x=np.arange(1.25,14.25), y=era_data, color=colors[0], marker=\"h\", s=50)  \n",
    "        \n",
    "        \n",
    "        # for (j, cmip_key), color in zip(enumerate(['cmip_500', 'cmip_500']), colors[1:]):\n",
    "        j = 0\n",
    "        \n",
    "        color = colors[2]\n",
    "        \n",
    "        quantiles = cmip_data.quantile([0.25, 0.5, 0.75], dim=('model'), skipna=True, keep_attrs=False)\n",
    "        iqr = quantiles.sel(quantile=0.75) - quantiles.sel(quantile=0.25)\n",
    "        max_val = (quantiles.sel(quantile=0.75) + 1.5 * iqr).assign_coords({'quantile': 'min'})\n",
    "        min_val = (quantiles.sel(quantile=0.25) - 1.5 * iqr).assign_coords({'quantile': 'max'})\n",
    "        # means = ds_dict[cmip_key][var_name + '_month_mean'].mean(dim='model', skipna=True).assign_coords({'quantile': 'mean'})\n",
    "        means = cmip_data.mean(dim='model', skipna=True).assign_coords({'quantile':'mean'})\n",
    "        stats = xr.concat([max_val, quantiles, min_val, means], dim='quantile')\n",
    "           \n",
    "        if j == 0:\n",
    "                # positions = np.arange(j + 0.75, j+12.75, 1)\n",
    "                positions = np.arange(j + 0.75, j + 13.75, 1)\n",
    "        else:\n",
    "                positions = np.arange(j + 0.25, j + 13.25, 1)\n",
    "            # boxplot_data = (ds_dict[cmip_key][var_name + '_month_stats'].sel(hemisphere=hemisphere,)).mean('model',skipna=True).transpose('quantile', 'month')\n",
    "        boxplot_data = stats.sel(hemisphere=hemisphere,).transpose('quantile', 'month')\n",
    "        bp[j] = ax[i].boxplot(boxplot_data,  positions=positions, widths=0.4, \n",
    "                            boxprops=dict(color=color, lw=1.5),\n",
    "                            medianprops=dict(color=color, lw=1.5),\n",
    "                            whiskerprops=dict(color=color, lw=1.5),\n",
    "                            capprops=dict(color=color, lw=1.5),\n",
    "                            flierprops=dict(marker='+',markeredgecolor=color, markersize=10),\n",
    "                            showmeans=True, meanprops=dict(marker='D',markerfacecolor=color, markersize=4),\n",
    "                            patch_artist=True,)\n",
    "            \n",
    "        for patch in bp[j]['boxes']:\n",
    "                patch.set(facecolor=color, alpha=0.5)\n",
    "            \n",
    "            \n",
    "          \n",
    "            \n",
    "        ax[i].set_title('Arctic' if hemisphere == 'NH' else 'Antarctic')   \n",
    "        ax[i].text(0.05, 0.95, f'{fig_label[i]}', fontweight='bold', horizontalalignment='left', verticalalignment='top', transform=ax[i].transAxes)\n",
    "        ax[i].set_xticks(np.arange(1,14)) \n",
    "        ax[i].set_xlim([0, 13.5])\n",
    "\n",
    "\n",
    "        ax[i].set_xticklabels(np.append(np.arange(1,13), 'years'), fontsize=12)\n",
    "        ax[i].set_xlabel('Month')\n",
    "        \n",
    "        # ax[i].set_yticks(np.arange(-1,1.5,.25))\n",
    "        if var_name != 'sf_eff':\n",
    "            ax[i].set_ylim([dict_label['vmin'],dict_label['vmax']])\n",
    "            ax[i].set_yticks(np.arange(0,110,10))\n",
    "        if var_name == 'sf_eff':\n",
    "            ax[i].set_ylim([0,8.])\n",
    "            ax[i].set_yticks(np.arange(0,8.50,.50))\n",
    "        \n",
    "        ax[i].set_ylabel(dict_label['cb_label'] if i==0 else '') \n",
    "        \n",
    "    s = f.subplotpars\n",
    "    bb = [s.left, s.top - 0.92, (s.right - s.left), 0.05]\n",
    "\n",
    "    ax[1].legend([\n",
    "            # Line2D([0], [0], color=colors[0], lw=1.5, label='ERA5 (30 km)', linestyle=(0, (1, 1))),\n",
    "            Line2D([0], [0], marker='o', color='w', label='CloudSat (500km)', markersize=10, markerfacecolor='k'),\n",
    "            Line2D([0], [0], marker='h', color='w', label='ERA5 (30 km)',markersize=10, markerfacecolor=colors[0], ),\n",
    "            bp[0][\"boxes\"][0],\n",
    "            # bp[1][\"boxes\"][0],\n",
    "        ],\n",
    "        ['CloudSat (500 km)',\n",
    "            'ERA5$_{mean}$ (500 km)', #'CMIP6 (500 km)', \n",
    "         'CMIP6 (500 km)'],\n",
    "        bbox_to_anchor=bb,loc=8,ncol=3,mode='expand',borderaxespad=0,fancybox=True,bbox_transform=f.transFigure,\n",
    "    )\n",
    "    \n",
    "    plt.tight_layout(pad=0., w_pad=0., h_pad=0.)  ;\n",
    "    \n",
    "    figname = f'{var_name}_monthly_model_variation_2007_2010.png'\n",
    "    plt.savefig(fig_dir + figname, format='png', bbox_inches='tight', transparent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var_name in d.keys():\n",
    "    plt_monthly_model_variation(ratios, var_name, d[var_name], FIG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratios['250'] = xr.concat([ratios['era_250'].assign_coords(coords={'model':'ERA5'}), ratios['cmip_250']], dim=(\"model\"))\n",
    "ratios['500'] = xr.concat([ratios['era_500'].assign_coords(coords={'model':'ERA5'}), ratios['cmip_500']], dim=(\"model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_linear_regression(df, model, ):\n",
    "    # To do this we use the polyfit function from Numpy. Polyfit does a least squares polynomial fit over the data that it is given. \n",
    "    # We want a linear regression over the data in columns cloudsat and MIROC6 so we pass these as parameters. The final parameter is the \n",
    "    # degree of the polynomial. For linear regression the degree is 1.\n",
    "    d = np.polyfit(df['reference'], df[model],1) # These are the a and b values we were looking for in the linear function formula.\n",
    "    # We then use the convenience function poly1d to provide us with a function that will do the fitting.\n",
    "    f = np.poly1d(d) #predict the estimated results\n",
    "    # We now use the function f to produce our linear regression data and inserting that into a new column called Treg.\n",
    "    df.insert(2, 'Treg', f(df['reference']))\n",
    "    # the R-squared value is a number between 0 and 1. And the closer it is to 1 the more accurate your linear regression model is.\n",
    "    R2 = r2_score(df[model],f(df['reference']))\n",
    "    return(R2,d)\n",
    "\n",
    "def calc_linear_regression_hemisphere(ratios_x, ratios_y, season, model, lat_slice, var_name):\n",
    "\n",
    "    df_NH = pd.DataFrame()\n",
    "    df_NH['reference'] = ratios_x[f'{var_name}_season'].sel(season=season, lat=lat_slice).to_dataframe()[f'{var_name}_season']\n",
    "    \n",
    "    df_NH[model] = ratios_y[f'{var_name}_season'].sel(season=season, model=model, lat=lat_slice).to_dataframe()[f'{var_name}_season']\n",
    "    df_NH.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df_NH.dropna(inplace=True)\n",
    "    R2_NH, d_NH = calc_linear_regression(df_NH, model)    \n",
    "    return(df_NH, R2_NH, d_NH)\n",
    "\n",
    "\n",
    "def calc_scatter_obs_model(ratios, var_name):\n",
    "    lat_north = slice(45, 90)\n",
    "    lat_south = slice(-90, -45)\n",
    "\n",
    "    df_NH = dict()\n",
    "    R2_NH = dict()\n",
    "    d_NH = dict()\n",
    "\n",
    "    df_SH = dict()\n",
    "    R2_SH = dict()\n",
    "    d_SH = dict()\n",
    "    for model in ratios['500'].model.values:\n",
    "        # print(model)\n",
    "        df_NH[model] = dict()\n",
    "        R2_NH[model] = dict()\n",
    "        d_NH[model] = dict()\n",
    "        \n",
    "        df_SH[model] = dict()\n",
    "        R2_SH[model] = dict()\n",
    "        d_SH[model] = dict()\n",
    "        for season in ratios['500'].season.values:\n",
    "            # print(season)\n",
    "            if var_name != 'sf_eff':\n",
    "                df_NH[model][season], R2_NH[model][season], d_NH[model][season] = calc_linear_regression_hemisphere(ratios['cloudsat_500'], \n",
    "                                                                                                                                            ratios['500'], \n",
    "                                                                                                                                            season, \n",
    "                                                                                                                                            model, \n",
    "                                                                                                                                            lat_north, var_name)\n",
    "                \n",
    "                df_SH[model][season], R2_SH[model][season], d_SH[model][season] = calc_linear_regression_hemisphere(ratios['cloudsat_500'], \n",
    "                                                                                                                                            ratios['500'], \n",
    "                                                                                                                                            season, \n",
    "                                                                                                                                            model, \n",
    "                                                                                                                                            lat_south, var_name)\n",
    "                \n",
    "            elif var_name == 'sf_eff':\n",
    "                df_NH[model][season], R2_NH[model][season], d_NH[model][season] = calc_linear_regression_hemisphere(ratios['era_500'], \n",
    "                                                                                                                                            ratios['500'], \n",
    "                                                                                                                                            season, \n",
    "                                                                                                                                            model, \n",
    "                                                                                                                                            lat_north, var_name)\n",
    "                \n",
    "                df_SH[model][season], R2_SH[model][season], d_SH[model][season] = calc_linear_regression_hemisphere(ratios['era_500'], \n",
    "                                                                                                                                            ratios['500'], \n",
    "                                                                                                                                            season, \n",
    "                                                                                                                                            model, \n",
    "                                                                                                                                            lat_south, var_name)\n",
    "    return(df_NH, R2_NH, d_NH, df_SH, R2_SH, d_SH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_scatter_obs_model(ratios, var_name, dict_label, fig_dir):\n",
    "    \n",
    "    df_NH, R2_NH, d_NH, df_SH, R2_SH, d_SH = calc_scatter_obs_model(ratios, var_name)\n",
    "    \n",
    "    if var_name == 'sf_eff':\n",
    "        df_NH.pop('ERA5')\n",
    "        # df_SH.pop('ERA5')\n",
    "    \n",
    "    f, axsm = plt.subplots(nrows=len(df_NH.keys()), ncols=4, sharex=True, sharey=True, figsize=[15, 37.5])\n",
    "    colors = cm.hawaii(range(0, 256, int(256 / 3) + 1))\n",
    "\n",
    "    f.suptitle(dict_label['cb_label'], fontsize=15)\n",
    "\n",
    "    for ax, k in zip(axsm.flatten(), range(len(fig_label))):\n",
    "        ax.text(0.05, 0.95, f'{fig_label[k]}', fontweight='bold', horizontalalignment='left', verticalalignment='top', transform=ax.transAxes)\n",
    "        \n",
    "\n",
    "    for i, model in zip(range(len(df_NH)), df_NH.keys()):\n",
    "        for ax, season in zip(axsm.flatten()[i*4: (i+1)*4+1], df_NH[model].keys()):\n",
    "                \n",
    "                \n",
    "                if i == 0:\n",
    "                    ax.set_title(f'season = {season}')\n",
    "                    \n",
    "                ax.axline((0,0), slope=1, color='black',linestyle='--', )\n",
    "                if var_name != 'sf_eff':\n",
    "                    ax.set_xlim([dict_label['vmin'],dict_label['vmax']])\n",
    "                    ax.set_ylim([dict_label['vmin'],dict_label['vmax']])\n",
    "                if var_name == 'sf_eff':\n",
    "                    ax.set_xlim([0,10])\n",
    "                    ax.set_ylim([0,10])\n",
    "                \n",
    "                df_NH[model][season].plot.scatter(ax = ax, x = 'reference',y=model, label = 'NH', color = colors[0].reshape(1,-1), alpha=0.1, legend=False)\n",
    "                df_NH[model][season].plot(x='reference', y='Treg',color=colors[0].reshape(1,-1),ax=ax, label= f\"y$_N$ = {d_NH[model][season][0].round(2)} x + {d_NH[model][season][1].round(2)}\" \n",
    "                    \"\\n\" \n",
    "                    f\"r$^2$ = {R2_NH[model][season].round(2)}\")\n",
    "                \n",
    "                df_SH[model][season].plot.scatter(ax = ax, x = 'reference',y=model, label = 'SH', color = colors[2].reshape(1,-1), alpha=0.1, legend=False)\n",
    "                df_SH[model][season].plot(x='reference', y='Treg',color=colors[2].reshape(1,-1),ax=ax, label= f\"y$_S$ = {d_SH[model][season][0].round(2)} x + {d_SH[model][season][1].round(2)}\" \n",
    "                    \"\\n\" \n",
    "                    f\"r$^2$ = {R2_SH[model][season].round(2)}\")\n",
    "                \n",
    "                ax.legend(loc='lower right')\n",
    "                ax.grid(True)\n",
    "                ax.set_ylabel(model)\n",
    "                if var_name != 'sf_eff':\n",
    "                    ax.set_xlabel('CloudSat')\n",
    "                if var_name == 'sf_eff':\n",
    "                    ax.set_xlabel('ERA5')\n",
    "            \n",
    "                \n",
    "    f.tight_layout(pad=0., w_pad=0., h_pad=0.)  \n",
    "    f.subplots_adjust(top=0.96)\n",
    "    \n",
    "    figname = f'{var_name}_season_scatter_2007_2010.png'\n",
    "    plt.savefig(fig_dir + figname, format='png', bbox_inches='tight', transparent=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var_name in d.keys():\n",
    "    plt_scatter_obs_model(ratios, var_name, d[var_name], FIG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_NH = dict()\n",
    "df_SH = dict()\n",
    "df = dict()\n",
    "for var_name in d.keys():\n",
    "    _, R2_var_NH, d_var_NH, _, R2_var_SH, d_var_SH = calc_scatter_obs_model(ratios, var_name)\n",
    "    df_NH[var_name] = (pd.DataFrame({model: np.array(list(R2_var_NH[model].values())) for model in R2_var_NH.keys()}, index=['NDJ', 'MJJ', 'FMA', 'ASO'])).transpose()\n",
    "    df_SH[var_name] = (pd.DataFrame({model: np.array(list(R2_var_SH[model].values())) for model in R2_var_SH.keys()}, index=['NDJ', 'MJJ', 'FMA', 'ASO'])).transpose()\n",
    "    \n",
    "    # df_NH[var_name][\"\"] = np.nan\n",
    "    # df[var_name] = pd.concat([df_NH[var_name], df_SH[var_name]],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set ERA5 correlation number to nan \n",
    "for season in df_NH['sf_eff'].columns:\n",
    "    df_NH['sf_eff'][season].loc['ERA5'] = np.nan\n",
    "    df_SH['sf_eff'][season].loc['ERA5'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_R2_heatmap_season(df_NH, df_SH, dict_label, fig_dir):\n",
    "    # define heatmap colors\n",
    "    cmap = cm.hawaii_r  # define the colormap\n",
    "    # extract all colors from the hawaii map\n",
    "    cmaplist = [cmap(i) for i in range(cmap.N)]\n",
    "    # create the new map\n",
    "    cmap = LinearSegmentedColormap.from_list(\n",
    "        'Custom cmap', cmaplist, cmap.N)\n",
    "\n",
    "    # define the bins and normalize\n",
    "    bounds = np.linspace(0, 1, 11)\n",
    "    norm = BoundaryNorm(bounds, cmap.N)\n",
    "    \n",
    "    \n",
    "    f, axsm = plt.subplots(nrows=1, ncols=len(df_NH)*2, sharex=True, sharey=True, figsize=[15, 5])\n",
    "    ax = axsm.flatten()\n",
    "\n",
    "    for ax, var_name, k in zip(axsm.flatten()[::2],dict_label.keys(), fig_label[::2]): \n",
    "        im = ax.imshow(df_NH[var_name], cmap=cmap, norm=norm)\n",
    "        ax.set_title(f'{k} Arctic', )#fontsize=10.)\n",
    "        \n",
    "        if k == 'a)':\n",
    "            ax.set(yticks=range(len(df_NH[var_name].index)), yticklabels=df_NH[var_name].index)\n",
    "        if var_name == 'FLCC':\n",
    "            x_position = 0.75  \n",
    "        elif var_name == 'FsLCC':\n",
    "            x_position = 0.75\n",
    "        elif var_name == 'FoP' or var_name == 'FoS' or var_name == 'sf_eff':\n",
    "            x_position = 0.55\n",
    "        \n",
    "        plt.figtext(x_position,-0.11, dict_label[var_name]['cb_label'], fontweight='bold', horizontalalignment='left', verticalalignment='center', transform=ax.transAxes)\n",
    "        # elif var_name == 'sf_eff':\n",
    "        #     plt.figtext(0.25,-0.09, dict_label[var_name]['cb_label'], fontweight='bold', horizontalalignment='left', verticalalignment='center', transform=ax.transAxes)\n",
    "        # elif var_name == 'lcc_w_snow':\n",
    "        #     plt.figtext(0.35,-0.09, dict_label[var_name]['cb_label'], fontweight='bold', horizontalalignment='left', verticalalignment='center', transform=ax.transAxes)\n",
    "        ax.set(xticks=range(len(df_NH[var_name].columns)), xticklabels=df_NH[var_name].columns,)\n",
    "    for ax, var_name, label, k in zip(axsm.flatten()[1::2],d.keys(), d.values(), fig_label[1::2]):     \n",
    "        im = ax.imshow(df_SH[var_name], cmap=cmap, norm=norm)\n",
    "        ax.set_title(f'{k} Antarctic')\n",
    "        ax.set(xticks=range(len(df_SH[var_name].columns)), xticklabels=df_SH[var_name].columns,)\n",
    "\n",
    "    # add space for colour bar\n",
    "    f.subplots_adjust(right=0.85)\n",
    "    cbar_ax = f.add_axes([1.01, 0.15, 0.0125, 0.7])\n",
    "    \n",
    "    f.colorbar(im, cax=cbar_ax, cmap=cmap, norm=norm,\n",
    "        spacing='proportional', ticks=bounds, boundaries=bounds, label=f'r$^2$-values',shrink=0.5)\n",
    "\n",
    "    \n",
    "\n",
    "    f.tight_layout(pad=0., w_pad=0.4, h_pad=0.)  \n",
    "    figname = f'R2_season_2007_2010.png'\n",
    "    plt.savefig(fig_dir + figname, format='png', bbox_inches='tight', transparent=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_R2_heatmap_season(df_NH, df_SH, d,FIG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_spatial_season_no_diff(hemisphere,ds, var_name, dict_label, fig_dir,lat_extent):\n",
    "  \n",
    "  val1      = ds['cloudsat_500'][var_name + '_season']\n",
    "  val1      = val1.where(val1 != 0., other=np.nan)\n",
    "  val1      = val1.where(~np.isnan(val1))\n",
    "  val1_mean = ds['cloudsat_500'][var_name + '' + '_season_mean']\n",
    "  \n",
    "  \n",
    "  val2 = ds['era_500'][var_name + '_season']\n",
    "  val2 = val2.where(val2 != 0., other=np.nan)\n",
    "  val2 = val2.where(~np.isnan(val2))\n",
    "  \n",
    "  val3      = ds['cmip_500'][var_name + '_season']\n",
    "  val3 = val3.where(val3 != 0., other=np.nan)\n",
    "  val3 = val3.where(~np.isnan(val3))\n",
    "\n",
    "  \n",
    "      \n",
    "  if var_name != 'sf_eff':\n",
    "    \n",
    "    val2_mean = ds['era_500'][var_name + '' + '_season_cs_mean']\n",
    "      \n",
    "    val3_mean = ds['cmip_500'][var_name + '' +'_season_cs_mean']\n",
    "\n",
    "    \n",
    "  if var_name == 'sf_eff':\n",
    "    \n",
    "    val2_mean = ds['era_500'][var_name + '' + '_season_cmip_mean']\n",
    "    \n",
    "    val3_mean = ds['cmip_500'][var_name + '' +'_season_era_mean']\n",
    " \n",
    "  if 'model' in val3.coords:\n",
    "    \n",
    "    # create model mean\n",
    "    val3 = val3.mean('model', skipna=True, keep_attrs=False)\n",
    "    val3 = val3.where(val3 != 0., other=np.nan)\n",
    "    val3 = val3.where(~np.isnan(val3))\n",
    "    val3_mean = val3_mean.mean('model',skipna=True, keep_attrs=False)\n",
    " \n",
    "  \n",
    "  projections = {'NH': ccrs.NorthPolarStereo(central_longitude=0.0, globe=None),\n",
    "                'SH': ccrs.SouthPolarStereo(central_longitude=0.0, globe=None)}\n",
    "\n",
    "  projection = projections[hemisphere]\n",
    "  density=4\n",
    "\n",
    "  \n",
    "  if var_name != 'sf_eff':\n",
    "    list1 = [val1, val2, val3]\n",
    "    list_glob = [val1_mean, val2_mean, val3_mean]\n",
    "    model_labels = ['CloudSat (500km)', 'ERA5 (500km)', 'CMIP6$_{mean}$ (500km)', ]\n",
    "    f, axsm = plt.subplots(nrows=3, ncols=4, subplot_kw={'projection': projection}, figsize=[12, 9], sharex=True, sharey=True)\n",
    "    \n",
    "      \n",
    "  if var_name == 'sf_eff':\n",
    "    list1 = [val2, val3]\n",
    "    list_glob = [val2_mean, val3_mean]\n",
    "    model_labels = ['ERA5 (500km)', 'CMIP6$_{mean}$ (500km)', ]\n",
    "    f, axsm = plt.subplots(nrows=2, ncols=4, subplot_kw={'projection': projection}, figsize=[12, 6], sharex=True, sharey=True)\n",
    "    \n",
    "  levels = dict_label['levels']\n",
    "  vmin = dict_label['vmin']\n",
    "  vmax = dict_label['vmax']\n",
    "  \n",
    "  coast = cy.feature.NaturalEarthFeature(category='physical', scale='110m', facecolor='none', name='coastline')\n",
    "\n",
    "  rows = model_labels\n",
    "  props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "\n",
    "\n",
    "  for ax, row in zip(axsm[:, 0], rows):\n",
    "      ax.text(-0.07, 0.55, row, va='bottom', ha='center', rotation='vertical', rotation_mode='anchor', transform=ax.transAxes, fontweight='bold')\n",
    "\n",
    "  for ax, k in zip(axsm.flatten(), range(len(fig_label))):\n",
    "      if hemisphere == 'NH':\n",
    "          ax.set_extent([-180, 180, 90, lat_extent], ccrs.PlateCarree())\n",
    "      elif hemisphere == 'SH':\n",
    "          ax.set_extent([-180, 180, -90, -1*lat_extent], ccrs.PlateCarree())\n",
    "      ax.add_feature(coast, alpha=0.5)\n",
    "      gl = ax.gridlines(draw_labels=True)\n",
    "      gl.top_labels = False\n",
    "      gl.right_labels = False\n",
    "      ax.text(0.05, 0.95, f'{fig_label[k]}', fontweight='bold', horizontalalignment='left', verticalalignment='top', transform=ax.transAxes)\n",
    "\n",
    "\n",
    "  for ax, k, season in zip(axsm.flatten()[:4], range(len(fig_label)), val1.season):\n",
    "      ax.set(title=f'season = {season.values}')\n",
    "\n",
    "  # for i, (value,mean) in enumerate(zip([\"CS_30\",\"CS_30 - ERA_30\" ,\"CS_250 - CMIP_250\", \"CS_500 - CMIP_500\"],\n",
    "  #                               [\"CS_30_glob\", \"CS_30 - ERA_30_glob\" ,\"CS_250 - CMIP_250_glob\", \"CS_500 - CMIP_500_glob\"])):\n",
    "  for i, (value,hemi_glob) in enumerate(zip(list1,list_glob)):\n",
    "    cmap = cm.hawaii_r\n",
    "    cmap = 'rainbow'\n",
    "          \n",
    "    if i == 1 or i == 2 or i ==3:# or i==1:\n",
    "        sub_title = \"\"\n",
    "    \n",
    "          \n",
    "    for ax, season in zip(axsm.flatten()[i*4: (i+1)*4+1], ratios['cloudsat_250'].season):\n",
    "      #   print(i, value,hemi_glob, season.values)\n",
    "        if i == 0:\n",
    "          sub_title =f'season = {season.values}'\n",
    "\n",
    "        val = value.sel(lat=slice(45,90)) if hemisphere == 'NH' else value.sel(lat=slice(-90,-45))\n",
    "        cf = (val.where(~np.isnan(val))).sel(season=season).plot(ax=ax, transform=ccrs.PlateCarree(), extend =None, add_colorbar=False, cmap=cmap, levels=levels, vmin=vmin, vmax=vmax)\n",
    "        \n",
    "\n",
    "        val2 = hemi_glob.sel(hemisphere=hemisphere, season=season).values.round(2)\n",
    "        if var_name != 'sf_eff':\n",
    "          ax.text(0.05, 0.125, f'{val2:.0f}%',transform=ax.transAxes, fontsize=11, verticalalignment='top',bbox=props)\n",
    "        if var_name == 'sf_eff':\n",
    "          ax.text(0.05, 0.125, f'{val2:.1f}'+'h$^{-1}$',transform=ax.transAxes, fontsize=11, verticalalignment='top',bbox=props)\n",
    "        ax.set_title(sub_title)\n",
    "        \n",
    "        \n",
    "    if i == 1:\n",
    "      cbaxes = f.add_axes([1.02, 0.25, 0.0125, 0.45])\n",
    "      cb_label = dict_label['cb_label']\n",
    "      cbar = plt.colorbar(cf, cax=cbaxes, shrink=0.5,extend=None, orientation='vertical', label=cb_label)\n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "  plt.tight_layout(pad=0., w_pad=0., h_pad=0.)  \n",
    "\n",
    "  figname = f'{var_name}_CS_ERA5_CMIP6_season_{hemisphere}_2007_2010.png'\n",
    "  plt.savefig(fig_dir + figname, format='png', bbox_inches='tight', transparent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var_name in d.keys():\n",
    "    for hemisphere in ['NH', 'SH']:\n",
    "        # print(hemisphere, var_name, cb_label)\n",
    "        plt_spatial_season_no_diff(hemisphere, ratios, var_name, d[var_name], FIG_DIR,45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_spatial_season_FLCC_FsLCC(hemisphere,ds, dict_label, fig_dir,lat_extent):\n",
    "    val1_res  = ds['cloudsat_500']['FLCC' + '_season']\n",
    "    val1_res  = val1_res.where(val1_res != 0., other=np.nan)\n",
    "    val1_res  = val1_res.where(~np.isnan(val1_res))\n",
    "    val1_res_mean = ds['cloudsat_500']['FLCC' + '' +'_season_mean']\n",
    "    \n",
    "    \n",
    "    val2_res = ds['cloudsat_500']['FsLCC' + '_season']\n",
    "    val2_res = val2_res.where(val2_res != 0., other=np.nan)\n",
    "    val2_res = val2_res.where(~np.isnan(val2_res))\n",
    "    val2_res_mean = ds['cloudsat_500']['FsLCC' + '' + '_season_mean']\n",
    "    \n",
    "    model_labels = ['FLCC', 'FsLCC', 'FLCC - FsLCC', ]\n",
    "\n",
    "    \n",
    "    diff_res = val1_res.where(~np.isnan(val2_res)) - val2_res.where(~np.isnan(val1_res))\n",
    "    list1 = [val1_res, val2_res, (diff_res)]\n",
    "    list_glob = [val1_res_mean, val2_res_mean, val1_res_mean - val2_res_mean]\n",
    "    \n",
    "    projections = {'NH': ccrs.NorthPolarStereo(central_longitude=0.0, globe=None),\n",
    "    'SH': ccrs.SouthPolarStereo(central_longitude=0.0, globe=None)}\n",
    "\n",
    "    projection = projections[hemisphere]\n",
    "\n",
    "\n",
    "\n",
    "    f, axsm = plt.subplots(nrows=3, ncols=4, subplot_kw={'projection': projection}, figsize=[12, 9], sharex=True, sharey=True)\n",
    "    coast = cy.feature.NaturalEarthFeature(category='physical', scale='110m', facecolor='none', name='coastline')\n",
    "\n",
    "    rows = model_labels\n",
    "    props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "\n",
    "\n",
    "    for ax, row in zip(axsm[:, 0], rows):\n",
    "        ax.text(-0.07, 0.55, row, va='bottom', ha='center', rotation='vertical', rotation_mode='anchor', transform=ax.transAxes, fontweight='bold')\n",
    "\n",
    "    for ax, k in zip(axsm.flatten(), range(len(fig_label))):\n",
    "        if hemisphere == 'NH':\n",
    "            ax.set_extent([-180, 180, 90, lat_extent], ccrs.PlateCarree())\n",
    "        elif hemisphere == 'SH':\n",
    "            ax.set_extent([-180, 180, -90, -1*lat_extent], ccrs.PlateCarree())\n",
    "        ax.add_feature(coast, alpha=0.5)\n",
    "        gl = ax.gridlines(draw_labels=True)\n",
    "        gl.top_labels = False\n",
    "        gl.right_labels = False\n",
    "        ax.text(0.05, 0.95, f'{fig_label[k]}', fontweight='bold', horizontalalignment='left', verticalalignment='top', transform=ax.transAxes)\n",
    "\n",
    "\n",
    "    # for ax, k, season in zip(axsm.flatten()[:4], range(len(fig_label)), val1_res.season):\n",
    "    #     ax.set(title=f'season = {season.values}')\n",
    "\n",
    "    for i, (value,hemi_glob) in enumerate(zip(list1,list_glob)):\n",
    "        if i == 0 or i == 1:\n",
    "            cmap = cm.hawaii_r\n",
    "            levels = dict_label['levels']\n",
    "            vmin = dict_label['vmin']\n",
    "            vmax = dict_label['vmax']\n",
    "        if i == 0:\n",
    "            cbaxes = f.add_axes([1.0, 0.41, 0.0125, 0.45])\n",
    "            cb_label = '(%)'\n",
    "        elif i == 1:\n",
    "            sub_title = \"\"\n",
    "        elif i == 2:\n",
    "            cmap = cm.bam\n",
    "            sub_title = \"\"\n",
    "            levels = dict_label['diff_levels']\n",
    "            vmin = dict_label['diff_vmin']\n",
    "            vmax = dict_label['diff_vmax']\n",
    "            \n",
    "            cbaxes = f.add_axes([1.0, 0.024, 0.0125, 0.3])\n",
    "            cb_label = 'FLCC - FsLCC (%)'\n",
    "            \n",
    "            \n",
    "        for ax, season in zip(axsm.flatten()[i*4: (i+1)*4+1], ds[f'cloudsat_500'].season):\n",
    "            if i == 0:\n",
    "                sub_title =f'season = {season.values}'\n",
    "\n",
    "            val = value.sel(lat=slice(45,90)) if hemisphere == 'NH' else value.sel(lat=slice(-90,-45))\n",
    "            cf = (val.where(~np.isnan(val))).sel(season=season).plot(ax=ax, transform=ccrs.PlateCarree(), extend =None, add_colorbar=False, cmap=cmap, levels=levels, vmin=vmin, vmax=vmax)\n",
    "\n",
    "            \n",
    "            val2 = hemi_glob.sel(hemisphere=hemisphere, season=season).values.round(2)\n",
    "            \n",
    "            ax.text(0.05, 0.125, f'{val2:.0f}%',transform=ax.transAxes, fontsize=11, verticalalignment='top',bbox=props)\n",
    "            ax.set_title(sub_title)\n",
    "            \n",
    "            \n",
    "        if i == 1 or i == 2:\n",
    "            cbar = plt.colorbar(cf, cax=cbaxes, shrink=0.5,extend=None, orientation='vertical', label=cb_label)\n",
    "        \n",
    "        \n",
    "            \n",
    "            \n",
    "    plt.tight_layout(pad=0., w_pad=0., h_pad=0.)  \n",
    "\n",
    "    figname = f'FLCC_FsLCC_season_{hemisphere}_2007_2010.png'\n",
    "    plt.savefig(fig_dir + figname, format='png', bbox_inches='tight', transparent=False)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for hemisphere in ['NH', 'SH']:\n",
    "    plt_spatial_season_FLCC_FsLCC(hemisphere,ratios, d['FLCC'], FIG_DIR,45)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_spatial_season_for_model(hemisphere,ds, var_name, dict_label, res, model, fig_dir,lat_extent):\n",
    "# \n",
    "  val2_res      = ds[f'{res}'][var_name + '_season']\n",
    "  val2_res = val2_res.where(val2_res != 0., other=np.nan)\n",
    "  val2_res = val2_res.where(~np.isnan(val2_res))\n",
    "  \n",
    "  \n",
    "  if var_name != 'sf_eff':\n",
    "    val1_res  = ds[f'cloudsat_{res}'][var_name + '_season']\n",
    "    val1_res  = val1_res.where(val1_res != 0., other=np.nan)\n",
    "    val1_res  = val1_res.where(~np.isnan(val1_res))\n",
    "    val1_res_mean = ds[f'cloudsat_{res}'][var_name + '' +'_season_mean']\n",
    "    \n",
    "    val2_res_mean = ds[f'{res}'][var_name + '' + '_season_cs_mean']\n",
    "    \n",
    "    model_labels = [f'CloudSat ({res}km)', f'{model} ({res}km)', f'CloudSat - {model}', ]\n",
    "\n",
    "  if var_name == 'sf_eff':\n",
    "    val1_res  = ds[f'era_{res}'][var_name + '_season']\n",
    "    val1_res  = val1_res.where(val1_res != 0., other=np.nan)\n",
    "    val1_res  = val1_res.where(~np.isnan(val1_res))\n",
    "    val1_res_mean = ds[f'era_{res}'][var_name + '' +'_season_mean']\n",
    "    \n",
    "    val2_res_mean = ds[f'{res}'][var_name + '' + '_season_era_mean']\n",
    "    \n",
    "    model_labels = [f'ERA5 ({res}km)', f'{model} ({res}km)', f'CloudSat - {model}', ]\n",
    "    \n",
    "    \n",
    "  if 'model' in val2_res.coords:\n",
    "        if model != 'ERA5':\n",
    "          # create model std\n",
    "          std = (val2_res.isel(model=slice(1,11))).std(dim='model',skipna=True, keep_attrs=False)\n",
    "          std = std.where(std != 0., other =np.nan)\n",
    "          std = std.where(~np.isnan(std))\n",
    "          # calculate statistic significance dependend on the model spread\n",
    "          SE = std/np.sqrt(len(val2_res['model'])) # standard error\n",
    "          CI = (SE) * 1.96 # for 95% confidence level, or CI=2.58*SE for 99% confidence level\n",
    "          \n",
    "        # create model mean\n",
    "        val2_res = val2_res.sel(model=model)#.mean('model', skipna=True, keep_attrs=False)\n",
    "        val2_res = val2_res.where(val2_res != 0., other=np.nan)\n",
    "        val2_res = val2_res.where(~np.isnan(val2_res))\n",
    "        val2_res_mean = val2_res_mean.sel(model=model)#.mean('model',skipna=True, keep_attrs=False)\n",
    "        \n",
    "        \n",
    "    \n",
    "  diff_res = val1_res.where(~np.isnan(val2_res)) - val2_res.where(~np.isnan(val1_res))\n",
    "  list1 = [val1_res, val2_res, (diff_res)]\n",
    "  list_glob = [val1_res_mean, val2_res_mean, val1_res_mean - val2_res_mean]\n",
    "\n",
    "\n",
    "  \n",
    "  \n",
    "\n",
    "  projections = {'NH': ccrs.NorthPolarStereo(central_longitude=0.0, globe=None),\n",
    "                'SH': ccrs.SouthPolarStereo(central_longitude=0.0, globe=None)}\n",
    "\n",
    "  projection = projections[hemisphere]\n",
    "  density=4\n",
    "\n",
    "\n",
    "\n",
    "  f, axsm = plt.subplots(nrows=3, ncols=4, subplot_kw={'projection': projection}, figsize=[12, 9], sharex=True, sharey=True)\n",
    "  coast = cy.feature.NaturalEarthFeature(category='physical', scale='110m', facecolor='none', name='coastline')\n",
    "\n",
    "  rows = model_labels\n",
    "  props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "\n",
    "\n",
    "  for ax, row in zip(axsm[:, 0], rows):\n",
    "      ax.text(-0.07, 0.55, row, va='bottom', ha='center', rotation='vertical', rotation_mode='anchor', transform=ax.transAxes, fontweight='bold')\n",
    "\n",
    "  for ax, k in zip(axsm.flatten(), range(len(fig_label))):\n",
    "      if hemisphere == 'NH':\n",
    "          ax.set_extent([-180, 180, 90, lat_extent], ccrs.PlateCarree())\n",
    "      elif hemisphere == 'SH':\n",
    "          ax.set_extent([-180, 180, -90, -1*lat_extent], ccrs.PlateCarree())\n",
    "      ax.add_feature(coast, alpha=0.5)\n",
    "      gl = ax.gridlines(draw_labels=True)\n",
    "      gl.top_labels = False\n",
    "      gl.right_labels = False\n",
    "      ax.text(0.05, 0.95, f'{fig_label[k]}', fontweight='bold', horizontalalignment='left', verticalalignment='top', transform=ax.transAxes)\n",
    "\n",
    "\n",
    "  for ax, k, season in zip(axsm.flatten()[:4], range(len(fig_label)), val1_res.season):\n",
    "      ax.set(title=f'season = {season.values}')\n",
    "\n",
    "  for i, (value,hemi_glob) in enumerate(zip(list1,list_glob)):\n",
    "    if i == 0 or i == 1:\n",
    "      cmap = cm.hawaii_r\n",
    "      levels = dict_label['levels']\n",
    "      vmin = dict_label['vmin']\n",
    "      vmax = dict_label['vmax']\n",
    "      if i == 0:\n",
    "        cbaxes = f.add_axes([0.92, 0.41, 0.0125, 0.45])\n",
    "        cb_label = dict_label['cb_label']\n",
    "      elif i == 1:\n",
    "        sub_title = \"\"\n",
    "    elif i == 2:# or i ==3:\n",
    "        cmap = cm.bam\n",
    "        sub_title = \"\"\n",
    "        levels = dict_label['diff_levels']\n",
    "        vmin = dict_label['diff_vmin']\n",
    "        vmax = dict_label['diff_vmax']\n",
    "        \n",
    "        cbaxes = f.add_axes([0.92, 0.13, 0.0125, 0.2])\n",
    "        if var_name != 'sf_eff':\n",
    "          cb_label = f'CloudSat - {model} (%)'\n",
    "        if var_name == 'sf_eff':\n",
    "          cb_label = f'ERA5 - {model} (%)'\n",
    "          \n",
    "    for ax, season in zip(axsm.flatten()[i*4: (i+1)*4+1], ds[f'cloudsat_{res}'].season):\n",
    "      #   print(i, value,hemi_glob, season.values)\n",
    "        if i == 0:\n",
    "          sub_title =f'season = {season.values}'\n",
    "\n",
    "        val = value.sel(lat=slice(45,90)) if hemisphere == 'NH' else value.sel(lat=slice(-90,-45))\n",
    "        cf = (val.where(~np.isnan(val))).sel(season=season).plot(ax=ax, transform=ccrs.PlateCarree(), extend =None, add_colorbar=False, cmap=cmap, levels=levels, vmin=vmin, vmax=vmax)\n",
    "\n",
    "        \n",
    "        val2 = hemi_glob.sel(hemisphere=hemisphere, season=season).values.round(2)\n",
    "        if var_name != 'sf_eff':\n",
    "          ax.text(0.05, 0.125, f'{val2:.0f}%',transform=ax.transAxes, fontsize=11, verticalalignment='top',bbox=props)\n",
    "        if var_name == 'sf_eff':\n",
    "          ax.text(0.05, 0.125, f'{val2:.1f}'+'h$^{-1}$',transform=ax.transAxes, fontsize=11, verticalalignment='top',bbox=props)\n",
    "        ax.set_title(sub_title)\n",
    "        \n",
    "        # plot statistic not significant \n",
    "        # take the difference with ERA, just check if this difference is lower then that confidence interval CI to find insignificant pixels\n",
    "\n",
    "        if i == 2 and model != 'ERA5':\n",
    "          diff = diff_res.sel(lat=slice(45,90)) if hemisphere == 'NH' else value.sel(lat=slice(-90,-45))\n",
    "          (diff.where(np.abs(diff) < CI)).sel(season=season).plot.contourf(ax=ax, transform=ccrs.PlateCarree(), colors='none', hatches=[density*'/',density*'/'],add_colorbar=False,)\n",
    "          ax.set_title('')\n",
    "        \n",
    "    if i == 1 or i == 2:\n",
    "        cbar = plt.colorbar(cf, cax=cbaxes, shrink=0.5,extend=None, orientation='vertical', label=dict_label['cb_label'])\n",
    "    \n",
    "    if i == 2:\n",
    "      bb = [0.95, 0.09, 0.0125, 0.05]\n",
    "      axsm.flatten()[i].legend([Patch(facecolor='none', edgecolor='k', hatch=density*'/', label='CI < 95%')\n",
    "        ],\n",
    "        ['CI < 95%'],\n",
    "        bbox_to_anchor=bb,loc=8,ncol=1,borderaxespad=0,fancybox=True,bbox_transform=f.transFigure,\n",
    "    )\n",
    "        \n",
    "        \n",
    "  plt.tight_layout(pad=0., w_pad=0., h_pad=0.)  \n",
    "\n",
    "  figname = f'{model}_{var_name}_season_{hemisphere}_2007_2010.png'\n",
    "  plt.savefig(fig_dir + figname, format='png', bbox_inches='tight', transparent=False)\n",
    "  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for var_name in d.keys():\n",
    "#         for model in ratios['500']['model'].values:\n",
    "#             for hemisphere in ['NH', 'SH']:\n",
    "#                 if model == 'CanESM5' or model == 'IPSL-CM5A2-INCA':\n",
    "#                     res = 500\n",
    "#                 else:\n",
    "#                     res = 250\n",
    "#                 plt_spatial_season_for_model(hemisphere,ratios, var_name, d[var_name], res, model, FIG_DIR,45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get only area as McIlhattan et al. (2017)\n",
    "Arctic circle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIG_DIR_mci =  os.path.join(FIG_DIR, 'McIlhattan/')\n",
    "\n",
    "# make figure directory\n",
    "try:\n",
    "    os.mkdir(FIG_DIR_mci)\n",
    "except OSError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_mci = ds\n",
    "for var in ds_mci.keys():\n",
    "    for model in ds_mci[var].keys():\n",
    "        ds_mci[var][model] = xr.concat([ds[var][model].sel(lat=slice(-90,-66.91)), ds[var][model].sel(lat=slice(66.91,90))], dim='lat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratios_mci = {}\n",
    "for model in list_models:\n",
    "    if model == 'cloudsat_250' or model == 'cloudsat_500':\n",
    "        ds_mci['lcc_2t_days'][model]['lwp'] = xr.DataArray(np.nan, coords=ds_mci['lcc_2t_days'][model]['sf_avg_lcc_snow'].coords, dims=ds_mci['lcc_2t_days'][model]['sf_avg_lcc_snow'].dims)\n",
    "        ratios_mci[model] = xr.merge(objects = [\n",
    "            # get_ratios_season_month(var1 = ds_mci['lcc_2t'][model]['n_lcc'], var2 = ds_mci['2t'][model]['n_cld'], stats = 'sum', out_var = 'lcc_wo_snow', weights = ds_mci['2t'][model]['areacella']),\n",
    "            get_ratios_season_month(var1 = ds_mci['orig'][model]['n_lcc'], var2 = ds_mci['orig'][model]['n_obs'],stats ='sum', out_var = 'FLCC', weights = ds_mci['orig'][model]['areacella']),\n",
    "            get_ratios_season_month(var1 = ds_mci['lcc_2t'][model]['n_lcc'], var2 = ds_mci['orig'][model]['n_obs'], stats = 'sum', out_var = 'FsLCC', weights = ds_mci['orig'][model]['areacella']), #out_var = 'lcc_wo_snow', weights = ds_mci['2t'][model]['areacella']),\n",
    "            get_ratios_season_month(var1 = ds_mci['lcc_sf'][model]['n_sf_lcc_snow'], var2 = ds_mci['orig'][model]['n_lcc'], stats = 'sum', out_var = 'FoP', weights = ds_mci['orig'][model]['areacella']),\n",
    "            get_ratios_season_month(var1 = ds_mci['lcc_2t_sf'][model]['n_sf_lcc_snow'], var2 = ds_mci['lcc_2t'][model]['n_lcc'], stats='sum', out_var = 'FoS', weights=ds_mci['orig'][model]['areacella']), #out_var='lcc_w_snow', weights=ds_mci['lcc_2t'][model]['areacella']),\n",
    "            get_ratios_season_month(var1 = ds_mci['lcc_2t_days'][model]['sf_avg_lcc_snow'], var2 = ds_mci['lcc_2t_days'][model]['lwp'], stats = 'mean', out_var = 'sf_eff', weights = ds_mci['orig'][model]['areacella'])\n",
    "        ])\n",
    "    else:\n",
    "        ratios_mci[model] = xr.merge(objects=[\n",
    "            # get_ratios_season_month(var1=ds_mci['lcc_2t'][model]['lwp'], var2=ds_mci['2t'][model]['twp'].where(ds_mci['2t'][model]['twp']>0.), stats='count', out_var='lcc_wo_snow', weights=ds_mci['2t'][model]['areacella']),              # relative frequency of liquid containing clouds in relation to when there is a cloud\n",
    "            # get_ratios_season_month(var1=ds_mci['lcc_2t'][model]['lwp'], var2=ds_mci['2t'][model]['tas'], stats='count', out_var='lcc_wo_snow', weights=ds_mci['2t'][model]['areacella']), # sLCC frequency compared to all observations when T<0C\n",
    "            ## use of 'tas' in var2 as this has values everywhere where data is valid, while 'lwp' or 'prsn' might not have values\n",
    "            get_ratios_season_month(var1=ds_mci['lcc'][model]['lwp'], var2=ds_mci['orig'][model]['tas'], stats='count', out_var='FLCC', weights=ds_mci['orig'][model]['areacella']),\n",
    "            get_ratios_season_month(var1=ds_mci['lcc_2t'][model]['lwp'], var2=ds_mci['orig'][model]['tas'], stats='count', out_var='FsLCC', weights=ds_mci['orig'][model]['areacella']),#out_var='lcc_wo_snow', weights=ds_mci['2t'][model]['areacella']), # sLCC frequency compared to all observations when T<0C\n",
    "            get_ratios_season_month(var1=ds_mci['lcc_sf'][model]['prsn'], var2=ds_mci['orig'][model]['tas'], stats='count', out_var='FoP', weights=ds_mci['orig'][model]['areacella']),\n",
    "            get_ratios_season_month(var1=ds_mci['lcc_2t_sf'][model]['prsn'], var2=ds_mci['lcc_2t'][model]['lwp'], stats='count', out_var='FoS', weights=ds_mci['orig'][model]['areacella']), #out_var='lcc_w_snow', weights=ds_mci['lcc_2t'][model]['areacella']),   # relative frequency of snowfall from liquid containing clouds\n",
    "            get_ratios_season_month(var1=ds_mci['lcc_2t_days'][model]['prsn'], var2=ds_mci['lcc_2t_days'][model]['lwp'], stats='mean', out_var='sf_eff', weights=ds_mci['orig'][model]['areacella']),      # relative snowfall (precipitation) efficency\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var_name in d.keys():\n",
    "    ratios_mci = get_only_valid_values(ratios_mci, '250', var_name)\n",
    "    ratios_mci = get_only_valid_values(ratios_mci, '500', var_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate weighted averages\n",
    "\n",
    "for model in ratios_mci.keys():\n",
    "    weights = ds['2t'][model]['areacella']\n",
    "    for vars in ratios_mci[model].keys():\n",
    "        ratios_mci[model][vars+'_mean'], ratios_mci[model][vars+'_std'], ratios_mci[model][vars+'_stats'] = weighted_average(ratios_mci[model][vars], weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var_name in d.keys():\n",
    "    for hemisphere in ['NH', 'SH']:\n",
    "        plt_spatial_season(hemisphere, ratios_mci, var_name, d[var_name],FIG_DIR_mci,66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for hemisphere in ['NH', 'SH']:\n",
    "    plt_spatial_season_FLCC_FsLCC(hemisphere,ratios_mci, d['FLCC'], FIG_DIR_mci,66)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var_name in d.keys():\n",
    "    plt_monthly_model_variation(ratios_mci, var_name, d[var_name], FIG_DIR_mci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratios_mci['250'] = xr.concat([ratios_mci['era_250'].assign_coords(coords={'model':'ERA5'}), ratios_mci['cmip_250']], dim=(\"model\"))\n",
    "ratios_mci['500'] = xr.concat([ratios_mci['era_500'].assign_coords(coords={'model':'ERA5'}), ratios_mci['cmip_500']], dim=(\"model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var_name in d.keys():\n",
    "    plt_scatter_obs_model(ratios_mci, var_name, d[var_name], FIG_DIR_mci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_NH_mci = dict()\n",
    "df_SH_mci = dict()\n",
    "df = dict()\n",
    "for var_name in d.keys():\n",
    "    _, R2_var_NH, d_var_NH, _, R2_var_SH, d_var_SH = calc_scatter_obs_model(ratios_mci, var_name)\n",
    "    df_NH_mci[var_name] = (pd.DataFrame({model: np.array(list(R2_var_NH[model].values())) for model in R2_var_NH.keys()}, index=['NDJ', 'MJJ', 'FMA', 'ASO'])).transpose()\n",
    "    df_SH_mci[var_name] = (pd.DataFrame({model: np.array(list(R2_var_SH[model].values())) for model in R2_var_SH.keys()}, index=['NDJ', 'MJJ', 'FMA', 'ASO'])).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set ERA5 correlation number to nan \n",
    "for season in df_NH_mci['sf_eff'].columns:\n",
    "    df_NH_mci['sf_eff'][season].loc['ERA5'] = np.nan\n",
    "    df_SH_mci['sf_eff'][season].loc['ERA5'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_R2_heatmap_season(df_NH_mci, df_SH_mci, d, FIG_DIR_mci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var_name in d.keys():\n",
    "    for hemisphere in ['NH', 'SH']:\n",
    "        # print(hemisphere, var_name, cb_label)\n",
    "        plt_spatial_season_no_diff(hemisphere, ratios_mci, var_name, d[var_name], FIG_DIR_mci,66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for var_name in d.keys():\n",
    "#         for model in ratios_mci['500']['model'].values:\n",
    "#             for hemisphere in ['NH', 'SH']:\n",
    "#                 if model == 'CanESM5' or model == 'IPSL-CM5A2-INCA':\n",
    "#                     res = 500\n",
    "#                 else:\n",
    "#                     res = 250\n",
    "#                 plt_spatial_season_for_model(hemisphere,ratios_mci, var_name, d[var_name], res, model, FIG_DIR_mci,66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "globalsnow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "01a47c9201b0e71806f8317dc994d10479d7bb1c7bfa2fc7a59a724dd50a1c8a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
