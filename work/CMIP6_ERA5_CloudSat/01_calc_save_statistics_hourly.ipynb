{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export PYTHONPATH=\"${PYTHONPATH}:/uio/kant/geo-geofag-u1/franzihe/Documents/Python/globalsnow/CloudSat_ERA5_CMIP6_analysis/utils/\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of CMIP6, ERA5, and CloudSat\n",
    "\n",
    "\n",
    "# Table of Contents\n",
    "<ul>\n",
    "<li><a href=\"#introduction\">1. Introduction</a></li>\n",
    "<li><a href=\"#data_wrangling\">2. Data Wrangling</a></li>\n",
    "<li><a href=\"#exploratory\">3. Exploratory Data Analysis</a></li>\n",
    "<li><a href=\"#conclusion\">4. Conclusion</a></li>\n",
    "<li><a href=\"#references\">5. References</a></li>\n",
    "</ul>\n",
    "\n",
    "# 1. Introduction <a id='introduction'></a>\n",
    "\n",
    "\n",
    "**Questions**\n",
    "* How is the cloud phase and snowfall \n",
    "\n",
    "\n",
    "> **_NOTE:_** .\n",
    "\n",
    "# 2. Data Wrangling <a id='data_wrangling'></a>\n",
    "\n",
    "\n",
    "## Organize my data\n",
    "\n",
    "- Define a prefix for my project (you may need to adjust it for your own usage on your infrastructure).\n",
    "    - input folder where all the data used as input to my Jupyter Notebook is stored (and eventually shared)\n",
    "    - output folder where all the results to keep are stored\n",
    "    - tool folder where all the tools\n",
    "\n",
    "The ERA5 0.25deg data is located in the folder `\\scratch\\franzihe\\`, CloudSat at ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mimi.uio.no\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import sys\n",
    "import socket\n",
    "hostname = socket.gethostname()\n",
    "\n",
    "abs_path = str(pathlib.Path(hostname).parent.absolute())\n",
    "WORKDIR = abs_path[:- (len(abs_path.split('/')[-2] + abs_path.split('/')[-1])+1)]\n",
    "\n",
    "\n",
    "if \"mimi\" in hostname:\n",
    "    print(hostname)\n",
    "    # DATA_DIR = \"/scratch/franzihe/\"\n",
    "    DATA_DIR = \"/mn/vann/franzihe/\"\n",
    "    # FIG_DIR = \"/uio/kant/geo-geofag-u1/franzihe/Documents/Figures/ERA5/\"\n",
    "    # FIG_DIR = \"/uio/kant/geo-geofag-u1/franzihe/Documents/Python/globalsnow/CloudSat_ERA5_CMIP6_analysis/Figures/CS_ERA5_CMIP6/\"\n",
    "elif \"glefsekaldt\" in hostname: \n",
    "    DATA_DIR = \"/home/franzihe/Data/\"\n",
    "    FIG_DIR = \"/home/franzihe/Documents/Figures/ERA5/\"\n",
    "\n",
    "INPUT_DATA_DIR = os.path.join(DATA_DIR, 'input')\n",
    "OUTPUT_DATA_DIR = os.path.join(DATA_DIR, 'output')\n",
    "UTILS_DIR = os.path.join(WORKDIR, 'utils')\n",
    "\n",
    "sys.path.append(UTILS_DIR)\n",
    "# # make figure directory\n",
    "# try:\n",
    "#     os.mkdir(FIG_DIR)\n",
    "# except OSError:\n",
    "#     pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import python packages\n",
    "- `Python` environment requirements: file [requirements_globalsnow.txt](../../requirements_globalsnow.txt) \n",
    "- load `python` packages from [imports.py](../../utils/imports.py)\n",
    "- load `functions` from [functions.py](../../utils/functions.py)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.core.options.set_options at 0x7f49a4132aa0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# supress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # don't output warnings\n",
    "\n",
    "# import packages\n",
    "from imports import(xr, intake, ccrs, cy, plt, glob, cm, fct, np, da, LogNorm, pd)\n",
    "xr.set_options(display_style='html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open variables\n",
    "Get the data requried for the analysis. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "era_in = os.path.join(INPUT_DATA_DIR, 'ERA5')\n",
    "cmip_in = os.path.join(INPUT_DATA_DIR, 'cmip6_hist')\n",
    "dat_out = os.path.join(OUTPUT_DATA_DIR, 'CS_ERA5_CMIP6_hourly')\n",
    "\n",
    "# make output data directory\n",
    "try:\n",
    "    os.mkdir(dat_out)\n",
    "except OSError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_id = ['tas', 'prsn', 'pr', 'lwp', 'clivi', 'areacella']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will load all available models into one dictonary, which includes an xarray dataset with `xarray.open_mfdataset(file)` and select the time range [by name](https://xarray.pydata.org/en/stable/user-guide/indexing.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_models = [\n",
    "               'ERA5',\n",
    "               'MIROC6', \n",
    "               'CanESM5', \n",
    "               'AWI-ESM-1-1-LR', \n",
    "               'MPI-ESM1-2-LR', \n",
    "               'UKESM1-0-LL', \n",
    "               'HadGEM3-GC31-LL',\n",
    "               'CNRM-CM6-1',\n",
    "               'CNRM-ESM2-1',\n",
    "               'IPSL-CM6A-LR',\n",
    "               'IPSL-CM5A2-INCA'\n",
    "            ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "era_dict = {}\n",
    "\n",
    "\n",
    "\n",
    "era_files = [sorted(glob(f'{era_in}/hourly/40NS/hourly_40*ERA5*.nc'))] + \\\n",
    "        sorted(glob(f'{era_in}/common_grid/40NS/ERA5_hourly*_IPSL-CM6A-LR*.nc')) + \\\n",
    "        sorted(glob(f'{era_in}/common_grid/40NS/ERA5_hourly*_IPSL-CM5A2-INCA*.nc'))\n",
    "for file in era_files:\n",
    "    if 'IPSL-CM6A-LR' in file:\n",
    "        res = 'era_250'\n",
    "    elif 'IPSL-CM5A2-INCA' in file:\n",
    "        res = 'era_500'\n",
    "    else:\n",
    "        res = 'era_30'\n",
    "    # res = '250' if 'IPSL-CM6A-LR' in file else '500'\n",
    "    era_dict[res] = xr.open_mfdataset(file)\n",
    "\n",
    "# Remove leap day for ERA files\n",
    "for res in era_dict.keys():\n",
    "    era_dict[res] = era_dict[res].sel(time=~((era_dict[res].time.dt.month == 2) & (era_dict[res].time.dt.day == 29)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cmip_dict = {}\n",
    "\n",
    "# for model in list_models[1:]:\n",
    "#     cmip_file_in = sorted(glob(f'{cmip_in}/single_model/{model}/*_{model}_*40*.nc'))\n",
    "#     cmip_dict[model] = xr.open_mfdataset(cmip_file_in, decode_times =True, use_cftime=True).rename_vars({'clivi': 'iwp'})\n",
    "    \n",
    "#     cmip_dict[model]['twp'] = cmip_dict[model]['lwp'] + cmip_dict[model]['iwp']\n",
    "#     cmip_dict[model] = fct.to_ERA5_date(cmip_dict[model], model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cmip_250 = {}\n",
    "# cmip_500 = {}\n",
    "# for model in list_models[1:]:\n",
    "#     # Read CMIP files\n",
    "#     cmip_files_250 = sorted(glob(f'{cmip_in}/common_grid/{model}/*_IPSL-CM6A-LR_{model}*40*.nc'))\n",
    "#     cmip_files_500 = sorted(glob(f'{cmip_in}/common_grid/{model}/*_IPSL-CM5A2-INCA_{model}*40*.nc'))\n",
    "#     if len(cmip_files_250) != 0:\n",
    "#         cmip_250[model] = xr.open_mfdataset(cmip_files_250, decode_times =True, use_cftime=True).rename_vars({'clivi': 'iwp'})\n",
    "#     cmip_500[model] = xr.open_mfdataset(cmip_files_500, decode_times =True, use_cftime=True).rename_vars({'clivi': 'iwp'})\n",
    "\n",
    "\n",
    "# # Calculate 'twp' variable and convert calendar\n",
    "# for cmip in [cmip_250, cmip_500]:\n",
    "#     for model in cmip.keys():\n",
    "#         cmip[model]['twp'] = cmip[model]['lwp'] + cmip[model]['iwp']\n",
    "#         cmip[model] = fct.to_ERA5_date(cmip[model], model)\n",
    "\n",
    "# _coord = list(cmip_250.keys())\n",
    "# _ds = list(cmip_250.values())\n",
    "# cmip_dict['cmip_250'] = xr.concat(objs=_ds, dim=_coord, coords='all').rename({'concat_dim':'model'})\n",
    "\n",
    "# _coord = list(cmip_500.keys())\n",
    "# _ds = list(cmip_500.values())\n",
    "# cmip_dict['cmip_500'] = xr.concat(objs=_ds, dim=_coord, coords='all').rename({'concat_dim':'model'})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics\n",
    "For variables:\n",
    "- Snowfall [prsn]\n",
    "- Total column cloud liquid, supercooled liqid, and rain water [twp]\n",
    "- Total column cloud ice, snow water [iwp]\n",
    "- 2m-Temperature [tas]\n",
    "\n",
    "1. Find where liquid water path is $\\ge$ 5 g m-2 \n",
    "2. Find where snowfall is $\\ge$ 0.01mm h-1\n",
    "3. Find where 2m-temperature $\\le$ 0 $^o$ C "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# th_lcc = 0.005\n",
    "# th_2t = 273.15\n",
    "# th_frac_days = 0.1\n",
    "# th_tp = 0.01\n",
    "# th_sf = 0.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_days_season(ds):\n",
    "    days_season = xr.DataArray(data = [xr.concat([ds.sel(time=fct.is_season(ds['time.month'], 1, 2)), \n",
    "                                                  ds.sel(time=fct.is_season(ds['time.month'],12,12))], dim='time').sizes['time'],\n",
    "                                       ds.sel(time=fct.is_season(ds['time.month'], 6, 8)).sizes['time'],\n",
    "                                       ds.sel(time=fct.is_season(ds['time.month'], 3, 5)).sizes['time'],\n",
    "                                       ds.sel(time=fct.is_season(ds['time.month'], 9, 11)).sizes['time'],], \n",
    "                                dims={'season'}, \n",
    "                                coords={'season':['DJF', 'JJA', 'MAM', 'SON']})\n",
    "    \n",
    "    _days = []\n",
    "    for month in np.arange(1,13):\n",
    "        _days.append(ds.sel(time=fct.is_season(ds['time.month'], month, month)).sizes['time'])\n",
    "        # print(month, )\n",
    "    days_month = xr.DataArray(data= np.array(_days),\n",
    "                            dims={'month'}, \n",
    "                            coords={'month':np.arange(1,13)} )\n",
    "    \n",
    "    return(days_season, days_month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_lcc_sf(ds, lwp_threshold):\n",
    "    days_season, days_month = calc_days_season(ds)\n",
    "    \n",
    "    # find where 2m-temperature <= 0C or <= threshold\n",
    "    # This should automatically assume that it is already only snow, but it could include supercooled \n",
    "    # rain in the case of total precipitation\n",
    "    th_2t = 273.15\n",
    "    ds_2t = ds.where(ds['tas'] <= th_2t, other=np.nan)\n",
    "    \n",
    "    # 1. find where liquid water >= 0.005 kgm-2 or >= threshold\n",
    "    th_lcc = 0.001*lwp_threshold\n",
    "    ds_lcc_2t = ds_2t.where(ds_2t['lwp']>=th_lcc, other=np.nan)\n",
    "    ds_lcc = ds.where(ds['lwp']>=th_lcc, other=np.nan)\n",
    "    \n",
    "\n",
    "    # # amount of freezing rain\n",
    "    # ds_lcc_2t['prfr'] = (ds_lcc_2t['pr'] - ds_lcc_2t['prsn'])\n",
    "    # ds_lcc_2t['prfr'].attrs = {'units': 'kg m-2 h-1', 'long_name': 'Mean freezing rain rate'}\n",
    "\n",
    "    # # if we want a precip or snowfall threshold apply here\n",
    "    # # find where total precipitation >0 kgm-2h-1 threshold in these liquid containg clouds\n",
    "    # # th_tp = 0.01\n",
    "    # # ds_lcc_2t = ds_lcc_2t.where(ds['pr']>=th_tp, other=np.nan) \n",
    "    # # 2.1 find where snowfall >= 0.24 mmday-1 or >= threshold in these liquid containing clouds, but not temperature threshold\n",
    "    # # multiply by 24 to make it comparable to McIllhattan et al. As they use 0.01mmh-1 as lower threshold\n",
    "    # # applying snowfall days, based on threshold (th_sf). Gives days where snowfall above th_sf and counts days in season and \n",
    "    # # devides by season days\n",
    "    # th_sf = 0.01\n",
    "    # ds_lcc_2t_sf = ds_lcc_2t.where(ds['prsn']>=th_sf, other=np.nan) \n",
    "    # # th_days = (ds_lcc_2t_sf['twp'].groupby('time.season').count(dim='time',keep_attrs=False))/days_season\n",
    "\n",
    "\n",
    "    # create dataset to use for calculating the precipitation efficency. For the precipitation efficency we want to remove th_frac \n",
    "    # days where liquid water content and temperature requirements are met. \n",
    "    # assign percent of snowfall days, required in a pixle, which should be included in the statistics\n",
    "    th_frac = 0.1\n",
    "    th_days_lcc_2t = (ds_lcc_2t['lwp'].groupby('time.season').count(dim='time',keep_attrs=False))/days_season\n",
    "    th_days_lcc    = (ds_lcc['lwp'].groupby('time.season').count(dim='time',keep_attrs=False))/days_season\n",
    "\n",
    "    ds_lcc_2t_season = ds_lcc_2t.groupby('time.season').mean('time', skipna=True, keep_attrs=True)\n",
    "    ds_lcc_2t_season = ds_lcc_2t_season.where(th_days_lcc_2t>=th_frac, other=np.nan)\n",
    "\n",
    "    ds_lcc_season = ds_lcc.groupby('time.season').mean('time', skipna=True, keep_attrs=True)\n",
    "    ds_lcc_season = ds_lcc_season.where(th_days_lcc >= th_frac, other=np.nan)\n",
    "    \n",
    "    # Now create daily dataset based on seasonal supercooled liquid containing cloud days above th_sf, and th_frac\n",
    "    # _mam = ((ds_lcc_2t.sel(time=fct.is_season(ds_lcc_2t['time.month'], 3, 5))).where(th_days_lcc_2t.sel(season='MAM') >=th_frac, other=np.nan)).drop('season')\n",
    "    _jja = ((ds_lcc_2t.sel(time=fct.is_season(ds_lcc_2t['time.month'], 6, 8))).where(th_days_lcc_2t.sel(season='JJA') >=th_frac, other=np.nan)).drop('season')\n",
    "    _son = ((ds_lcc_2t.sel(time=fct.is_season(ds_lcc_2t['time.month'], 9, 11))).where(th_days_lcc_2t.sel(season='SON') >=th_frac, other=np.nan)).drop('season')\n",
    "    # _djf = ((xr.concat([ds_lcc_2t.sel(time=fct.is_season(ds_lcc_2t['time.month'], 1, 2)), \n",
    "    #                 ds_lcc_2t.sel(time=fct.is_season(ds_lcc_2t['time.month'],12,12))], dim='time')).where(th_days_lcc_2t.sel(season='DJF') >=th_frac, other=np.nan)).drop('season')\n",
    "    # ds_lcc_2t_days = xr.merge(objects=[_djf, _jja, _mam, _son])\n",
    "    ds_lcc_2t_days = xr.merge(objects=[_jja, _son])\n",
    "\n",
    "    # _mam = ((ds_lcc.sel(time=fct.is_season(ds_lcc['time.month'], 3, 5))).where(th_days_lcc_2t.sel(season='MAM') >=th_frac, other=np.nan)).drop('season')\n",
    "    _jja = ((ds_lcc.sel(time=fct.is_season(ds_lcc['time.month'], 6, 8))).where(th_days_lcc_2t.sel(season='JJA') >=th_frac, other=np.nan)).drop('season')\n",
    "    _son = ((ds_lcc.sel(time=fct.is_season(ds_lcc['time.month'], 9, 11))).where(th_days_lcc_2t.sel(season='SON') >=th_frac, other=np.nan)).drop('season')\n",
    "    # _djf = ((xr.concat([ds_lcc.sel(time=fct.is_season(ds_lcc['time.month'], 1, 2)), \n",
    "    #                 ds_lcc.sel(time=fct.is_season(ds_lcc['time.month'],12,12))], dim='time')).where(th_days_lcc_2t.sel(season='DJF') >=th_frac, other=np.nan)).drop('season')\n",
    "    # ds_lcc_days = xr.merge(objects=[_djf, _jja, _mam, _son])\n",
    "    ds_lcc_days = xr.merge(objects=[_jja, _son])\n",
    "\n",
    "\n",
    "\n",
    "    # for all the other statistics we want to remove th_frac days where liquid content, temperature, and snowfall requirements are met\n",
    "    # which also means we have to apply the threshold for the total precipitation\n",
    "    # find where total precipitation >= 0.01 kg m-2 h-1 in LCCs with T2<0C\n",
    "    # th_tp = 0.01\n",
    "    # ds_lcc_2t_sf = ds_lcc_2t_days.where(ds_lcc_2t_days['pr'] >=th_tp, other=np.nan)\n",
    "    # find where snowfall >= 0.01 kg m-2 h-1 or >= threshold in these liquid containing clouds. \n",
    "    th_sf = 0.01\n",
    "    # # ds_lcc_2t_sf = ds_lcc_2t_sf.where(ds_lcc_2t_sf['prsn'] >= th_sf, other=np.nan)\n",
    "    # # ds_lcc_2t_sf = ds_lcc_2t_days.where(ds_lcc_2t_days['prsn'] >= th_sf, other=np.nan)\n",
    "    # ds_lcc_2t_sf = ds_lcc_2t.where(ds_lcc_2t['prsn'] >= th_sf, other=np.nan)\n",
    "    # # applying snowfall days, based on threshold (th_sf). Gives days where snowfall above th_sf and counts days in season and devides \n",
    "    # # by season days\n",
    "    # # th_days_sf = (ds_lcc_2t_sf['twp'].groupby('time.season').count(dim='time', keep_attrs=False))/days_season\n",
    "    # ds_lcc_2t_sf_season = ds_lcc_2t_sf.groupby('time.season').mean('time', skipna=True, keep_attrs=True)\n",
    "    # # ds_lcc_2t_sf_season = ds_lcc_2t_season.where(th_days_sf>=th_frac, other=np.nan)   # not needed for statistic\n",
    "    # ds_lcc_2t_sf_season\n",
    "    \n",
    "    # ds_lcc_sf = ds_lcc.where(ds_lcc['prsn'] >= th_sf, other=np.nan)\n",
    "    # ds_lcc_sf_season = ds_lcc_sf.groupby('time.season').mean('time', skipna=True,keep_attrs=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Now create daily dataset based on seasonal supercooled liquid containing cloud days above th_sf, and th_frac\n",
    "    # _mam = ((ds_lcc_2t_sf.sel(time=fct.is_season(ds_lcc_2t_sf['time.month'], 3, 5))).where(th_days_sf.sel(season='MAM') >=th_frac)).drop('season')\n",
    "    # _jja = ((ds_lcc_2t_sf.sel(time=fct.is_season(ds_lcc_2t_sf['time.month'], 6, 8))).where(th_days_sf.sel(season='JJA') >=th_frac)).drop('season')\n",
    "    # _son = ((ds_lcc_2t_sf.sel(time=fct.is_season(ds_lcc_2t_sf['time.month'], 9, 11))).where(th_days_sf.sel(season='SON') >=th_frac)).drop('season')\n",
    "    # _djf = ((xr.concat([ds_lcc_2t_sf.sel(time=fct.is_season(ds_lcc_2t_sf['time.month'], 1, 2)), \n",
    "    #                 ds_lcc_2t_sf.sel(time=fct.is_season(ds_lcc_2t_sf['time.month'],12,12))], dim='time')).where(th_days_sf.sel(season='DJF') >=th_frac)).drop('season')\n",
    "\n",
    "    # ds_lcc_2t_sf_days = xr.merge(objects=[_djf, _jja, _mam, _son])\n",
    "    \n",
    "    \n",
    "    # ds_lcc, ds_lcc_2t_days, #ds_lcc_2t_sf_days\n",
    "    return(ds_2t, \n",
    "           ds_lcc_2t, ds_lcc_2t_days, #ds_lcc_2t_sf,\n",
    "           ds_lcc,    ds_lcc_days, #ds_lcc_sf, \n",
    "           days_season, days_month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistic_to_netcdf(dict_ds, statistic, days_season, days_month):\n",
    "    for model in dict_ds.keys():\n",
    "        dict_ds[model]['days_season'] = days_season[model]\n",
    "        dict_ds[model]['days_month']  = days_month[model]\n",
    "        \n",
    "        if 'areacella' not in list(dict_ds[model].keys()):\n",
    "            weights = fct.area_grid(dict_ds[model]['lat'].data, dict_ds[model]['lon'].data)\n",
    "            weights = weights.fillna(0)\n",
    "            dict_ds[model]['areacella'] = weights\n",
    "        if 'areacella' in list(dict_ds[model].keys()):\n",
    "            weights = dict_ds[model]['areacella'].fillna(0)\n",
    "            dict_ds[model]['areacella'] = weights\n",
    "            \n",
    "        if 'time' in dict_ds[model]['areacella'].coords:\n",
    "            dict_ds[model]['areacella'] = dict_ds[model]['areacella'].isel(time=0).squeeze()    \n",
    "        # try:\n",
    "        #     dict_ds[model]['areacella'] = dict_ds[model]['areacella'].isel(time=0).squeeze()\n",
    "        # except ValueError:\n",
    "        #     print('...')\n",
    "            \n",
    "        starty = dict_ds[model].indexes['time'].year.unique()[0]\n",
    "        endy = dict_ds[model].indexes['time'].year.unique()[-1]\n",
    "        out_dir = f'{dat_out}/{statistic}'\n",
    "        try:\n",
    "            os.mkdir(out_dir)\n",
    "        except OSError:\n",
    "            pass\n",
    "        \n",
    "        dict_ds_NH = dict_ds[model].sel(lat=slice(45,90))\n",
    "        dict_ds_SH = dict_ds[model].sel(lat=slice(-90,-45))\n",
    "        \n",
    "        ds_out = xr.concat([dict_ds_SH, dict_ds_NH], dim='lat')\n",
    "        # if 'model' in dict_ds[model].dims:\n",
    "        #     ds_out = ds_out[['time', 'lat', 'lon', 'season', 'month', 'model']]\n",
    "        # else:\n",
    "        #     ds_out = ds_out[['time', 'lat', 'lon', 'season', 'month']]\n",
    "\n",
    "        \n",
    "        file_out = f'{out_dir}/{model}_{statistic}_{starty}0101-{endy}1231.nc'\n",
    "        print(f'writing file ... {file_out}')\n",
    "        if 'model' in dict_ds[model].dims:\n",
    "            (ds_out.transpose('time', 'lat', 'lon', 'season', 'month', 'model', ..., missing_dims='ignore')).to_netcdf(file_out)\n",
    "        else:\n",
    "        # ds_out.to_netcdf(file_out)\n",
    "            (ds_out.transpose('time', 'lat', 'lon', 'season', 'month',..., missing_dims='ignore')).to_netcdf(file_out)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate statistics in ERA5 ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing file ... /mn/vann/franzihe/output/CS_ERA5_CMIP6_hourly/3_lcc_2t/era_30_3_lcc_2t_20070101-20101231.nc\n",
      "writing file ... /mn/vann/franzihe/output/CS_ERA5_CMIP6_hourly/3_lcc_2t/era_250_3_lcc_2t_20070101-20101231.nc\n",
      "writing file ... /mn/vann/franzihe/output/CS_ERA5_CMIP6_hourly/3_lcc_2t/era_500_3_lcc_2t_20070101-20101231.nc\n",
      "writing file ... /mn/vann/franzihe/output/CS_ERA5_CMIP6_hourly/3_lcc_2t_days/era_30_3_lcc_2t_days_20070101-20101231.nc\n",
      "writing file ... /mn/vann/franzihe/output/CS_ERA5_CMIP6_hourly/3_lcc_2t_days/era_250_3_lcc_2t_days_20070101-20101231.nc\n",
      "writing file ... /mn/vann/franzihe/output/CS_ERA5_CMIP6_hourly/3_lcc_2t_days/era_500_3_lcc_2t_days_20070101-20101231.nc\n",
      "writing file ... /mn/vann/franzihe/output/CS_ERA5_CMIP6_hourly/3_lcc/era_30_3_lcc_20070101-20101231.nc\n",
      "writing file ... /mn/vann/franzihe/output/CS_ERA5_CMIP6_hourly/3_lcc/era_250_3_lcc_20070101-20101231.nc\n",
      "writing file ... /mn/vann/franzihe/output/CS_ERA5_CMIP6_hourly/3_lcc/era_500_3_lcc_20070101-20101231.nc\n",
      "writing file ... /mn/vann/franzihe/output/CS_ERA5_CMIP6_hourly/3_lcc_days/era_30_3_lcc_days_20070101-20101231.nc\n",
      "writing file ... /mn/vann/franzihe/output/CS_ERA5_CMIP6_hourly/3_lcc_days/era_250_3_lcc_days_20070101-20101231.nc\n",
      "writing file ... /mn/vann/franzihe/output/CS_ERA5_CMIP6_hourly/3_lcc_days/era_500_3_lcc_days_20070101-20101231.nc\n"
     ]
    }
   ],
   "source": [
    "# for threshold in range(10,25,5):\n",
    "# for threshold in range(150,200,50):\n",
    "for threshold in [3]:\n",
    "    era_d = era_dict\n",
    "    # print(threshold*0.001)\n",
    "    \n",
    "    ds_2t      = {}\n",
    "    ds_lcc_2t   = {}\n",
    "    ds_lcc_2t_days  = {} \n",
    "    # ds_lcc_2t_sf = {}\n",
    "\n",
    "    ds_lcc = {}\n",
    "    ds_lcc_days = {}\n",
    "    # ds_lcc_sf = {}\n",
    "    days_season = {}\n",
    "    days_month  = {}\n",
    "    print('Calculate statistics in ERA5 ...')\n",
    "    for model in era_d.keys():\n",
    "        ds_2t[model], ds_lcc_2t[model], ds_lcc_2t_days[model], \\\n",
    "            ds_lcc[model], ds_lcc_days[model],  \\\n",
    "                days_season[model], days_month[model] = find_lcc_sf(era_d[model], threshold)\n",
    "                \n",
    "    # print('Calculate statistics in CMIP6 ...')\n",
    "    # for model in cmip_dict.keys():        \n",
    "    #     ds_2t[model], ds_lcc_2t[model], ds_lcc_2t_days[model], ds_lcc_2t_sf[model], \\\n",
    "    #         ds_lcc[model], ds_lcc_days[model], ds_lcc_sf[model], \\\n",
    "    #                 days_season[model], days_month[model] = find_lcc_sf(cmip_dict[model], threshold)\n",
    "                    \n",
    "    # statistic_to_netcdf(era_d, 'orig', days_season, days_month)\n",
    "    # # statistic_to_netcdf(cmip_dict, 'orig', days_season, days_month)\n",
    "    # statistic_to_netcdf(ds_2t, '2t', days_season, days_month)\n",
    "    statistic_to_netcdf(ds_lcc_2t, f'{threshold}_lcc_2t', days_season, days_month )\n",
    "    statistic_to_netcdf(ds_lcc_2t_days, f'{threshold}_lcc_2t_days', days_season, days_month)\n",
    "    # statistic_to_netcdf(ds_lcc_2t_sf, f'{threshold}_lcc_2t_sf', days_season, days_month)\n",
    "    statistic_to_netcdf(ds_lcc, f'{threshold}_lcc', days_season, days_month)\n",
    "    statistic_to_netcdf(ds_lcc_days, f'{threshold}_lcc_days', days_season, days_month)\n",
    "    # statistic_to_netcdf(ds_lcc_sf, f'{threshold}_lcc_sf', days_season, days_month)\n",
    "\n",
    "    del era_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "globalsnow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "01a47c9201b0e71806f8317dc994d10479d7bb1c7bfa2fc7a59a724dd50a1c8a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
