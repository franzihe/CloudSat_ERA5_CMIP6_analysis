{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export PYTHONPATH=\"${PYTHONPATH}:/uio/kant/geo-geofag-u1/franzihe/Documents/Python/globalsnow/CloudSat_ERA5_CMIP6_analysis/utils/\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example with ERA5 high-resolution (~0.25deg) monthly means\n",
    "\n",
    "\n",
    "# Table of Contents\n",
    "<ul>\n",
    "<li><a href=\"#introduction\">1. Introduction</a></li>\n",
    "<li><a href=\"#data_wrangling\">2. Data Wrangling</a></li>\n",
    "<li><a href=\"#exploratory\">3. Exploratory Data Analysis</a></li>\n",
    "<li><a href=\"#conclusion\">4. Conclusion</a></li>\n",
    "<li><a href=\"#references\">5. References</a></li>\n",
    "</ul>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction <a id='introduction'></a>\n",
    "Cloud feedbacks are a major contributor to the spread of climate sensitivity in global climate models (GCMs) ([Zelinka et al. (2020)](https://doi-org.ezproxy.uio.no/10.1029/2019GL085782)]). Among the most poorly understood cloud feedbacks is the one associated with the cloud phase, which is expected to be modified with climate change ([Bjordal et al. (2020)](https://doi-org.ezproxy.uio.no/10.1038/s41561-020-00649-1)). Cloud phase bias, in addition, has significant implications for the simulation of radiative properties and glacier and ice sheet mass balances in climate models. \n",
    "\n",
    "In this context, this work aims to expand our knowledge on how the representation of the cloud phase affects snow formation in GCMs. Better understanding this aspect is necessary to develop climate models further and improve future climate predictions. \n",
    "\n",
    "* Load ERA5 data previously downloaded locally via [Jupyter Notebook - download ERA5](https://github.com/franzihe/download_ERA5)\n",
    "* find clouds: liquid-only, ice-only, mixed-phase\n",
    "* Regridd the ERA5 variables to the same horizontal resolution as high-resolution CMIP6 models with [`xesmf`](https://xesmf.readthedocs.io/en/latest/)\n",
    "* Calculate and plot the seasonal mean of the variable\n",
    "\n",
    "**Questions**\n",
    "* How is the cloud phase and snowfall varying between 1985 and 2014?\n",
    "\n",
    "\n",
    "> **_NOTE:_** We answer questions related to the comparison of CMIP models to ERA5 in another [Jupyter Notebook](../CMIP6_ERA5_CloudSat/plt_seasonal_mean.ipynb)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Wrangling <a id='data_wrangling'></a>\n",
    "\n",
    "This study will compare surface snowfall, ice, and liquid water content from the Coupled Model Intercomparison Project Phase 6 ([CMIP6](https://esgf-node.llnl.gov/projects/cmip6/)) climate models (accessed through [Pangeo](https://pangeo.io/)) to the European Centre for Medium-Range Weather Forecast Re-Analysis 5 ([ERA5](https://www.ecmwf.int/en/forecasts/datasets/reanalysis-datasets/era5)) data from **2007 to 2010**. We conduct statistical analysis at the annual and seasonal timescales to determine the biases in cloud phase and precipitation (liquid and solid) in the CMIP6 models and their potential connection between them. The CMIP6 data analysis can be found in the [Jupyter Notebook for CMIP6](../cmip/CMIP6_hr_1985-2014.ipynb).\n",
    "\n",
    "- Time period: 2007 to 2010\n",
    "- horizonal resolution: ~0.25deg\n",
    "- time resolution: daily atmospheric data \n",
    "- Variables:\n",
    "\n",
    "<span style=\"color:red\">some *!!! Update table! with variables for daily mean values* text</span>.  \n",
    "| shortname     |             Long name                   |      Units    |  levels |\n",
    "| ------------- |:---------------------------------------:| -------------:|--------:|\n",
    "| sf            |    snowfall                             |[m of water eq]| surface |\n",
    "| 2t            |    2 metre temperature                  |  [K]          | surface |\n",
    "| tclw          |   Total column cloud liquid water       |  [kg m-2]     | single  |\n",
    "| tcrw          |   Total column rain water       |  [kg m-2]     | single  |\n",
    "| tciw          |   Total column cloud ice water          |  [kg m-2]     | single  |\n",
    "| tcsw          |   Total column snow water               |  [kg m-2]     | single  |\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organize my data\n",
    "\n",
    "- Define a prefix for my project (you may need to adjust it for your own usage on your infrastructure).\n",
    "    - input folder where all the data used as input to my Jupyter Notebook is stored (and eventually shared)\n",
    "    - output folder where all the results to keep are stored\n",
    "    - tool folder where all the tools\n",
    "\n",
    "The ERA5 0.25deg data is located in the folder `/input/ERA5/daily_means/`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import sys\n",
    "import socket\n",
    "hostname = socket.gethostname()\n",
    "\n",
    "abs_path = str(pathlib.Path(hostname).parent.absolute())\n",
    "WORKDIR = abs_path[:- (len(abs_path.split('/')[-2] + abs_path.split('/')[-1])+1)]\n",
    "\n",
    "\n",
    "if \"mimi\" in hostname:\n",
    "    print(hostname)\n",
    "    DATA_DIR = \"/scratch/franzihe/\"\n",
    "    # FIG_DIR = \"/uio/kant/geo-geofag-u1/franzihe/Documents/Figures/ERA5/\"\n",
    "    FIG_DIR = \"/uio/kant/geo-geofag-u1/franzihe/Documents/Python/globalsnow/CloudSat_ERA5_CMIP6_analysis/Figures/ERA5/\"\n",
    "elif \"glefsekaldt\" in hostname: \n",
    "    DATA_DIR = \"/home/franzihe/Data/\"\n",
    "    FIG_DIR = \"/home/franzihe/Documents/Figures/ERA5/\"\n",
    "\n",
    "INPUT_DATA_DIR = os.path.join(DATA_DIR, 'input')\n",
    "OUTPUT_DATA_DIR = os.path.join(DATA_DIR, 'output')\n",
    "UTILS_DIR = os.path.join(WORKDIR, 'utils')\n",
    "\n",
    "sys.path.append(UTILS_DIR)\n",
    "# make figure directory\n",
    "try:\n",
    "    os.mkdir(FIG_DIR)\n",
    "except OSError:\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import python packages\n",
    "- `Python` environment requirements: file [requirements_globalsnow.txt](../../requirements_globalsnow.txt) \n",
    "- load `python` packages from [imports.py](../../utils/imports.py)\n",
    "- load `functions` from [functions.py](../../utils/functions.py)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# supress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # don't output warnings\n",
    "\n",
    "# import packages\n",
    "from imports import(xr, intake, ccrs, cy, plt, glob, cm, fct, np, da, LogNorm, pd)\n",
    "xr.set_options(display_style='html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open ERA5 variables\n",
    "Get the data requried for the analysis. Beforehand we downloaded the monthly averaged data on single levels and pressure levels via the Climate Data Store (CDS) infrastructure. The github repository [Download ERA5](https://github.com/franzihe/download_ERA5) gives examples on how to download the data from the CDS. We use the Jupyter Notebooks [download_Amon_single_level](https://github.com/franzihe/download_ERA5/blob/main/download_Amon_single_level.ipynb) and [download_Amon_pressure_level](https://github.com/franzihe/download_ERA5/blob/main/download_Amon_pressure_level.ipynb). Both, download the monthly means for the variables mentioned above between 1985 and 2014.\n",
    "\n",
    "> **_NOTE:_** To download from CDS a user has to have a CDS user account, please create the account [here](https://cds.climate.copernicus.eu/user/register).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "era_in = os.path.join(INPUT_DATA_DIR, 'ERA5/daily_means/')\n",
    "era_out = os.path.join(INPUT_DATA_DIR, 'ERA5/common_grid/')\n",
    "# make output data directory\n",
    "try:\n",
    "    os.mkdir(era_out)\n",
    "except OSError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed for regridding ERA5 data to CMIP6 grids\n",
    "cmip_in = os.path.join(INPUT_DATA_DIR, 'cmip6_hist/single_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_id=[\n",
    "            'sf', \n",
    "            'tclw',\n",
    "            'tcrw',\n",
    "            'tciw',\n",
    "            'tcsw',\n",
    "            '2t', \n",
    "            'tp',\n",
    "            'mtpr',\n",
    "            'msr'\n",
    "             ]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the moment we have downloaded 30 years (1985-2014) for ERA5. We define start and end year to ensure to only extract the 30-year period between 1985 and 2014.\n",
    "\n",
    "$\\rightarrow$ Define a start and end year\n",
    "\n",
    "We will load all available variables into one xarray dataset with `xarray.open_mfdataset(file)` and select the time range [by name](https://xarray.pydata.org/en/stable/user-guide/indexing.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rsync -av --progress login.nird.sigma2.no:/projects/NS9600K/data/ERA5/monthly_means/0.25deg /scratch/franzihe/input/ERA5/monthly_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starty = 2007\n",
    "endy = 2010\n",
    "year_range = range(starty, endy + 1)\n",
    "\n",
    "\n",
    "# Uncomment the line below if you want to search for files matching the pattern in the directory specified by `era_in`\n",
    "# era_file_in = glob(f'{era_in}/*_ERA5_*.nc')\n",
    "# era_file_in = sorted(glob(f'{era_in}/40NS/*_ERA5_*.nc'))\n",
    "era_file_in = []\n",
    "for var_id in variable_id:\n",
    "    era_file_in.extend(sorted(glob(f'{era_in}/40NS/{var_id}*_ERA5_*.nc')))\n",
    "\n",
    "ds_era = xr.open_mfdataset(era_file_in, )\n",
    "# Uncomment the line below if you want to rename the variable `t2m` to `2t`\n",
    "ds_era = ds_era.rename_vars({'t2m': '2t'})\n",
    "\n",
    "ds_era = ds_era.sel(time=ds_era.time.dt.year.isin(year_range)).squeeze()\n",
    "\n",
    "ds_era = ds_era.assign_coords(lon=(((ds_era.lon + 180) % 360) - 180)).sortby(['lon', 'time'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_era.time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove leap day\n",
    "ds_era = ds_era.sel(time=~((ds_era.time.dt.month == 2) & (ds_era.time.dt.day == 29)))\n",
    "\n",
    "# Remove the 'realization' dimension\n",
    "ds_era = ds_era.drop('realization')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change attributes matching CMIP6 data\n",
    "\n",
    "We will assign the attributes to the variables to make CMIP6 and ERA5 variables comperable.\n",
    "\n",
    "The data [documentation of daily means](https://cds.climate.copernicus.eu/cdsapp#!/software/app-c3s-daily-era5-statistics?tab=doc) gives information about the accumulations in daily means. Compute daily statistics in local time (via the Time zone widget) of variables from a number of hourly ERA5 reanalysis datasets. Before computing the daily statistics the ERA5 hourly data can be subsampled in time (using the Frequency widget) and space (using the Grid and Area widget). Further details can be found in the application [Overview](https://cds.climate.copernicus.eu/cdsapp#!/software/app-c3s-daily-era5-statistics?tab=overview) and [Documentation](https://cds.climate.copernicus.eu/cdsapp#!/software/app-c3s-daily-era5-statistics?tab=doc)\n",
    "* [`sf`](https://apps.ecmwf.int/codes/grib/param-db?id=144) is in **m** $\\rightarrow$  multiply by **1000** to get **kg m-2** or **mm**.\n",
    "* [`tp`](https://apps.ecmwf.int/codes/grib/param-db?id=228) is in **m** $\\rightarrow$  multiply by **1000** to get **kg m-2** or **mm**.\n",
    "\n",
    "* [`msr`](https://apps.ecmwf.int/codes/grib/param-db?id=235031) is in **kg m-2 s-1** $\\rightarrow$ multiply by **3600s** to get **kg m-2 h-1**.\n",
    "* [`mtpr`](https://apps.ecmwf.int/codes/grib/param-db?id=235055) is in **kg m-2 s-1** $\\rightarrow$ multiply by **3600s** to get **kg m-2 h-1**.\n",
    "\n",
    "* [`tciw`](https://apps.ecmwf.int/codes/grib/param-db?id=79) and [`tclw`](https://apps.ecmwf.int/codes/grib/param-db?id=78) is in **kg m-2** \n",
    "\n",
    "\n",
    "**msr:** \tThis parameter is the rate of snowfall at the Earth's surface. It is the sum of large-scale and convective snowfall. Large-scale snowfall is generated by the cloud scheme in the ECMWF Integrated Forecasting System (IFS). The cloud scheme represents the formation and dissipation of clouds and large-scale precipitation due to changes in atmospheric quantities (such as pressure, temperature and moisture) predicted directly at spatial scales of the grid box or larger. Convective snowfall is generated by the convection scheme in the IFS, which represents convection at spatial scales smaller than the grid box. In the IFS, precipitation is comprised of rain and snow. This parameter is a mean over a particular time period (the processing period) which depends on the data extracted. For the reanalysis, the processing period is over the 1 hour ending at the validity date and time. For the ensemble members, ensemble mean and ensemble spread, the processing period is over the 3 hours ending at the validity date and time. It is the rate the snowfall would have if it were spread evenly over the grid box. 1 kg of water spread over 1 square metre of surface is 1 mm deep (neglecting the effects of temperature on the density of water), therefore the units are equivalent to mm (of liquid water) per second. Care should be taken when comparing model parameters with observations, because observations are often local to a particular point in space and time, rather than representing averages over a model grid box.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this optimized version, you define a dictionary conversion_factors that maps each variable ID (var_id) \n",
    "# to its corresponding conversion factor and attributes. The code then iterates over the keys of the dataset \n",
    "# ds_era and checks if the variable ID exists in the conversion_factors dictionary. If it does, it retrieves \n",
    "# the conversion factor and attributes for that variable and performs the necessary operations.\n",
    "\n",
    "conversion_factors = {\n",
    "    'sf': {'factor': 1000, 'attrs': {'units': 'kg m-2', 'long_name': 'Total Snowfall per day'}},\n",
    "    'tp': {'factor': 1000, 'attrs': {'units': 'kg m-2', 'long_name': 'Total Snowfall per day'}},\n",
    "    'msr': {'factor': 3600, 'attrs': {'units': 'kg m-2 h-1', 'long_name': 'Mean snowfall rate'}},\n",
    "    'mtpr': {'factor': 3600, 'attrs': {'units': 'kg m-2 h-1', 'long_name': 'Mean total precipitation rate'}}\n",
    "}\n",
    "\n",
    "for var_id in ds_era.keys():\n",
    "    if var_id in conversion_factors:\n",
    "        factor = conversion_factors[var_id]['factor']\n",
    "        attrs = conversion_factors[var_id]['attrs']\n",
    "\n",
    "        ds_era[var_id] = ds_era[var_id] * factor\n",
    "        ds_era[var_id].attrs = attrs\n",
    "        ds_era[var_id] = ds_era[var_id].where(ds_era[var_id] >= 0., other=np.nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var_id in ds_era.keys():\n",
    "    print(var_id, ds_era[var_id].attrs['units'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specific ice and snow water content\n",
    "\n",
    "To get all all frozen particles in the column we have to add the variables `specific cloud ice content` and `specific snow water content`.\n",
    "\n",
    "$$\\sum clic + cswc = cisc$$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_era['cisc'] = ds_era['clic'] + ds_era['cswc']\n",
    "# ds_era['cisc'].attrs = {'units': 'kg kg-1', 'long_name':'Specific cloud ice and snow water content'}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total column cloud ice water and total column cloud snow water\n",
    "\n",
    "To get all all frozen particles in the column we have to add the variables `total_column_cloud_ice_water` and `\ttotal_column_snow_water`.\n",
    "\n",
    "$$\\sum tciw + tcsw = iwp$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_era['iwp'] = ds_era['tciw'] + ds_era['tcsw']\n",
    "ds_era['iwp'].attrs = {'units': 'kg m-2', 'long_name': 'Daily average total ice water path'}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total column cloud liquid water, total column supercooled liquid water, total column rain water\n",
    "\n",
    "To get all all liquid particles in the column we have to add the variables `total_column_cloud_liquid_water`, `total_column_supercooled_liquid_water`, and `total_column_rain_water`.\n",
    "\n",
    "$$\\sum tclw + tcslw + tcrw = lwp$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_era['lwp'] = ds_era['tclw'] + ds_era['tcrw'] \n",
    "ds_era['lwp'].attrs = {'units': 'kg m-2', 'long_name': 'Daily average total liquid water path'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_era['twp'] = ds_era['iwp'] +ds_era['lwp']\n",
    "ds_era['twp'].attrs = {'units': 'kg m-2', 'long_name': 'Daily average total water path'}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save ERA5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starty = 2006; endy = 2009\n",
    "counter = 0\n",
    "\n",
    "# Rename variables and drop unused variables\n",
    "ds_era = ds_era.rename_vars({'msr': 'prsn', 'mtpr': 'pr', '2t': 'tas'})\n",
    "ds_era = ds_era.drop_vars(('sf', 'tciw', 'tclw', 'tcrw', 'tcslw', 'tcsw', 'tcw', 'tp'), errors=\"ignore\")\n",
    "\n",
    "ds_era\n",
    "filename = f'daily_mean_40NS_ERA5_{starty+1}01_{endy+1}12.nc'\n",
    "# filename = f'daily_mean_40NS_ERA5_{starty+1}01_{endy+1}12.nc'\n",
    "\n",
    "    \n",
    "era_file_out = os.path.join(era_in, f'40NS/{filename}')\n",
    "# era_file_out = os.path.join(era_in, filename)\n",
    "\n",
    "if os.path.exists(era_file_out):\n",
    "    ds_era.to_netcdf(era_file_out)\n",
    "    print('file written: {}'.format(era_file_out))\n",
    "        #     print(f'{era_file_out} is downloaded')\n",
    "        #     counter += 1\n",
    "        #     print(f'Have regridded in total: {counter} files')\n",
    "else:\n",
    "    ds_era.to_netcdf(era_file_out)\n",
    "    print('file written: {}'.format(era_file_out))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regrid ERA5 data to common CMIP6 model grid <a id='regrid_hz'></a>\n",
    "\n",
    "We want to conduct statistical analysis at the annual and seasonal timescales to determine the biases in cloud phase and precipitation (liquid and solid) for the CMIP6 models in comparison to ERA5. \n",
    "\n",
    "The ERA5 data has a nominal resolution of 0.25 deg and has to be regridded to the same horizontal resolution as the CMIP6 model. Hence we will make use of the python package `xesmf` and [decreasing resolution](https://xesmf.readthedocs.io/en/latest/notebooks/Compare_algorithms.html#Decreasing-resolution), [Limitations and warnings](https://xesmf.readthedocs.io/en/latest/notebooks/Masking.html?highlight=conservative#Limitations-and-warnings).  \n",
    "\n",
    "$\\rightarrow$ Define CMIP6 model as the reference grid `ds_out`.\n",
    "\n",
    "Save all variables in one file and each variable to a `netcdf` datasets between 2007 an 2010, locally.\n",
    "\n",
    "> **_NOTE:_** This can take a while, so be patient\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "year_range = range(starty, endy+1)\n",
    "# source_id\n",
    "list_models = [\n",
    "               'ERA5',\n",
    "               'MIROC6', \n",
    "               'CanESM5', \n",
    "               'AWI-ESM-1-1-LR', \n",
    "               'MPI-ESM1-2-LR', \n",
    "               'UKESM1-0-LL', \n",
    "               'HadGEM3-GC31-LL',\n",
    "               'CNRM-CM6-1',\n",
    "               'CNRM-ESM2-1',\n",
    "               'IPSL-CM6A-LR',\n",
    "               'IPSL-CM5A2-INCA'\n",
    "            ]\n",
    "\n",
    "## experiment\n",
    "experiment_id = ['historical']\n",
    "\n",
    "## time resolution\n",
    "t_res = ['day',]\n",
    "\n",
    "variable_id = ['areacella', 'prsn']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search corresponding data, select Northern and Southern Hemisphere, regrid ERA5 data to CMIP6 models resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in list_models:\n",
    "    if model == 'ERA5':\n",
    "        era_in_grid = ds_era\n",
    "        filename = f'ERA5_daily_mean_{model}_{starty+1}01_{endy+1}12.nc'\n",
    "    else:\n",
    "        if model in ['UKESM1-0-LL', 'HadGEM3-GC31-LL']:\n",
    "            cmip_file_in = glob(f'{cmip_in}/{model}/{variable_id[1]}_day_{model}*')\n",
    "        else:\n",
    "            cmip_file_in = glob(f'{cmip_in}/{model}/{variable_id[0]}_fx_*{model}*')\n",
    "        \n",
    "        if len(cmip_file_in) == 0:\n",
    "            continue\n",
    "        ds_cmip = xr.open_dataset(cmip_file_in[0], drop_variables=variable_id)\n",
    "            \n",
    "        # Shift longitude to be from -180 to 180\n",
    "        ds_cmip = ds_cmip.assign_coords(lon=(((ds_cmip['lon'] + 180) % 360) - 180)).sortby('lon')\n",
    "            \n",
    "        # Regrid data\n",
    "        SH = fct.regrid_data(ds_era.sel(lat=slice(-90,-40)), ds_cmip.sel(lat=slice(-90,-40)))\n",
    "        NH = fct.regrid_data(ds_era.sel(lat=slice(40,90)), ds_cmip.sel(lat=slice(40,90)))\n",
    "            \n",
    "        era_in_grid = xr.concat([SH, NH], 'lat')\n",
    "            \n",
    "        filename = f'ERA5_daily_mean_{model}_{starty}01_{endy}12.nc'\n",
    "    \n",
    "    era_file_out = os.path.join(era_out, f'40NS/{filename}')\n",
    "    \n",
    "\n",
    "    if os.path.exists(era_file_out):\n",
    "        era_in_grid.to_netcdf(era_file_out)\n",
    "        print('file written: {}'.format(era_file_out))\n",
    "        #     print(f'{era_file_out} is downloaded')\n",
    "        #     counter += 1\n",
    "        #     print(f'Have regridded in total: {counter} files')\n",
    "    else:\n",
    "        era_in_grid.to_netcdf(era_file_out)\n",
    "        print('file written: {}'.format(era_file_out))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count the days in one season over year range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_season(month,lower_val, upper_val):\n",
    "    return (month >= lower_val) & (month <= upper_val)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days_season = xr.DataArray(data = [xr.concat([ds_era.sel(time=is_season(ds_era['time.month'], 1, 2)), ds_era.sel(time=is_season(ds_era['time.month'],12,12))], dim='time').sizes['time'],\n",
    "                                   ds_era.sel(time=is_season(ds_era['time.month'], 6, 8)).sizes['time'],\n",
    "                                   ds_era.sel(time=is_season(ds_era['time.month'], 3, 5)).sizes['time'],\n",
    "                                   ds_era.sel(time=is_season(ds_era['time.month'], 9, 11)).sizes['time'],], \n",
    "                           dims={'season'}, \n",
    "                           coords={'season':['DJF', 'JJA', 'MAM', 'SON']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_days = []\n",
    "for month in np.arange(1,13):\n",
    "    _days.append(ds_era.sel(time=is_season(ds_era['time.month'], month, month)).sizes['time'])\n",
    "    # print(month, )\n",
    "days_month = xr.DataArray(data= np.array(_days),\n",
    "                          dims={'month'}, \n",
    "                          coords={'month':np.arange(1,13)} )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supercooled liquid water fraction\n",
    "\n",
    "$$SLF = \\frac{\\text{cloud liquid water content}}{\\text{cloud liquid water content} + \\text{cloud ice water content}}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics\n",
    "For variables:\n",
    "- Snowfall [sf]\n",
    "- Total column cloud liquid, supercooled liqid, and rain water [tclslrw]\n",
    "- Total column cloud ice, snow water [iwp]\n",
    "- 2m-Temperature [2t]\n",
    "\n",
    "1. Find where liquid water path is $\\ge$ 5 g m-2 \n",
    "2. Find where 2m-temperature $\\le$ 0 $^o$ C \n",
    "\n",
    "(3. Find where snowfall is $\\ge$ 0.01mm h-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. find where liquid water >= 0.005 kgm-2 or >= threshold\n",
    "th_lcc = 0.005\n",
    "ds_era_lcc = ds_era.where(ds_era['lwp']>=th_lcc, other=np.nan)\n",
    "# find where 2m-temperature <= 0C or <= threshold\n",
    "# This should automatically assume that it is already only snow, but it could include supercooled \n",
    "# rain in the case of total precipitation\n",
    "th_2t = 273.15\n",
    "ds_era_lcc_2t = ds_era_lcc.where(ds_era['2t'] <= th_2t, other=np.nan)\n",
    "\n",
    "# amount of freezing rain\n",
    "ds_era_lcc_2t['mfrr'] = (ds_era_lcc_2t['mtpr'] - ds_era_lcc_2t['msr'])\n",
    "ds_era_lcc_2t['mfrr'].attrs = {'units': 'kg m-2 h-1', 'long_name': 'Mean freezing rain rate'}\n",
    "\n",
    "# # if we want a precip or snowfall threshold apply here\n",
    "# # find where total precipitation >0 kgm-2h-1 threshold in these liquid containg clouds\n",
    "# # th_tp = 0.01\n",
    "# # ds_era_lcc_2t = ds_era_lcc_2t.where(ds_era['mtpr']>=th_tp, other=np.nan) \n",
    "# # 2.1 find where snowfall >= 0.24 mmday-1 or >= threshold in these liquid containing clouds, but not temperature threshold\n",
    "# # multiply by 24 to make it comparable to McIllhattan et al. As they use 0.01mmh-1 as lower threshold\n",
    "# # applying snowfall days, based on threshold (th_sf). Gives days where snowfall above th_sf and counts days in season and \n",
    "# # devides by season days\n",
    "# th_sf = 0.01\n",
    "# ds_era_lcc_2t_sf = ds_era_lcc_2t.where(ds_era['msr']>=th_sf, other=np.nan) \n",
    "# # th_days = (ds_era_lcc_2t_sf['twp'].groupby('time.season').count(dim='time',keep_attrs=False))/days_season\n",
    "\n",
    "\n",
    "# create dataset to use for calculating the precipitation efficency. For the precipitation efficency we want to remove th_frac \n",
    "# days where liquid water content and temperature requirements are met. \n",
    "# assign percent of snowfall days, required in a pixle, which should be included in the statistics\n",
    "th_frac = 0.1\n",
    "th_days_lcc_2t = (ds_era_lcc_2t['twp'].groupby('time.season').count(dim='time',keep_attrs=False))/days_season\n",
    "\n",
    "ds_era_lcc_2t_season = ds_era_lcc_2t.groupby('time.season').mean('time', skipna=True, keep_attrs=True)\n",
    "ds_era_lcc_2t_season = ds_era_lcc_2t_season.where(th_days_lcc_2t>=th_frac)\n",
    "\n",
    "# for all the other statistics we want to remove th_frac days where liquid content, temperature, and snowfall requirements are met\n",
    "# which also means we have to apply the threshold for the total precipitation\n",
    "# find where total precipitation >= 0.01 kg m-2 h-1 in LCCs with T2<0C\n",
    "th_tp = 0.01\n",
    "ds_era_lcc_2t_sf = ds_era_lcc_2t.where(ds_era_lcc_2t['mtpr'] >=th_tp, other=np.nan)\n",
    "# find where snowfall >= 0.01 kg m-2 h-1 or >= threshold in these liquid containing clouds. \n",
    "th_sf = 0.01\n",
    "ds_era_lcc_2t_sf = ds_era_lcc_2t_sf.where(ds_era_lcc_2t_sf['msr'] >= th_sf, other=np.nan)\n",
    "# applying snowfall days, based on threshold (th_sf). Gives days where snowfall above th_sf and counts days in season and devides \n",
    "# by season days\n",
    "th_days_sf = (ds_era_lcc_2t_sf['twp'].groupby('time.season').count(dim='time', keep_attrs=False))/days_season\n",
    "ds_era_lcc_2t_sf_season = ds_era_lcc_2t_sf.groupby('time.season').mean('time', skipna=True, keep_attrs=True)\n",
    "ds_era_lcc_2t_sf_season = ds_era_lcc_2t_season.where(th_days_sf>=th_frac)\n",
    "ds_era_lcc_2t_sf_season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now create daily dataset based on seasonal supercooled liquid containing cloud days above th_sf, and th_frac\n",
    "_mam = ((ds_era_lcc_2t.sel(time=is_season(ds_era_lcc_2t['time.month'], 3, 5))).where(th_days_lcc_2t.sel(season='MAM') >=th_frac)).drop('season')\n",
    "_jja = ((ds_era_lcc_2t.sel(time=is_season(ds_era_lcc_2t['time.month'], 6, 8))).where(th_days_lcc_2t.sel(season='JJA') >=th_frac)).drop('season')\n",
    "_son = ((ds_era_lcc_2t.sel(time=is_season(ds_era_lcc_2t['time.month'], 9, 11))).where(th_days_lcc_2t.sel(season='SON') >=th_frac)).drop('season')\n",
    "_djf = ((xr.concat([ds_era_lcc_2t.sel(time=is_season(ds_era_lcc_2t['time.month'], 1, 2)), \n",
    "                  ds_era_lcc_2t.sel(time=is_season(ds_era_lcc_2t['time.month'],12,12))], dim='time')).where(th_days_lcc_2t.sel(season='DJF') >=th_frac)).drop('season')\n",
    "\n",
    "ds_era_lcc_2t_days = xr.merge(objects=[_djf, _jja, _mam, _son])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now create daily dataset based on seasonal supercooled liquid containing cloud days above th_sf, and th_frac\n",
    "_mam = ((ds_era_lcc_2t_sf.sel(time=is_season(ds_era_lcc_2t_sf['time.month'], 3, 5))).where(th_days_sf.sel(season='MAM') >=th_frac)).drop('season')\n",
    "_jja = ((ds_era_lcc_2t_sf.sel(time=is_season(ds_era_lcc_2t_sf['time.month'], 6, 8))).where(th_days_sf.sel(season='JJA') >=th_frac)).drop('season')\n",
    "_son = ((ds_era_lcc_2t_sf.sel(time=is_season(ds_era_lcc_2t_sf['time.month'], 9, 11))).where(th_days_sf.sel(season='SON') >=th_frac)).drop('season')\n",
    "_djf = ((xr.concat([ds_era_lcc_2t_sf.sel(time=is_season(ds_era_lcc_2t_sf['time.month'], 1, 2)), \n",
    "                  ds_era_lcc_2t_sf.sel(time=is_season(ds_era_lcc_2t_sf['time.month'],12,12))], dim='time')).where(th_days_sf.sel(season='DJF') >=th_frac)).drop('season')\n",
    "\n",
    "ds_era_lcc_2t_sf_days = xr.merge(objects=[_djf, _jja, _mam, _son])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_precip_cloud(dset):\n",
    "#     # 1. find where liquid_water >=0.005 kgm-2\n",
    "#     snowfall = dset['sf'].where(dset['lwp']>=0.005, other= np.nan)\n",
    "#     liquid_water = dset['lwp'].where(dset['lwp']>=0.005, other= np.nan)\n",
    "#     ice_water = dset['iwp'].where(dset['lwp']>=0.005, other= np.nan)\n",
    "#     total_water = dset['twp'].where(dset['lwp']>=0.005, other= np.nan)\n",
    "#     temperature = dset['2t'].where(dset['lwp']>=0.005, other= np.nan)\n",
    "    \n",
    "#     # 2. find where snowfall > 0.01mm h-1\n",
    "#     unit_snowfall = dset['sf']\n",
    "#     snowfall = snowfall.where(unit_snowfall>=0.01*24, other= np.nan) # multiply by 24 to make it comparable to McIllhattan et al. As they use 0.01mmh-1 as lower threshold\n",
    "#     liquid_water = liquid_water.where(unit_snowfall>=0.01*24, other= np.nan)\n",
    "#     ice_water = ice_water.where(unit_snowfall>=0.01*24, other= np.nan)\n",
    "#     total_water = total_water.where(unit_snowfall>=0.01*24, other= np.nan)\n",
    "#     temperature = temperature.where(unit_snowfall>=0.01*24, other= np.nan)\n",
    "        \n",
    "#     # # 3. find where 2m-temperature <= 0C\n",
    "#     snowfall = snowfall.where(dset['2t']<=273.15, other= np.nan)\n",
    "#     liquid_water = liquid_water.where(dset['2t']<=273.15, other= np.nan)\n",
    "#     ice_water = ice_water.where(dset['2t']<=273.15, other= np.nan)\n",
    "#     total_water = total_water.where(dset['2t']<=273.15, other= np.nan)\n",
    "#     temperature = temperature.where(dset['2t']<=273.15, other= np.nan)\n",
    "        \n",
    "#     snowfall_count = snowfall.groupby('time.season').count(dim='time',keep_attrs=True)\n",
    "#     # liquid_water_count = liquid_water.groupby('time.season').count(dim='time',keep_attrs=True)\n",
    "#     # ice_water_count = ice_water.groupby('time.season').count(dim='time', keep_attrs=True)\n",
    "#     # temperature_count = temperature.groupby('time.season').count(dim='time', keep_attrs=True)\n",
    "    \n",
    "#     return(snowfall_count, snowfall, ice_water, liquid_water, total_water)\n",
    "\n",
    "\n",
    "# ds_era['lcc_count'],ds_era['sf_lcc'],ds_era['iwp_lcc'],ds_era['lwp_lcc'], ds_era['twp_lcc'] = find_precip_cloud(ds_era)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How often do we have SLC with thresholds for surface snowfall, liquid water path, and temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset with all cumm\n",
    "cumm = xr.Dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the days per season when there was a LCC, and 2t<=0C, and where 10% per season a lcc cloud existed when also the surface temp was below freezing\n",
    "cumm['lcc_days'] = (ds_era_lcc_2t_days['twp'].groupby('time.season').count(dim='time',keep_attrs=False))/days_season\n",
    "print('min:', cumm['lcc_days'].min().round(3).values,\n",
    "      'max:', cumm['lcc_days'].max().round(3).values,  \n",
    "      'std:', cumm['lcc_days'].std(skipna=True).round(3).values, \n",
    "      'mean:', cumm['lcc_days'].mean(skipna=True).round(3).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figname = 'cum_lcc_days_season_{}_{}.png'.format(starty, endy)\n",
    "fct.plt_seasonal_NH_SH((cumm['lcc_days'].where(cumm['lcc_days'] > 0.))*100, levels=np.arange(0,110,10), cbar_label='Frequency of liquid containing cloud days (%)', plt_title='ERA5 ({} - {})'.format(starty,endy), extend=None)\n",
    "plt.savefig(FIG_DIR + figname, format = 'png', bbox_inches = 'tight', transparent = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumm['sf_days'] = (ds_era_lcc_2t_sf_days['twp'].groupby('time.season').count(dim='time',keep_attrs=False))/days_season\n",
    "print('min:', cumm['sf_days'].min().round(3).values,\n",
    "      'max:', cumm['sf_days'].max().round(3).values,  \n",
    "      'std:', cumm['sf_days'].std(skipna=True).round(3).values, \n",
    "      'mean:', cumm['sf_days'].mean(skipna=True).round(3).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figname = 'cum_sf_days_season_{}_{}.png'.format(starty, endy)\n",
    "fct.plt_seasonal_NH_SH((cumm['sf_days'].where(cumm['sf_days'] >0.))*100, levels=np.arange(0,110,10), cbar_label='Frequency of snowfall days from LCCs (%)', plt_title='ERA5 ({} - {})'.format(starty,endy), extend=None)\n",
    "plt.savefig(FIG_DIR + figname, format = 'png', bbox_inches = 'tight', transparent = False)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Liquid containing cloud frequency\n",
    "\n",
    " Relative frequency\n",
    "\n",
    "$$RF = \\frac{f}{n} = \\frac{\\text{number of times the data occurred in an observation}}{\\text{total frequency}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratios = xr.Dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relative frequency of liquid containing clouds per season\n",
    "f = ds_era_lcc_2t_days['lwp'].groupby('time.season').count(dim='time', keep_attrs=False) # number of times a super cooled liquid water cloud occured when the liquid water path is larger than 5gm-2, and temperature below 0degC\n",
    "n = ds_era_lcc['lwp'].groupby('time.season').count(dim='time', keep_attrs=False)         # frequency of liquid clouds (only LWP threshold applied) when lwp is larger than 5gm-2\n",
    "\n",
    "ratios['lcc_wo_snow_season'] = f/n\n",
    "\n",
    "print('min:', ratios['lcc_wo_snow_season'].min().round(3).values,\n",
    "      'max:', ratios['lcc_wo_snow_season'].max().round(3).values,  \n",
    "      'std:', ratios['lcc_wo_snow_season'].std(skipna=True).round(3).values, \n",
    "      'mean:', ratios['lcc_wo_snow_season'].mean(skipna=True).round(3).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fct.plt_seasonal_NH_SH((ratios['lcc_wo_snow_season'].where(ratios['lcc_wo_snow_season']>0.))*100, np.arange(0,110,10), 'Liquid containing cloud frequency (%)', plt_title='ERA5 ({} - {})'.format(starty,endy), extend=None)\n",
    "\n",
    "figname = '{}_rf_lcc_wo_snow_season_{}_{}.png'.format('ERA5',starty, endy)\n",
    "plt.savefig(FIG_DIR + figname, format = 'png', bbox_inches = 'tight', transparent = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relative frequency of liquid containing clouds per month\n",
    "f = ds_era_lcc_2t_days['lwp'].groupby('time.month').count(dim='time', keep_attrs=False) # number of times a super cooled liquid water cloud occured when the liquid water path is larger than 5gm-2, and temperature below 0degC\n",
    "n = ds_era_lcc['lwp'].groupby('time.month').count(dim='time', keep_attrs=False)         # frequency of liquid clouds (only LWP threshold applied) when lwp is larger than 5gm-2\n",
    "\n",
    "ratios['lcc_wo_snow_month'] = f/n\n",
    "\n",
    "print('min:', ratios['lcc_wo_snow_month'].min().round(3).values,\n",
    "      'max:', ratios['lcc_wo_snow_month'].max().round(3).values,  \n",
    "      'std:', ratios['lcc_wo_snow_month'].std(skipna=True).round(3).values, \n",
    "      'mean:', ratios['lcc_wo_snow_month'].mean(skipna=True).round(3).values)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masked and weighted average\n",
    "Make use of the `xarray` function `weighted`, but calculate the weights with the code above from Luke Gloege."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NH_mean = xr.Dataset()\n",
    "SH_mean = xr.Dataset()\n",
    "\n",
    "NH_std = xr.Dataset()\n",
    "SH_std = xr.Dataset()\n",
    "\n",
    "# Grid cells have different area, so when we do the global average, they have to be weigted by the area of each grid cell.\n",
    "weights = fct.area_grid(ratios['lat'].values, ratios['lon'].values)\n",
    "\n",
    "NH_mean['lcc_wo_snow_month'] = ratios['lcc_wo_snow_month'].sel(lat=slice(45,90)).weighted(weights).mean(('lat', 'lon'))\n",
    "SH_mean['lcc_wo_snow_month'] = ratios['lcc_wo_snow_month'].sel(lat=slice(-90,-45)).weighted(weights).mean(('lat', 'lon'))\n",
    "\n",
    "NH_std['lcc_wo_snow_month'] = ratios['lcc_wo_snow_month'].sel(lat=slice(45,90)).weighted(weights).std(('lat', 'lon'))\n",
    "SH_std['lcc_wo_snow_month'] = ratios['lcc_wo_snow_month'].sel(lat=slice(-90,-45)).weighted(weights).std(('lat', 'lon'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for year in np.unique(ds_era['time.year']):\n",
    "#     # print(year)\n",
    "#     f = ds_era_lcc_2t_days['lwp'].sel(time=slice(str(year))).groupby('time.month').count(dim='time',keep_attrs=False)\n",
    "#     n = ds_era_lcc['lwp'].sel(time=slice(str(year))).groupby('time.month').count(dim='time',keep_attrs=False)\n",
    "\n",
    "#     _ratio = f/n\n",
    "    \n",
    "#     NH_mean['lcc_wo_snow_month_'+str(year)] = _ratio.sel(lat=slice(45,90)).weighted(weights).mean(('lat', 'lon'))\n",
    "#     SH_mean['lcc_wo_snow_month_'+str(year)] = _ratio.sel(lat=slice(-90,-45)).weighted(weights).mean(('lat', 'lon'))\n",
    "    \n",
    "#     NH_std['lcc_wo_snow_month_'+str(year)] = _ratio.sel(lat=slice(45,90)).weighted(weights).std(('lat', 'lon'))\n",
    "#     SH_std['lcc_wo_snow_month_'+str(year)] = _ratio.sel(lat=slice(-90,-45)).weighted(weights).std(('lat', 'lon'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annual cycle of frequency of liquid containing clouds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fct.plt_annual_cycle(NH_mean['lcc_wo_snow_month'], SH_mean['lcc_wo_snow_month'], NH_std['lcc_wo_snow_month'], SH_std['lcc_wo_snow_month'], 'Liquid containing cloud frequency','ERA5 ({} - {})'.format(starty,endy))\n",
    "figname = '{}_rf_lcc_wo_snow_annual_{}_{}.png'.format('ERA5',starty, endy)\n",
    "plt.savefig(FIG_DIR + figname, format = 'png', bbox_inches = 'tight', transparent = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency of snowfall from LCCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relative frequency of snowing liquid containing clouds per season\n",
    "f = ds_era_lcc_2t_sf_days['msr'].groupby('time.season').count(dim='time', keep_attrs=False)  # number of times a super cooled liquid water cloud occured when the liquid water path is larger than 5gm-2, and temperature below 0degC\n",
    "n = ds_era_lcc_2t_days['lwp'].groupby('time.season').count(dim='time', keep_attrs=False)\n",
    "\n",
    "ratios['lcc_w_snow_season'] = f/n\n",
    "print('min:', ratios['lcc_w_snow_season'].min().round(3).values,\n",
    "      'max:', ratios['lcc_w_snow_season'].max().round(3).values,  \n",
    "      'std:', ratios['lcc_w_snow_season'].std(skipna=True).round(3).values, \n",
    "      'mean:', ratios['lcc_w_snow_season'].mean(skipna=True).round(3).values)\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fct.plt_seasonal_NH_SH((ratios['lcc_w_snow_season'].where(ratios['lcc_w_snow_season']>0.))*100, np.arange(0,110,10), 'Frequency of snowfall from LCCs (%)', plt_title='ERA5 ({} - {})'.format(starty,endy), extend=None)\n",
    "\n",
    "figname = '{}_rf_lcc_w_snow_season_{}_{}.png'.format('ERA5',starty, endy)\n",
    "plt.savefig(FIG_DIR + figname, format = 'png', bbox_inches = 'tight', transparent = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relative frequency of liquid containing clouds per month\n",
    "f = ds_era_lcc_2t_sf_days['msr'].groupby('time.month').count(dim='time', keep_attrs=False) # number of times a super cooled liquid water cloud occured when the liquid water path is larger than 5gm-2, and temperature below 0degC\n",
    "n = ds_era_lcc_2t_days['lwp'].groupby('time.month').count(dim='time', keep_attrs=False)         # frequency of liquid clouds (only LWP threshold applied) when lwp is larger than 5gm-2\n",
    "\n",
    "ratios['lcc_w_snow_month'] = f/n\n",
    "\n",
    "print('min:', ratios['lcc_w_snow_month'].min().round(3).values,\n",
    "      'max:', ratios['lcc_w_snow_month'].max().round(3).values,  \n",
    "      'std:', ratios['lcc_w_snow_month'].std(skipna=True).round(3).values, \n",
    "      'mean:', ratios['lcc_w_snow_month'].mean(skipna=True).round(3).values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid cells have different area, so when we do the global average, they have to be weigted by the area of each grid cell.\n",
    "weights = fct.area_grid(ratios['lat'].values, ratios['lon'].values)\n",
    "\n",
    "NH_mean['lcc_w_snow_month'] = ratios['lcc_w_snow_month'].sel(lat=slice(45,90)).weighted(weights).mean(('lat', 'lon'))\n",
    "SH_mean['lcc_w_snow_month'] = ratios['lcc_w_snow_month'].sel(lat=slice(-90,-45)).weighted(weights).mean(('lat', 'lon'))\n",
    "\n",
    "NH_std['lcc_w_snow_month'] = ratios['lcc_w_snow_month'].sel(lat=slice(45,90)).weighted(weights).std(('lat', 'lon'))\n",
    "SH_std['lcc_w_snow_month'] = ratios['lcc_w_snow_month'].sel(lat=slice(-90,-45)).weighted(weights).std(('lat', 'lon'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annual cycle of frequency of snowfall from LCCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fct.plt_annual_cycle(NH_mean['lcc_w_snow_month'], SH_mean['lcc_w_snow_month'], NH_std['lcc_w_snow_month'], SH_std['lcc_w_snow_month'], 'Frequency of snowfall from LCCs','ERA5 ({} - {})'.format(starty,endy))\n",
    "# figname = '{}_rf_lcc_wo_snow_annual_{}_{}.png'.format('ERA5',starty, endy)\n",
    "plt.savefig(FIG_DIR + figname, format = 'png', bbox_inches = 'tight', transparent = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relative snowfall efficency\n",
    "\n",
    "The precipitation efficiency of convection is a measure of how much of the water that condenses in a rising column of air reaches the surface as precipitation [3].\n",
    "\n",
    "In our case, we calculate the snowfall efficency by deviding the mean surface snowfall rate (sf, with units kg m-2 h-1) with the column integrated total water path (TWP, with units kg m-2). This leaves us with units of h-1, which means we get a daily mean relative snowfall efficency per hour. \n",
    "\n",
    "$$\\eta = \\frac{sf}{TWP}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# snowfall efficency per season\n",
    "# first averavge over time and then fraction remove weighting only for spatial average\n",
    "\n",
    "ratios['sf_eff_season'] = ds_era_lcc_2t_season['msr']/ds_era_lcc_2t_season['twp']\n",
    "# plt_seasonal_NH_SH(ds_era_lcc_2t_sf_season['msr']/ds_era_lcc_2t_sf_season['twp'], np.arange(0,1, 0.1),'','')\n",
    "print('min:', ratios['sf_eff_season'].min().round(3).values,\n",
    "      'max:', ratios['sf_eff_season'].max().round(3).values,  \n",
    "      'std:', ratios['sf_eff_season'].std(skipna=True).round(3).values, \n",
    "      'mean:', ratios['sf_eff_season'].mean(skipna=True).round(3).values)\n",
    "\n",
    "fct.plt_seasonal_NH_SH(ratios['sf_eff_season'].where(ratios['sf_eff_season']>0.), np.arange(0,1.1,0.1), 'relative snowfall eff. (h$^{-1}$)', 'ERA5 ({} - {})'.format(starty,endy), extend='max')\n",
    "# save precip efficency from mixed-phase clouds figure\n",
    "figname = '{}_sf_twp_season_{}_{}.png'.format('ERA5', starty, endy)\n",
    "plt.savefig(FIG_DIR + figname, format = 'png', bbox_inches = 'tight', transparent = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annual cycle of relative snowfall efficency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shouldn't it be the mean not the sum?\n",
    "f = ds_era_lcc_2t['msr'].groupby('time.month').sum(dim='time', skipna=True, keep_attrs=False)\n",
    "n = ds_era_lcc_2t['twp'].groupby('time.month').sum(dim='time', skipna=True, keep_attrs=False)\n",
    "ratios['sf_eff_month'] = f/n\n",
    "NH_mean['sf_eff_month'] = ratios['sf_eff_month'].sel(lat=slice(45,90)).weighted(weights).mean(('lat', 'lon'))\n",
    "SH_mean['sf_eff_month'] = ratios['sf_eff_month'].sel(lat=slice(-90,-45)).weighted(weights).mean(('lat', 'lon'))\n",
    "\n",
    "NH_std['sf_eff_month'] = ratios['sf_eff_month'].sel(lat=slice(45,90)).weighted(weights).std(('lat', 'lon'))\n",
    "SH_std['sf_eff_month'] = ratios['sf_eff_month'].sel(lat=slice(-90,-45)).weighted(weights).std(('lat', 'lon'))\n",
    "\n",
    "print('min:', ratios['sf_eff_month'].min().round(3).values,\n",
    "      'max:', ratios['sf_eff_month'].max().round(3).values,  \n",
    "      'std:', ratios['sf_eff_month'].std(skipna=True).round(3).values, \n",
    "      'mean:', ratios['sf_eff_month'].mean(skipna=True).round(3).values)\n",
    "\n",
    "# for year in np.unique(ds_era['time.year']):\n",
    "#     # print(year)\n",
    "#     f = ds_era_lcc_2t['msr'].sel(time=slice(str(year))).groupby('time.month').sum(dim='time', skipna=True, keep_attrs=False)\n",
    "#     n = ds_era_lcc_2t['twp'].sel(time=slice(str(year))).groupby('time.month').sum(dim='time', skipna=True, keep_attrs=False)\n",
    "\n",
    "#     _ratio = f/n\n",
    "    \n",
    "#     NH_mean['sf_eff_month_'+str(year)] = _ratio.sel(lat=slice(45,90)).weighted(weights).mean(('lat', 'lon'))\n",
    "#     SH_mean['sf_eff_month_'+str(year)] = _ratio.sel(lat=slice(-90,-45)).weighted(weights).mean(('lat', 'lon'))\n",
    "    \n",
    "#     NH_std['sf_eff_month_'+str(year)] = _ratio.sel(lat=slice(45,90)).weighted(weights).std(('lat', 'lon'))\n",
    "#     SH_std['sf_eff_month_'+str(year)] = _ratio.sel(lat=slice(-90,-45)).weighted(weights).std(('lat', 'lon'))\n",
    "\n",
    "fct.plt_annual_cycle(NH_mean['sf_eff_month'], SH_mean['sf_eff_month'], NH_std['sf_eff_month'], SH_std['sf_eff_month'], 'relative snowfall eff. (h$^{-1}$)', 'ERA5 ({} - {})'.format(starty,endy))\n",
    "figname = '{}_sf_twp_annual_{}_{}.png'.format('ERA5',starty, endy)\n",
    "plt.savefig(FIG_DIR + figname, format = 'png', bbox_inches = 'tight', transparent = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relative precipitation efficency\n",
    "\n",
    "Use of total precipitation, but where LWP $\\ge$ 0.005 gm-2, 2-meter temperature $\\le$ 273.15K.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# supercooled precipitation efficency per season\n",
    "# first averavge over time and then fraction remove weighting only for spatial average\n",
    "\n",
    "ratios['tp_eff_season'] = ds_era_lcc_2t_season['mtpr']/ds_era_lcc_2t_season['twp']\n",
    "# plt_seasonal_NH_SH(ds_era_lcc_2t_tp_season['msr']/ds_era_lcc_2t_tp_season['twp'], np.arange(0,1, 0.1),'','')\n",
    "print('min:', ratios['tp_eff_season'].min().round(3).values,\n",
    "      'max:', ratios['tp_eff_season'].max().round(3).values,  \n",
    "      'std:', ratios['tp_eff_season'].std(skipna=True).round(3).values, \n",
    "      'mean:', ratios['tp_eff_season'].mean(skipna=True).round(3).values)\n",
    "\n",
    "# Plot precipitation efficency\n",
    "\n",
    "fct.plt_seasonal_NH_SH(ratios['tp_eff_season'].where(ratios['tp_eff_season'] >0.), np.arange(0,1.1,0.1), 'relative supercooled precipitation eff. (h$^{-1}$)', 'ERA5 ({} - {})'.format(starty,endy), extend='max')\n",
    "# save precip efficency from mixed-phase clouds figure\n",
    "figname = '{}_tp_twp_season_{}_{}.png'.format('ERA5', starty, endy)\n",
    "plt.savefig(FIG_DIR + figname, format = 'png', bbox_inches = 'tight', transparent = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annual cycle of relative supercooled precipitation efficency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = ds_era_lcc_2t['mtpr'].groupby('time.month').sum(dim='time', skipna=True, keep_attrs=False)\n",
    "n = ds_era_lcc_2t['twp'].groupby('time.month').sum(dim='time', skipna=True, keep_attrs=False)\n",
    "ratios['tp_eff_month'] = f/n\n",
    "NH_mean['tp_eff_month'] = ratios['tp_eff_month'].sel(lat=slice(45,90)).weighted(weights).mean(('lat', 'lon'))\n",
    "SH_mean['tp_eff_month'] = ratios['tp_eff_month'].sel(lat=slice(-90,-45)).weighted(weights).mean(('lat', 'lon'))\n",
    "\n",
    "NH_std['tp_eff_month'] = ratios['tp_eff_month'].sel(lat=slice(45,90)).weighted(weights).std(('lat', 'lon'))\n",
    "SH_std['tp_eff_month'] = ratios['tp_eff_month'].sel(lat=slice(-90,-45)).weighted(weights).std(('lat', 'lon'))\n",
    "\n",
    "print('min:', ratios['tp_eff_month'].min().round(3).values,\n",
    "      'max:', ratios['tp_eff_month'].max().round(3).values,  \n",
    "      'std:', ratios['tp_eff_month'].std(skipna=True).round(3).values, \n",
    "      'mean:', ratios['tp_eff_month'].mean(skipna=True).round(3).values)\n",
    "# for year in np.unique(ds_era['time.year']):\n",
    "#     # print(year)\n",
    "#     f = ds_era_lcc_2t['mtpr'].sel(time=slice(str(year))).groupby('time.month').sum(dim='time', skipna=True, keep_attrs=False)\n",
    "#     n = ds_era_lcc_2t['twp'].sel(time=slice(str(year))).groupby('time.month').sum(dim='time', skipna=True, keep_attrs=False)\n",
    "\n",
    "#     _ratio = f/n\n",
    "    \n",
    "#     NH_mean['tp_eff_month_'+str(year)] = _ratio.sel(lat=slice(45,90)).weighted(weights).mean(('lat', 'lon'))\n",
    "#     SH_mean['tp_eff_month_'+str(year)] = _ratio.sel(lat=slice(-90,-45)).weighted(weights).mean(('lat', 'lon'))\n",
    "    \n",
    "#     NH_std['tp_eff_month_'+str(year)] = _ratio.sel(lat=slice(45,90)).weighted(weights).std(('lat', 'lon'))\n",
    "#     SH_std['tp_eff_month_'+str(year)] = _ratio.sel(lat=slice(-90,-45)).weighted(weights).std(('lat', 'lon'))\n",
    "\n",
    "fct.plt_annual_cycle(NH_mean['tp_eff_month'], SH_mean['tp_eff_month'], NH_std['tp_eff_month'], SH_std['tp_eff_month'], 'relative supercooled precipitation eff. (h$^{-1}$)', 'ERA5 ({} - {})'.format(starty,endy))\n",
    "figname = '{}_tp_twp_annual_{}_{}.png'.format('ERA5',starty, endy)\n",
    "plt.savefig(FIG_DIR + figname, format = 'png', bbox_inches = 'tight', transparent = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency of freezing rain \n",
    "Use of the thresholds. \n",
    "\n",
    "$$RF = \\frac{\\text{total precipitation}(T\\le273.15; LWP\\ge0.005gm^{-2}) - \\text{snowfall}(T\\le273.15; LWP\\ge0.005gm^{-2})}{\\text{total precipitation}(T\\le273.15; LWP\\ge0.005gm^{-2})}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first count the days where we have freezing rain per season and relate to total precipitation days\n",
    "fr_d = ds_era_lcc_2t_days['mfrr'].where(ds_era_lcc_2t_days['mfrr']>0.).groupby('time.season').count('time',keep_attrs=False)\n",
    "tp_d = ds_era_lcc_2t_days['mtpr'].where(ds_era_lcc_2t_days['mtpr']>0.).groupby('time.season').count('time',keep_attrs=False)\n",
    "\n",
    "ratios['mfrr_mtpr_days'] = fr_d/tp_d\n",
    "\n",
    "print('min:', ratios['mfrr_mtpr_days'].min().round(3).values,\n",
    "      'max:', ratios['mfrr_mtpr_days'].max().round(3).values,  \n",
    "      'std:', ratios['mfrr_mtpr_days'].std(skipna=True).round(3).values, \n",
    "      'mean:', ratios['mfrr_mtpr_days'].mean(skipna=True).round(3).values)\n",
    "\n",
    "fct.plt_seasonal_NH_SH(ratios['mfrr_mtpr_days'].where(ratios['mfrr_mtpr_days']>0)*100, np.arange(0,110,10), 'Freezing rain days (%)', 'ERA5 ({} - {})'.format(starty,endy), extend=None)\n",
    "figname = '{}_mfrr_mtpr_days_{}_{}.png'.format('ERA5',starty, endy)\n",
    "plt.savefig(FIG_DIR + figname, format = 'png', bbox_inches = 'tight', transparent = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then bring the amount of freezing rain into relation to the total precipitation (How many percent is freezing rain in comparison to total precipitation)\n",
    "fr = ds_era_lcc_2t_days['mfrr'].where(ds_era_lcc_2t_days['mfrr']>0.)\n",
    "tp = ds_era_lcc_2t_days['mtpr'].where(ds_era_lcc_2t_days['mtpr']>0.)\n",
    "\n",
    "ratios['mfrr_mtpr_frac'] = (fr/tp).groupby('time.season').mean('time',skipna=True,keep_attrs=False)\n",
    "print('min:', ratios['mfrr_mtpr_frac'].min().round(3).values,\n",
    "      'max:', ratios['mfrr_mtpr_frac'].max().round(3).values,  \n",
    "      'std:', ratios['mfrr_mtpr_frac'].std(skipna=True).round(3).values, \n",
    "      'mean:', ratios['mfrr_mtpr_frac'].mean(skipna=True).round(3).values)\n",
    "\n",
    "fct.plt_seasonal_NH_SH(ratios['mfrr_mtpr_frac'].where(ratios['mfrr_mtpr_frac']>0)*100, np.arange(0,110,10), 'Freezing rain fraction (%)', 'ERA5 ({} - {})'.format(starty,endy), extend=None)\n",
    "figname = '{}_mfrr_mtpr_amount_season_{}_{}.png'.format('ERA5',starty, endy)\n",
    "plt.savefig(FIG_DIR + figname, format = 'png', bbox_inches = 'tight', transparent = False)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Way to Average the Globe \n",
    "Code and text taken from Luke Gloege at https://towardsdatascience.com/the-correct-way-to-average-the-globe-92ceecd172b7 \n",
    "\n",
    "\"The average. It’s a simple enough calculation: add up all your data points and divide by the total number points. Does this simple equation work if you are, for instance, calculating the global average temperature? Not quite. This post outlines how to calculate the global average of a gridded dataset with dimensions: `time`, `latitude`, and `longitude`. In other words, calculating the mean across `latitude` and `longitude` at each `time` step. I first explain why it is not appropriate to calculate the spatial mean of a variable using the classic formula, I then show why a weighted-mean is appropriate, and finally give an example using a publicly available dataset.\n",
    "\n",
    "**The traditional mean** \n",
    "The equation below is what probably comes to mind when we think of the mean of a dataset, `X`. This is the traditional mean. In the equation, i represents each `latitude`, `longitude` coordinate pair and `N` is the total number pairs.\n",
    "\n",
    "$mean = \\frac{\\sum_{i=1}^N X_i}{N}$\n",
    "\n",
    "\n",
    "I also call this the “flat Earth mean,” it assumes no curvature to the planet and data at each latitude and longitude location is treated equally. Why is that important? Because a 1º x 1º grid box occupies less area as it moves from the tropics to the pole. What this means is data near the equator has more influence on the mean than the arctic because it occupies more area. This suggests we should weight each data point by the area it occupies.\n",
    "\n",
    "In `xarray` notation, this traditional or “flat Earth” mean can be calculated like this: `traditional_mean = ds['temperature'].mean(['latitude','longitude'])`\n",
    "\n",
    "**The weighted mean**\n",
    "\n",
    "This is the general equation for a weighted mean:\n",
    "\n",
    "$\\text{weighted mean} = \\frac{\\sum_{i=1}^N w_i X_i}{\\sum_{i=1}^N w_i}$\n",
    "\n",
    "This is similar to the traditional mean, except each data point has a weight, `wi`, and the denominator is the sum of all the weights. The traditional mean is just a special case of the weighted mean where each weight is equal to 1.\n",
    "\n",
    "**Calculating the global area**\n",
    "\n",
    "In Earth science, the preferred way is to weight the data by area, where you weight each data point by the area of each grid cell. Before we calculate the area-weighted mean, we first need to know the area of each grid cell. If the grid cell area is not provided the code below will facilitate calculating an area grid.\n",
    "\n",
    "The shape of the Earth is an oblate spheroid, which is like “squashed” sphere. The code below calculates the grid cell area for Earth, with the radius as a function of latitude from [World Geodetic System 1984](https://earth-info.nga.mil/GandG/publications/tr8350.2/tr8350.2-a/Chapter%203.pdf). A derivation of the equation is [here](https://planetcalc.com/7721/).\n",
    "\n",
    "\n",
    "**Area weighted global mean**\n",
    "\n",
    "Now that we know what a weighted mean is and how to calculate an area map, let’s put all of this information together and calculate the global average mean temperature anomaly using the code below.\n",
    "```\n",
    "# area dataArray\n",
    "da_area = area_grid(ds['latitude'], ds['longitude'])\n",
    "\n",
    "# total area\n",
    "total_area = da_area.sum(['latitude','longitude'])\n",
    "\n",
    "# temperature weighted by grid-cell area\n",
    "temp_weighted = (ds['temperature']*da_area) / total_area\n",
    "\n",
    "# area-weighted mean temperature\n",
    "temp_weighted_mean = temp_weighted.sum(['latitude','longitude'])\n",
    "```\n",
    "\n",
    "\n",
    "**Final thoughts**\n",
    "\n",
    "Weighting your data when calculating geospatial means is not just important, it is required in or order to honestly display the average and not overstate a conclusion.\n",
    "\n",
    "Here are a few things to be mindful of when calculating global averages:\n",
    "\n",
    "1. Be mindful of the total area. If you are calculating global average temperature just on land, then you need to mask out the ocean in your area dataset and normalize the weights by the total land area.\n",
    "2. If the dataset is spatially incomplete, sophisticated methods [2] are more appropriate.\n",
    "\n",
    "In summary, area-weighting is the correct and best way to calculate global averages if you have a spatially complete gridded dataset. However, advanced techniques are required if the gridded data contains gaps. Finally, an alternative approach is to cosine weight your data, i.e. weighting your data by the cosine of latitude. The concept is the same, instead of using grid cell area, you would use the `cos('latitude')` as your weight matrix.\n",
    "\n",
    "Hopefully this post provides some helpful guidance when calculating global averages.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating bin and bin sizes\n",
    "https://www.statisticshowto.com/choose-bin-sizes-statistics/\n",
    "\n",
    "Useful to plot TWP vs. Snowfall (precipitation efficency)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Find the smallest and largest data point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Python program to print the data\n",
    "\n",
    "d = {'twp_NH': [(ds_era_lcc_sf_2t['twp'].where(ds_era_lcc_sf_2t['lat']>=45).min().round(4)).values, (ds_era_lcc_sf_2t['twp'].where(ds_era_lcc_sf_2t['lat']>=45).max().round(4)).values, 'NH'],\n",
    "     'sf_NH': [(ds_era_lcc_sf_2t['msr'].where(ds_era_lcc_sf_2t['lat']>=45).min().round(4)).values, (ds_era_lcc_sf_2t['msr'].where(ds_era_lcc_sf_2t['lat']>=45).max().round(4)).values, 'NH'],\n",
    "     'twp_SH': [(ds_era_lcc_sf_2t['twp'].where(ds_era_lcc_sf_2t['lat']<=-45).min().round(4)).values, (ds_era_lcc_sf_2t['twp'].where(ds_era_lcc_sf_2t['lat']<=-45).max().round(4)).values, 'SH'],\n",
    "     'sf_SH': [(ds_era_lcc_sf_2t['msr'].where(ds_era_lcc_sf_2t['lat']>=-45).min().round(4)).values, (ds_era_lcc_sf_2t['msr'].where(ds_era_lcc_sf_2t['lat']<=-45).max().round(4)).values, 'SH'],  \n",
    "}\n",
    "print (\"{:<8} {:<10} {:<10} {:<10}\".format('Var','min','max','Hemisphere'))\n",
    "for k, v in d.items():\n",
    "    lang, perc, change = v\n",
    "    print (\"{:<8} {:<10.4f} {:<10.4f} {:<10}\".format(k, lang, perc, change))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Smallest and/or largest numbers are not whole numbers $\\rightarrow$ Lower the minimum a little and raise the maximum a little\n",
    "\n",
    "|     | Min   |Max    |Diff   |Hemisphere|\n",
    "| ----|:-----:|:-----:|:-----:|---------:|\n",
    "| TWP | 0.006 | 7.1   | 7.1   |NH       |  \n",
    "| sf  | 0.01  | 6.5   | 6.5   |NH       |\n",
    "| ----|-------|-------|-------|---------|\n",
    "| TWP | 0.006 | 4.7   | 4.7   |NH       |  \n",
    "| sf  | 0.01  | 4.3   | 4.3   |NH       |\n",
    "\n",
    "3. Decide how many bins you need\n",
    "\n",
    "|     | Min   |Max    |Diff   |Bins   |Hemisphere|\n",
    "| ----|:-----:|:-----:|:-----:|:-----:|---------:|\n",
    "| TWP | 0.006 | 7.1   | 7.1   |80     |NH       |  \n",
    "| sf  | 0.01  | 6.5   | 6.5   |70     |NH       |\n",
    "| ----|-------|-------|-------|-------|---------|\n",
    "| TWP | 0.006 | 4.7   | 4.7   |50     |NH       |  \n",
    "| sf  | 0.01  | 4.3   | 4.3   |50     |NH       |\n",
    "\n",
    "4. Divide your range (the numbers in your data set) by the bin size chosen in Step 3. E.g. numbers range from 0 to 50 $\\rightarrow$ choose 5 bins $\\rightarrow$ bin size is 50/5=10.\n",
    "\n",
    "|     | Min   |Max    |Diff   |Bins   |Bin size |Hemisphere|\n",
    "| ----|:-----:|:-----:|:-----:|:-----:|:-------:|---------:|\n",
    "| TWP | 0.005 | 7.1   | 7.1   |80     |8/80=0.1 |NH       |  \n",
    "| sf  | 0.01  | 6.5   | 6.5   |70     |7/70=0.1 |NH       |\n",
    "| ----|-------|-------|-------|-------|---------|---------|\n",
    "| TWP | 0.006 | 4.7   | 4.7   |50     |5/50=0.1 |NH       |  \n",
    "| sf  | 0.01  | 105   | 4.3   |50     |5/50=0.1 |NH       |\n",
    "\n",
    "\n",
    "5. Create the bin boundaries, starting with the smallest number (Step 1, 2) and adding the bin size from Step 4. E.g. smallest number is 0, bin size = 10 $\\rightarrow$ boundaries 0, 10,20,...\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Snowfall and Precipitation efficency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_seasonal_2dhist_wp_sf(da_x, da_y, plt_title, xlabel, ylabel):\n",
    "    f, axsm = plt.subplots(nrows=2,ncols=4,figsize =[10,5], sharex=True, sharey=True)\n",
    "    # cmap = cm.batlow\n",
    "    cmap = cm.hawaii_r\n",
    "    # levels = np.arange(0.1,65000,5000)\n",
    "    # norm = BoundaryNorm(levels, ncolors=cmap.N, )\n",
    "    norm = LogNorm(vmin=1, vmax=50000)\n",
    "\n",
    "    ax = axsm.flatten()[0]\n",
    "\n",
    "\n",
    "    for ax, season in zip(axsm.flatten()[:4], da_x['season'].values):\n",
    "        ax.plot([0, 1], [0, 1],ls=\"--\", c=\".6\", transform=ax.transAxes)\n",
    "        x_value = da_x.sel(season=season, lat=slice(45,90)).values.flatten()        \n",
    "        y_value = da_y.sel(season=season, lat=slice(45,90)).values.flatten()   \n",
    "        Z, xedges, yedges = np.histogram2d(x_value[(~np.isnan(x_value)) & (~np.isnan(y_value))],\n",
    "                                        y_value[(~np.isnan(x_value)) & (~np.isnan(y_value))],\n",
    "                                        bins=[80, 80], \n",
    "                                        range=[[0,8],[0, 8]])\n",
    "        \n",
    "        im = ax.pcolormesh(xedges, yedges, Z.transpose(),cmap=cmap,norm=norm,)\n",
    "        # cbar = f.colorbar(im, ax=ax,)\n",
    "        ax.set(title =r'lat$\\geq 45^\\circ$N; season = {}'.format(season))\n",
    "        ax.grid()\n",
    "        \n",
    "        _corr = _corr = xr.corr(da_x.sel(season=season, lat=slice(45,90)), \n",
    "                                da_y.sel(season=season, lat=slice(45,90)))\n",
    "        props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "        ax.text(0.55, 0.95, 'Corr: {}'.format(np.round(_corr,3).values), transform=ax.transAxes, fontsize=11,\n",
    "                verticalalignment='top', bbox=props)\n",
    "        \n",
    "        #find line of best fit\n",
    "        a, b = np.polyfit(x_value[(~np.isnan(x_value)) & (~np.isnan(y_value))], y_value[(~np.isnan(x_value)) & (~np.isnan(y_value))], 1)\n",
    "        #add line of best fit to plot\n",
    "        ax.plot(x_value[(~np.isnan(x_value)) & (~np.isnan(y_value))], a*(x_value[(~np.isnan(x_value)) & (~np.isnan(y_value))])+b, color='steelblue', linestyle='--', linewidth=2)\n",
    "        #add fitted regression equation to plot\n",
    "        ax.text(1, 0.17, 'y = ' + '{:.2f}'.format(b) + ' + {:.2f}'.format(a) + 'x', size=14)\n",
    "\n",
    "        \n",
    "        \n",
    "    for ax, season in zip(axsm.flatten()[4:], da_x['season'].values):\n",
    "        ax.plot([0, 1], [0, 1],ls=\"--\", c=\".6\", transform=ax.transAxes)\n",
    "        x_value = da_x.sel(season=season, lat=slice(-90,-45)).values.flatten()\n",
    "        y_value = da_y.sel(season=season, lat=slice(-90,-45)).values.flatten()\n",
    "        Z, xedges, yedges = np.histogram2d(x_value[(~np.isnan(x_value)) & (~np.isnan(y_value))],\n",
    "                                        y_value[(~np.isnan(x_value)) & (~np.isnan(y_value))],\n",
    "                                        bins=[80, 80], \n",
    "                                        range=[[0,8],[0, 8]])\n",
    "        \n",
    "        im = ax.pcolormesh(xedges, yedges, Z.transpose(),cmap=cmap,norm=norm,)\n",
    "        # cbar = f.colorbar(im, ax=ax, )\n",
    "        ax.set(title =r'lat$\\leq-45^\\circ$S; season = {}'.format(season))\n",
    "            \n",
    "        ax.set_xlabel('{} ({})'.format(xlabel,da_x.attrs['units']))\n",
    "        ax.grid()\n",
    "        \n",
    "        \n",
    "        _corr = xr.corr(da_x.sel(season=season, lat=slice(-90,45)),\n",
    "                        da_y.sel(season=season, lat=slice(-90,45)))\n",
    "        props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "        ax.text(0.55, 0.95, 'Corr: {}'.format(np.round(_corr,3).values), transform=ax.transAxes, fontsize=11,\n",
    "                verticalalignment='top', bbox=props)\n",
    "        \n",
    "        #find line of best fit\n",
    "        a, b = np.polyfit(x_value[(~np.isnan(x_value)) & (~np.isnan(y_value))], y_value[(~np.isnan(x_value)) & (~np.isnan(y_value))], 1)\n",
    "        #add line of best fit to plot\n",
    "        ax.plot(x_value[(~np.isnan(x_value)) & (~np.isnan(y_value))], a*(x_value[(~np.isnan(x_value)) & (~np.isnan(y_value))])+b, color='steelblue', linestyle='--', linewidth=2)\n",
    "        #add fitted regression equation to plot\n",
    "        ax.text(1, 0.17, 'y = ' + '{:.2f}'.format(b) + ' + {:.2f}'.format(a) + 'x', size=14)\n",
    "\n",
    "        \n",
    "    axsm.flatten()[0].set_ylabel('{} ({})'.format(ylabel, da_y.attrs['units']))\n",
    "    axsm.flatten()[4].set_ylabel('{} ({})'.format(ylabel, da_y.attrs['units']))\n",
    "\n",
    "\n",
    "    cbaxes = f.add_axes([1.0125, 0.025, 0.025, 0.9])\n",
    "    cbar = plt.colorbar(im, cax=cbaxes, shrink=0.5, orientation='vertical', label='Frequency')\n",
    "    f.suptitle(plt_title, fontweight=\"bold\");\n",
    "        \n",
    "        \n",
    "        \n",
    "    plt.tight_layout(pad=0.5, w_pad=0.5, h_pad=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# precip efficency from ice\n",
    "da_x = seasonal_lcc_sf_2t['iwp']\n",
    "da_y = seasonal_lcc_sf_2t['msr']\n",
    "xlabel='Ice Water Path'\n",
    "ylabel='Snowfall'\n",
    "plt_title = 'ERA5 ({} - {}) Seasonal ice snowfall efficency'.format(starty,endy)\n",
    "\n",
    "figname = '2dhist_iwp_sf_season_{}_{}.png'.format(starty, endy)\n",
    "plt_seasonal_2dhist_wp_sf(da_x, da_y, plt_title, xlabel, ylabel)\n",
    "plt.savefig(FIG_DIR + figname, format = 'png', bbox_inches = 'tight', transparent = False)\n",
    "\n",
    "######################################\n",
    "# precip efficency from liquid\n",
    "da_x = seasonal_lcc_sf_2t['lwp']\n",
    "da_y = seasonal_lcc_sf_2t['msr']\n",
    "xlabel='Liquid Water Path'\n",
    "ylabel='Snowfall'\n",
    "plt_title = 'ERA5 ({} - {}) Seasonal liquid snowfall efficency'.format(starty,endy)\n",
    "\n",
    "figname = '2dhist_lwp_sf_season_{}_{}.png'.format(starty, endy)\n",
    "plt_seasonal_2dhist_wp_sf(da_x, da_y, plt_title, xlabel, ylabel)\n",
    "plt.savefig(FIG_DIR + figname, format = 'png', bbox_inches = 'tight', transparent = False)\n",
    "\n",
    "######################################\n",
    "# precip efficency from mixed-phase clouds\n",
    "da_x = seasonal_lcc_sf_2t['twp']\n",
    "da_y = seasonal_lcc_sf_2t['msr']\n",
    "xlabel='Total Water Path'\n",
    "ylabel='Snowfall'\n",
    "plt_title = 'ERA5 ({} - {}) Seasonal total snowfall efficency'.format(starty,endy)\n",
    "\n",
    "figname = '2dhist_lwp_iwp_sf_season_{}_{}.png'.format(starty, endy)\n",
    "plt_seasonal_2dhist_wp_sf(da_x, da_y, plt_title, xlabel, ylabel)\n",
    "plt.savefig(FIG_DIR + figname, format = 'png', bbox_inches = 'tight', transparent = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# precip efficency from ice\n",
    "da_x = seasonal_lcc_2t['iwp']\n",
    "da_y = seasonal_lcc_2t['mtpr']\n",
    "xlabel='Ice Water Path'\n",
    "ylabel='Snowfall'\n",
    "plt_title = 'ERA5 ({} - {}) Seasonal ice precipitation efficency'.format(starty,endy)\n",
    "\n",
    "figname = '2dhist_iwp_tp_season_{}_{}.png'.format(starty, endy)\n",
    "plt_seasonal_2dhist_wp_sf(da_x, da_y, plt_title, xlabel, ylabel)\n",
    "plt.savefig(FIG_DIR + figname, format = 'png', bbox_inches = 'tight', transparent = False)\n",
    "\n",
    "######################################\n",
    "# precip efficency from liquid\n",
    "da_x = seasonal_lcc_2t['lwp']\n",
    "da_y = seasonal_lcc_2t['mtpr']\n",
    "xlabel='Liquid Water Path'\n",
    "ylabel='Snowfall'\n",
    "plt_title = 'ERA5 ({} - {}) Seasonal liquid precipitation efficency'.format(starty,endy)\n",
    "\n",
    "figname = '2dhist_lwp_tp_season_{}_{}.png'.format(starty, endy)\n",
    "plt_seasonal_2dhist_wp_sf(da_x, da_y, plt_title, xlabel, ylabel)\n",
    "plt.savefig(FIG_DIR + figname, format = 'png', bbox_inches = 'tight', transparent = False)\n",
    "\n",
    "######################################\n",
    "# precip efficency from mixed-phase clouds\n",
    "da_x = seasonal_lcc_2t['twp']\n",
    "da_y = seasonal_lcc_2t['mtpr']\n",
    "xlabel='Total Water Path'\n",
    "ylabel='Snowfall'\n",
    "plt_title = 'ERA5 ({} - {}) Seasonal total precipitation efficency'.format(starty,endy)\n",
    "\n",
    "figname = '2dhist_lwp_iwp_tp_season_{}_{}.png'.format(starty, endy)\n",
    "plt_seasonal_2dhist_wp_sf(da_x, da_y, plt_title, xlabel, ylabel)\n",
    "plt.savefig(FIG_DIR + figname, format = 'png', bbox_inches = 'tight', transparent = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regrid data\n",
    "SH = ds_era.sel(lat=slice(-90,-45))\n",
    "NH = ds_era.sel(lat=slice(45,90))\n",
    "    \n",
    "era_con = xr.concat([SH, NH], 'lat')\n",
    "\n",
    "filename = 'ERA5_daily_mean_{}_{}01_{}12.nc'.format('ERA5',starty, endy)\n",
    "era_file_out = os.path.join(era_out, 'model_grid/' +filename)\n",
    "\n",
    "try:\n",
    "    os.mkdir(os.path.join(era_out, 'model_grid'))\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "counter = 0\n",
    "files = glob(era_file_out)\n",
    "if era_file_out in files:\n",
    "    print('{} is downloaded'.format(era_file_out))\n",
    "    counter += 1\n",
    "    print('Have regridded in total: {:} files'.format(str(counter))) \n",
    "else: # Save to netcdf file\n",
    "    era_con.to_netcdf(era_file_out)\n",
    "    print('file written: {}'.format(era_file_out))\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mask atmopsheric temperature\n",
    "\n",
    "During the process of finding the SLF50, SLF30, SLF70 we encountered the problem, that the atmospheric temperature is too cold with $T<175K$. To avoid cold temperatures we will mask the atmospheric temperature values. For this we will try a few thresholds and see how it looks on a spatial map, if we are masking too many temperatures then we know that our threshold is too warm.\n",
    "\n",
    "Test for:\n",
    "- 150K = -123.15 $^oC$ \n",
    "- 175K = -98.15 $^oC$ \n",
    "- 200K = -73.15 $^oC$ \n",
    "- 210K = -63.15 $^oC$ \n",
    "- 220K = -53.15 $^oC$\n",
    "- 230K = -43.15 $^oC$ \n",
    "\n",
    "We have different possibilities to mask the temperature:\n",
    "1. Mask t, but all the other vertical variables still keep their values where t<threshold\n",
    "2. Mask t, and all other vertical variables where t<threshold\n",
    "3. Mask t, find the lowest pressure level and t, if t==NaN then take the atmospheric pressure, temperature from the layer above\n",
    "\n",
    "> use Method 1. for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# var_id = 't'\n",
    "# temp = (150, 175, 200, 210, 220, 230)\n",
    "# for t in temp:\n",
    "#     ds_era['{}_{}'.format(var_id, t)] = ds_era[var_id].where(ds_era[var_id] >= t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counter = 0\n",
    "\n",
    "# keys = 'ERA5'\n",
    "\n",
    "# # Regrid data\n",
    "# era_in_1deg = fct.regrid_data(ds_era, ds_out)\n",
    "\n",
    "\n",
    "# filename = '{}_Amon_1deg_{}01_{}12.nc'.format(keys,starty, endy)\n",
    "# era_file_out = os.path.join(era_out, filename)\n",
    "# files = glob(era_file_out)\n",
    "# if era_file_out in files:\n",
    "# #     print('{} is downloaded'.format(era_file_out))\n",
    "# #     counter += 1\n",
    "# #     print('Have regridded in total: {:} files'.format(str(counter))) \n",
    "# # else: # Save to netcdf file\n",
    "#     era_in_1deg.to_netcdf(era_file_out)\n",
    "#     print('file written: {}'.format(era_file_out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for var_id in era_in_1deg.data_vars:\n",
    "#     if var_id.find('occurence')==-1:\n",
    "#         # select where data should be saved\n",
    "#         filename = '{}_{}_Amon_1deg_{}01_{}12.nc'.format(var_id, keys,starty, endy)\n",
    "#         era_file_out = os.path.join(era_out, filename)\n",
    "#         files = glob(era_file_out)\n",
    "#         if era_file_out in files:\n",
    "#         #     print('{} is downloaded'.format(era_file_out))\n",
    "#         #     counter += 1\n",
    "#         #     print('Have regridded in total: {:} files'.format(str(counter))) \n",
    "#         # else: # Save to netcdf file\n",
    "#             era_in_1deg[var_id].to_netcdf(era_file_out)\n",
    "#             print('file written: {}'.format(era_file_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ERA5 data\n",
    "# rsync -av --progress /scratch/franzihe/output/ERA5/monthly_means/1deg/ login.nird.sigma2.no:/projects/NS9600K/data/ERA5/monthly_means/1deg/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Exploratory Data Analysis <a id='exploratory'></a>\n",
    "\n",
    "## Create global mean and seasonal mean/spread of all ERA5 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 0.25 deg resolution data\n",
    "# season_mean_025deg = ds_era.copy()\n",
    "# for var_id in ds_era.data_vars:\n",
    "#     if var_id.find('occurence')==-1:\n",
    "#         season_mean_025deg = fct.seasonal_mean_std(season_mean_025deg, var_id)\n",
    "# season_mean_025deg = season_mean_025deg.drop_dims('time')\n",
    "\n",
    "# # 1 deg resolution data\n",
    "# season_mean_1deg = era_in_1deg.copy()\n",
    "# for var_id in era_in_1deg.data_vars:\n",
    "#     if var_id.find('occurence')==-1:\n",
    "#         season_mean_1deg = fct.seasonal_mean_std(season_mean_1deg, var_id)\n",
    "# season_mean_1deg = season_mean_1deg.drop_dims('time')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find mixed-phase clouds\n",
    "Calculate the IWC/LWC statistics given by the values. Setting the value to 0.5 will find the level in the atmosphere where IWC and LWC are 50/50. Setting it to a value of 0.7 will find the level where IWC is 70% while LWC is 30%.\n",
    "\n",
    "1. find the fraction of IWC to LWC \n",
    "$$SLF = \\frac{LWC}{IWC + LWC}$$ \n",
    "$$SLF: \\text{Super-cooled liquid water fraction}$$\n",
    "\n",
    "2. find the nearest value to given IWC-fraction\n",
    "3. find atmospheric pressure levels where IWC/LWC fraction occurs\n",
    "4. find the index of the first atmospheric pressure level\n",
    "5. use the index to select variables\n",
    "\n",
    "\n",
    "Create dictionary from the list of datasets we want to use for the IWC/LWC statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SLF = {'SLF50':0.5, 'SLF70':0.7, 'SLF30':0.5}\n",
    "\n",
    "\n",
    "# # find IWC/LWC levels\n",
    "# ds_era_025deg = fct.dataset_IWC_LWC_level(ds_era, SLF, 'clwc', 'cisc', keys)\n",
    "\n",
    "# ds_era_1deg = fct.dataset_IWC_LWC_level(era_in_1deg, SLF, 'clwc', 'cisc', keys)\n",
    "\n",
    "# season_mean_025deg = fct.dataset_IWC_LWC_level(season_mean_025deg, SLF, 'clwc_mean', 'cisc_mean', keys)\n",
    "\n",
    "# season_mean_1deg = fct.dataset_IWC_LWC_level(season_mean_1deg, SLF, 'clwc_mean', 'cisc_mean', keys)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the seasonal mean of each variable \n",
    "\n",
    "- for the original dataset\n",
    "- and where the IWC/LWC level were found\n",
    "    - for IWC/LWC 50/50\n",
    "    - for IWC/LWC 70/30\n",
    "    - for IWC/LWC 30/70   \n",
    "\n",
    "All plots are for ERA resolution at 1 deg.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stat = 'ERA5_SLF50'\n",
    "# var_id = 't'\n",
    "# extend = 'both'\n",
    "# title = '{} {} ({} - {})'.format(stat, var_id, starty, endy)\n",
    "\n",
    "# variable = season_mean_025deg[var_id+'_mean'].sel(statistic=stat).sum('level', skipna=True, keep_attrs=True)\n",
    "            \n",
    "# global_mean = ds_era_025deg[var_id].sel(statistic=stat).sum('level', skipna=True, keep_attrs=True).mean(('latitude', 'longitude'),skipna=True, keep_attrs=True).groupby(\"time.season\").mean(\"time\", skipna=True, keep_attrs=True)\n",
    "\n",
    "\n",
    "# fg = variable.plot(\n",
    "#         col=\"season\",\n",
    "#         col_wrap=2,\n",
    "#         transform=ccrs.PlateCarree(),  # remember to provide this!\n",
    "#         subplot_kws={\n",
    "#             \"projection\": ccrs.PlateCarree()\n",
    "#         },\n",
    "#         cbar_kwargs={\"orientation\": \"vertical\", \"shrink\": 0.8, \"aspect\": 40},\n",
    "#         cmap=cm.devon_r, \n",
    "#         figsize=[10, 7],\n",
    "#         robust=True,\n",
    "#         extend=extend,\n",
    "#         # add_colorbar=False,\n",
    "#         # vmin=fct.plt_dict[var_id][fct.plt_dict['header'].index('vmin')],\n",
    "#         # vmax=fct.plt_dict[var_id][fct.plt_dict['header'].index('vmax')],\n",
    "#         # levels=fct.plt_dict[var_id][fct.plt_dict['header'].index('levels')],\n",
    "#     )\n",
    "# for ax, i in zip(fg.axes.flat, variable.season.values):\n",
    "#         ax.set_title('season: {}, global mean: {:.3f}'.format(i, global_mean.sel(season=i).values))\n",
    "    \n",
    "\n",
    "# fg.map(lambda: plt.gca().coastlines())\n",
    "# fg.fig.suptitle(title, fontsize=16, fontweight=\"bold\")\n",
    "# # fg.add_colorbar(fraction=0.05, pad=0.04)\n",
    "\n",
    "# # fg.cbar.set_label(label='{}'.format(fct.plt_dict[var_id][fct.plt_dict['header'].index('label')], weight='bold'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for t in temp:\n",
    "#     var_id = 't_{}'.format(t)\n",
    "#     title = '{} {} ({} - {})'.format(stat, var_id, starty, endy)\n",
    "\n",
    "#     variable = season_mean_025deg[var_id+'_mean'].sel(statistic=stat).sum('level', skipna=True, keep_attrs=True)\n",
    "                \n",
    "#     global_mean = ds_era_025deg[var_id].sel(statistic=stat).sum('level', skipna=True, keep_attrs=True).mean(('latitude', 'longitude'),skipna=True, keep_attrs=True).groupby(\"time.season\").mean(\"time\", skipna=True, keep_attrs=True)\n",
    "\n",
    "\n",
    "#     fg = variable.plot(\n",
    "#             col=\"season\",\n",
    "#             col_wrap=2,\n",
    "#             transform=ccrs.PlateCarree(),  # remember to provide this!\n",
    "#             subplot_kws={\n",
    "#                 \"projection\": ccrs.PlateCarree()\n",
    "#             },\n",
    "#             cbar_kwargs={\"orientation\": \"vertical\", \"shrink\": 0.8, \"aspect\": 40},\n",
    "#             cmap=cm.devon_r, \n",
    "#             figsize=[10, 7],\n",
    "#             robust=True,\n",
    "#             extend=extend,\n",
    "#             # add_colorbar=False,\n",
    "#             # vmin=fct.plt_dict['t'][fct.plt_dict['header'].index('vmin')],\n",
    "#             # vmax=fct.plt_dict['t'][fct.plt_dict['header'].index('vmax')],\n",
    "#             # levels=fct.plt_dict['t'][fct.plt_dict['header'].index('levels')],\n",
    "#         )\n",
    "\n",
    "#     for ax, i in zip(fg.axes.flat, variable.season.values):\n",
    "#         ax.set_title('season: {}, global mean: {:.3f}'.format(i, global_mean.sel(season=i).values))\n",
    "    \n",
    "\n",
    "#     fg.map(lambda: plt.gca().coastlines())\n",
    "#     fg.fig.suptitle(title, fontsize=16, fontweight=\"bold\")\n",
    "#     # fg.add_colorbar(fraction=0.05, pad=0.04)\n",
    "\n",
    "#     # fg.cbar.set_label(label='{}'.format(fct.plt_dict['t'][fct.plt_dict['header'].index('label')], weight='bold'))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# var_id = 't'\n",
    "# for t in temp:\n",
    "#     title = '{} {}_{}-{} ({} - {})'.format(stat, var_id, t, var_id, starty, endy)\n",
    "#     _diff = season_mean_025deg['{}_{}_mean'.format(var_id,t)].sel(statistic=stat).sum('level', skipna=True, keep_attrs=True) - season_mean_025deg[var_id+'_mean'].sel(statistic=stat).sum('level', skipna=True, keep_attrs=True)\n",
    "    \n",
    "#     fg = _diff.plot(col=\"season\",\n",
    "#                 col_wrap=2,\n",
    "#                 transform=ccrs.PlateCarree(),  # remember to provide this!\n",
    "#                 subplot_kws={\n",
    "#                     \"projection\": ccrs.PlateCarree()\n",
    "#                 },\n",
    "#                 cbar_kwargs={\"orientation\": \"vertical\", \"shrink\": 0.8, \"aspect\": 40},\n",
    "#                 cmap=cm.bam, \n",
    "#                 figsize=[10, 7],\n",
    "#                 robust=True,\n",
    "#                 extend=extend,\n",
    "#                 vmin=-0.4,\n",
    "#                 vmax=0.4,\n",
    "#                 levels=20,\n",
    "#                 )\n",
    "\n",
    "#     fg.map(lambda: plt.gca().coastlines())\n",
    "#     fg.fig.suptitle(title, fontsize=16, fontweight=\"bold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for stat in season_mean_1deg.statistic.values:\n",
    "#     for var_id in ds_era.data_vars:\n",
    "#         if var_id == '2t' or var_id=='t' or var_id=='pressure':\n",
    "#             extend='both'\n",
    "#         else:\n",
    "#             extend='max'\n",
    "#         if stat == 'ERA5' and var_id.find('occurence')==-1 and (len(ds_era[var_id].dims))==3:\n",
    "#             # print(stat, var_id)\n",
    "            \n",
    "#             variable = season_mean_1deg[var_id+'_mean'].sel(statistic=stat, level=1000)#.sum('level', skipna=True, keep_attrs=True)\n",
    "            \n",
    "#             global_mean = ds_era_1deg[var_id].sel(statistic=stat, level=1000).mean(('lat', 'lon'),skipna=True, keep_attrs=True).groupby(\"time.season\").mean(\"time\", skipna=True, keep_attrs=True)\n",
    "            \n",
    "\n",
    "\n",
    "#             title = '{} MEAN ({} - {})'.format(stat, starty, endy)\n",
    "#             fct.plt_spatial_seasonal_mean(variable, global_mean, title, var_id,extend)\n",
    "            \n",
    "#             # save seasonal mean + std figure to png\n",
    "#             figname = '{}_{}_season_mean_1deg_{}_{}.png'.format(var_id,stat, starty, endy)\n",
    "#             plt.savefig(FIG_DIR + figname, format = 'png', bbox_inches = 'tight', transparent = False)\n",
    "\n",
    "\n",
    "#         elif stat!= 'ERA5' and var_id.find('occurence')==-1:\n",
    "#             # print(stat, var_id)\n",
    "#             variable = season_mean_1deg[var_id+'_mean'].sel(statistic=stat).sum('level', skipna=True, keep_attrs=True)\n",
    "            \n",
    "#             global_mean = ds_era_1deg[var_id].sel(statistic=stat).sum('level', skipna=True, keep_attrs=True).mean(('lat', 'lon'),skipna=True, keep_attrs=True).groupby(\"time.season\").mean(\"time\", skipna=True, keep_attrs=True)\n",
    "            \n",
    "\n",
    "\n",
    "#             title = '{} MEAN ({} - {})'.format(stat, starty, endy)\n",
    "#             fct.plt_spatial_seasonal_mean(variable, global_mean, title, var_id, extend)\n",
    "\n",
    "#             # save seasonal mean + std figure to png\n",
    "#             figname = '{}_{}_season_mean_1deg_{}_{}.png'.format(var_id,stat, starty, endy)\n",
    "#             plt.savefig(FIG_DIR + figname, format = 'png', bbox_inches = 'tight', transparent = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lat_SH = (-90, -40); lat_NH = (40,90); step = 5\n",
    "# iteration_SH = range(lat_SH[1], lat_SH[0], -step)\n",
    "# iteration_NH = range(lat_NH[0], lat_NH[1], step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# markers = ['o', 'v', 's']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for _lat in iteration_SH:\n",
    "#     print(_lat-step, _lat, ds_era_1deg.sel(statistic=stat, lat=slice(_lat-step, _lat)).lat.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_dict = {}\n",
    "# for _lat in iteration_NH:\n",
    "#     # print(_lat, _lat+step, ds_era_025deg.sel(statistic=stat, latitude=slice(_lat+step, _lat)).latitude.values)\n",
    "#     ds_dict['{}_{}'.format(_lat, _lat+step)] = ds_era_025deg.sel(latitude = slice(_lat+step, _lat), ).mean(('longitude', 'latitude', 'level'), skipna=True, keep_attrs=True).groupby('time.season').mean('time', skipna=True, keep_attrs=True)\n",
    "\n",
    "# for _lat in iteration_SH:\n",
    "#     ds_dict['{}_{}'.format(_lat, _lat-step)] = ds_era_025deg.sel(latitude = slice(_lat, _lat-step), ).mean(('longitude', 'latitude', 'level'), skipna=True, keep_attrs=True).groupby('time.season').mean('time', skipna=True, keep_attrs=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _ds = list(ds_dict.values())\n",
    "# _coord = list(ds_dict.keys())\n",
    "# ds_lat = xr.concat(objs=_ds, dim=_coord, coords=\"all\").rename({'concat_dim':'lat'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_lat.sel(statistic='ERA5_SLF50',season='DJF').plot.scatter(x='pressure', y='t',hue='lat',hue_style='discrete', add_guide=True, cmap=cm.hawaii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# season ='DJF'\n",
    "# ds_lat.sel(statistic='ERA5_SLF50').plot.scatter(col=\"season\",col_wrap=2,\n",
    "#                                                 x='pressure', y='t', \n",
    "#                                                 #size=[10, 7],\n",
    "#                                                 cmap=cm.hawaii,)\n",
    "# # ,lat='{}_{}'.format(_lat, _lat-step) label='[{} : {})'.format(_lat-step, _lat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axsm = plt.subplots(2,2, figsize=[10,7],sharex=True, sharey=True)\n",
    "# axs = axsm.flatten()\n",
    "# for ax, season in zip(axs, season_mean_1deg.season.values):\n",
    "\n",
    "#     for _lat, c in zip(iteration_SH, cm.hawaii(range(0, 256, int(256/len(iteration_SH))))):\n",
    "\n",
    "#         _season=ds_era_1deg.sum('level', skipna=True, keep_attrs=True).sel(lat = slice(_lat-step, _lat)).mean(('lon', 'lat'), skipna=True, keep_attrs=True).groupby('time.season').mean('time', skipna=True, keep_attrs=True)\n",
    "#         _month=ds_era_1deg.sum('level', skipna=True, keep_attrs=True).sel(lat = slice(_lat-step, _lat)).mean(('lon', 'lat'), skipna=True, keep_attrs=True).groupby('time.month').mean('time', skipna=True, keep_attrs=True)\n",
    "       \n",
    "#         _season.sel(statistic=stat, season=season).plot.scatter(ax=ax, x='pressure', y='t', label='[{} : {})'.format(_lat-step, _lat), color=c, marker='p')\n",
    "#         # if season =='DJF':\n",
    "#         #     for month, marker in zip(_month.sel(statistic=stat).month.values[11:], markers):\n",
    "#         #         _month.sel(statistic=stat, month=month).plot.scatter(ax=ax, \n",
    "#         #                                                                   x='pressure', y='t',\n",
    "#         #                                                                   label='[{} : {})'.format(_lat-step, _lat),\n",
    "#         #                                                                   color=c, marker=marker)\n",
    "#         #     for month, marker in zip(_month.sel(statistic=stat).month.values[:2], markers[1:]):\n",
    "#         #         _month.sel(statistic=stat, month=month).plot.scatter(ax=ax, \n",
    "#         #                                                                  x='pressure', y='t',\n",
    "#         #                                                                  label='[{} : {})'.format(_lat-step, _lat),\n",
    "#         #                                                                  color=c, marker=marker)\n",
    "        \n",
    "#         # if season=='MAM':\n",
    "#         #     for month, marker in zip(_month.sel(statistic=stat).month.values[2:5], markers):\n",
    "#         #         _month.sel(statistic=stat, month=month).plot.scatter(ax=ax, \n",
    "#         #                                                                  x='pressure', y='t',\n",
    "#         #                                                                  label='[{} : {})'.format(_lat-step, _lat),\n",
    "#         #                                                                  color=c, marker=marker)\n",
    "        \n",
    "#         # if season=='JJA':\n",
    "#         #     for month, marker in zip(_month.sel(statistic=stat).month.values[5:8], markers):\n",
    "#         #         _month.sel(statistic=stat, month=month).plot.scatter(ax=ax, \n",
    "#         #                                                                   x='pressure', y='t',\n",
    "#         #                                                                   label='[{} : {})'.format(_lat-step, _lat),\n",
    "#         #                                                                   color=c, marker=marker)\n",
    "\n",
    "#         # if season=='SON':\n",
    "#         #     for month, marker in zip(_month.sel(statistic=stat).month.values[8:11], markers):\n",
    "#         #         _month.sel(statistic=stat, month=month).plot.scatter(ax=ax, \n",
    "#         #                                                                  x='pressure', y='t',\n",
    "#         #                                                                  label='[{} : {})'.format(_lat-step, _lat),\n",
    "#         #                                                                  color=c, marker=marker)\n",
    "\n",
    "\n",
    "# axs[1].legend(loc='upper left', bbox_to_anchor=(1, 1), fontsize=\"small\", fancybox=True,)\n",
    "# plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axsm = plt.subplots(2,2, figsize=[10,7],sharex=True, sharey=True)\n",
    "# axs = axsm.flatten()\n",
    "# for ax, season in zip(axs, season_mean_1deg.season.values):\n",
    "\n",
    "#     for _lat, c in zip(iteration_SH, cm.hawaii(range(0, 256, int(256/len(iteration_SH))))):\n",
    "\n",
    "#         _season=ds_era_1deg.sum('level').sel(lat = slice(_lat-step, _lat)).mean(('lon', 'lat'), skipna=True, keep_attrs=True).groupby('time.season').mean('time', skipna=True, keep_attrs=True)\n",
    "#         _month=ds_era_1deg.sum('level').sel(lat = slice(_lat-step, _lat)).mean(('lon', 'lat'), skipna=True, keep_attrs=True).groupby('time.month').mean('time', skipna=True, keep_attrs=True)\n",
    "       \n",
    "#         _season.sel(statistic=stat, season=season).plot.scatter(ax=ax, x='pressure', y='t_220', label='[{} : {})'.format(_lat-step, _lat), color=c, marker='p')\n",
    "#         # if season =='DJF':\n",
    "#         #     for month, marker in zip(_month.sel(statistic=stat).month.values[11:], markers):\n",
    "#         #         _month.sel(statistic=stat, month=month).plot.scatter(ax=ax, \n",
    "#         #                                                                   x='pressure', y='t',\n",
    "#         #                                                                   label='[{} : {})'.format(_lat-step, _lat),\n",
    "#         #                                                                   color=c, marker=marker)\n",
    "#         #     for month, marker in zip(_month.sel(statistic=stat).month.values[:2], markers[1:]):\n",
    "#         #         _month.sel(statistic=stat, month=month).plot.scatter(ax=ax, \n",
    "#         #                                                                  x='pressure', y='t',\n",
    "#         #                                                                  label='[{} : {})'.format(_lat-step, _lat),\n",
    "#         #                                                                  color=c, marker=marker)\n",
    "        \n",
    "#         # if season=='MAM':\n",
    "#         #     for month, marker in zip(_month.sel(statistic=stat).month.values[2:5], markers):\n",
    "#         #         _month.sel(statistic=stat, month=month).plot.scatter(ax=ax, \n",
    "#         #                                                                  x='pressure', y='t',\n",
    "#         #                                                                  label='[{} : {})'.format(_lat-step, _lat),\n",
    "#         #                                                                  color=c, marker=marker)\n",
    "        \n",
    "#         # if season=='JJA':\n",
    "#         #     for month, marker in zip(_month.sel(statistic=stat).month.values[5:8], markers):\n",
    "#         #         _month.sel(statistic=stat, month=month).plot.scatter(ax=ax, \n",
    "#         #                                                                   x='pressure', y='t',\n",
    "#         #                                                                   label='[{} : {})'.format(_lat-step, _lat),\n",
    "#         #                                                                   color=c, marker=marker)\n",
    "\n",
    "#         # if season=='SON':\n",
    "#         #     for month, marker in zip(_month.sel(statistic=stat).month.values[8:11], markers):\n",
    "#         #         _month.sel(statistic=stat, month=month).plot.scatter(ax=ax, \n",
    "#         #                                                                  x='pressure', y='t',\n",
    "#         #                                                                  label='[{} : {})'.format(_lat-step, _lat),\n",
    "#         #                                                                  color=c, marker=marker)\n",
    "\n",
    "\n",
    "# axs[1].legend(loc='upper left', bbox_to_anchor=(1, 1), fontsize=\"small\", fancybox=True,)\n",
    "# plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for _lat in iteration_NH:\n",
    "#     print(_lat, _lat+step, ds_era_1deg.sel(statistic='ERA5_50', lat=slice(_lat, _lat+step)).lat.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axsm = plt.subplots(2,2, figsize=[10,7],sharex=True, sharey=True)\n",
    "# axs = axsm.flatten()\n",
    "# for ax, season in zip(axs, season_mean_1deg.season.values):\n",
    "\n",
    "#     for _lat, c in zip(iteration_NH, cm.hawaii(range(0, 256, int(256/len(iteration_NH))))):\n",
    "\n",
    "#         _season=ds_era_1deg.sum('level').sel(lat = slice(_lat, _lat+step)).mean(('lon', 'lat'), skipna=True, keep_attrs=True).groupby('time.season').mean('time', skipna=True, keep_attrs=True)\n",
    "#         _month=ds_era_1deg.sum('level').sel(lat = slice(_lat, _lat+step)).mean(('lon', 'lat'), skipna=True, keep_attrs=True).groupby('time.month').mean('time', skipna=True, keep_attrs=True)\n",
    "       \n",
    "#         _season.sel(statistic='ERA5_50', season=season).plot.scatter(ax=ax, x='t', y='clic', label='[{} : {})'.format(_lat, _lat+step), color=c, marker='p')\n",
    "#         if season =='DJF':\n",
    "#             for month, marker in zip(_month.sel(statistic='ERA5_50').month.values[11:], markers):\n",
    "#                 _month.sel(statistic='ERA5_50', month=month).plot.scatter(ax=ax, \n",
    "#                                                                           x='t', y='clic',\n",
    "#                                                                           label='[{} : {})'.format(_lat, _lat+step),\n",
    "#                                                                           color=c, marker=marker)\n",
    "#             for month, marker in zip(_month.sel(statistic='ERA5_50').month.values[:2], markers[1:]):\n",
    "#                 _month.sel(statistic='ERA5_50', month=month).plot.scatter(ax=ax, \n",
    "#                                                                          x='t', y='clic',\n",
    "#                                                                          label='[{} : {})'.format(_lat, _lat+step),\n",
    "#                                                                          color=c, marker=marker)\n",
    "        \n",
    "#         if season=='MAM':\n",
    "#             for month, marker in zip(_month.sel(statistic='ERA5_50').month.values[2:5], markers):\n",
    "#                 _month.sel(statistic='ERA5_50', month=month).plot.scatter(ax=ax, \n",
    "#                                                                          x='t', y='clic',\n",
    "#                                                                          label='[{} : {})'.format(_lat, _lat+step),\n",
    "#                                                                          color=c, marker=marker)\n",
    "        \n",
    "#         if season=='JJA':\n",
    "#             for month, marker in zip(_month.sel(statistic='ERA5_50').month.values[5:8], markers):\n",
    "#                 _month.sel(statistic='ERA5_50', month=month).plot.scatter(ax=ax, \n",
    "#                                                                           x='t', y='clic',\n",
    "#                                                                           label='[{} : {})'.format(_lat, _lat+step),\n",
    "#                                                                           color=c, marker=marker)\n",
    "\n",
    "#         if season=='SON':\n",
    "#             for month, marker in zip(_month.sel(statistic='ERA5_50').month.values[8:11], markers):\n",
    "#                 _month.sel(statistic='ERA5_50', month=month).plot.scatter(ax=ax, \n",
    "#                                                                          x='t', y='clic',\n",
    "#                                                                          label='[{} : {})'.format(_lat, _lat+step),\n",
    "#                                                                          color=c, marker=marker)\n",
    "\n",
    "\n",
    "# axs[1].legend(loc='upper left', bbox_to_anchor=(1, 1), fontsize=\"small\", fancybox=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axsm = plt.subplots(2,2, figsize=[10,7],sharex=True, sharey=True)\n",
    "# axs = axsm.flatten()\n",
    "# for ax, season in zip(axs, season_mean_1deg.season.values):\n",
    "\n",
    "#     for _lat, c in zip(iteration_NH, cm.hawaii(range(0, 256, int(256/len(iteration_NH))))):\n",
    "\n",
    "#         _season=ds_era_1deg.sum('level').sel(lat = slice(_lat, _lat+step)).mean(('lon', 'lat'), skipna=True, keep_attrs=True).groupby('time.season').mean('time', skipna=True, keep_attrs=True)\n",
    "#         _month=ds_era_1deg.sum('level').sel(lat = slice(_lat, _lat+step)).mean(('lon', 'lat'), skipna=True, keep_attrs=True).groupby('time.month').mean('time', skipna=True, keep_attrs=True)\n",
    "       \n",
    "#         _season.sel(statistic='ERA5_50', season=season).plot.scatter(ax=ax, x='t', y='sf', label='[{} : {})'.format(_lat, _lat+step), color=c, marker='p')\n",
    "#         if season =='DJF':\n",
    "#             for month, marker in zip(_month.sel(statistic='ERA5_50').month.values[11:], markers):\n",
    "#                 _month.sel(statistic='ERA5_50', month=month).plot.scatter(ax=ax, \n",
    "#                                                                           x='t', y='sf',\n",
    "#                                                                           label='[{} : {})'.format(_lat, _lat+step),\n",
    "#                                                                           color=c, marker=marker)\n",
    "#             for month, marker in zip(_month.sel(statistic='ERA5_50').month.values[:2], markers[1:]):\n",
    "#                 _month.sel(statistic='ERA5_50', month=month).plot.scatter(ax=ax, \n",
    "#                                                                          x='t', y='sf',\n",
    "#                                                                          label='[{} : {})'.format(_lat, _lat+step),\n",
    "#                                                                          color=c, marker=marker)\n",
    "        \n",
    "#         if season=='MAM':\n",
    "#             for month, marker in zip(_month.sel(statistic='ERA5_50').month.values[2:5], markers):\n",
    "#                 _month.sel(statistic='ERA5_50', month=month).plot.scatter(ax=ax, \n",
    "#                                                                          x='t', y='sf',\n",
    "#                                                                          label='[{} : {})'.format(_lat, _lat+step),\n",
    "#                                                                          color=c, marker=marker)\n",
    "        \n",
    "#         if season=='JJA':\n",
    "#             for month, marker in zip(_month.sel(statistic='ERA5_50').month.values[5:8], markers):\n",
    "#                 _month.sel(statistic='ERA5_50', month=month).plot.scatter(ax=ax, \n",
    "#                                                                           x='t', y='sf',\n",
    "#                                                                           label='[{} : {})'.format(_lat, _lat+step),\n",
    "#                                                                           color=c, marker=marker)\n",
    "\n",
    "#         if season=='SON':\n",
    "#             for month, marker in zip(_month.sel(statistic='ERA5_50').month.values[8:11], markers):\n",
    "#                 _month.sel(statistic='ERA5_50', month=month).plot.scatter(ax=ax, \n",
    "#                                                                          x='t', y='sf',\n",
    "#                                                                          label='[{} : {})'.format(_lat, _lat+step),\n",
    "#                                                                          color=c, marker=marker)\n",
    "\n",
    "\n",
    "# axs[1].legend(loc='upper left', bbox_to_anchor=(1, 1), fontsize=\"small\", fancybox=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axsm = plt.subplots(2,2, figsize=[10,7],sharex=True, sharey=True)\n",
    "# axs = axsm.flatten()\n",
    "# for ax, season in zip(axs, season_mean_1deg.season.values):\n",
    "\n",
    "#     for _lat, c in zip(iteration_NH, cm.hawaii(range(0, 256, int(256/len(iteration_NH))))):\n",
    "\n",
    "#         _season=ds_era_1deg.sum('level').sel(lat = slice(_lat, _lat+step)).mean(('lon', 'lat'), skipna=True, keep_attrs=True).groupby('time.season').mean('time', skipna=True, keep_attrs=True)\n",
    "#         _month=ds_era_1deg.sum('level').sel(lat = slice(_lat, _lat+step)).mean(('lon', 'lat'), skipna=True, keep_attrs=True).groupby('time.month').mean('time', skipna=True, keep_attrs=True)\n",
    "       \n",
    "#         _season.sel(statistic='ERA5_50', season=season).plot.scatter(ax=ax, x='pressure', y='sf', label='[{} : {})'.format(_lat, _lat+step), color=c, marker='p')\n",
    "#         if season =='DJF':\n",
    "#             for month, marker in zip(_month.sel(statistic='ERA5_50').month.values[11:], markers):\n",
    "#                 _month.sel(statistic='ERA5_50', month=month).plot.scatter(ax=ax, \n",
    "#                                                                           x='pressure', y='sf',\n",
    "#                                                                           label='[{} : {})'.format(_lat, _lat+step),\n",
    "#                                                                           color=c, marker=marker)\n",
    "#             for month, marker in zip(_month.sel(statistic='ERA5_50').month.values[:2], markers[1:]):\n",
    "#                 _month.sel(statistic='ERA5_50', month=month).plot.scatter(ax=ax, \n",
    "#                                                                          x='pressure', y='sf',\n",
    "#                                                                          label='[{} : {})'.format(_lat, _lat+step),\n",
    "#                                                                          color=c, marker=marker)\n",
    "        \n",
    "#         if season=='MAM':\n",
    "#             for month, marker in zip(_month.sel(statistic='ERA5_50').month.values[2:5], markers):\n",
    "#                 _month.sel(statistic='ERA5_50', month=month).plot.scatter(ax=ax, \n",
    "#                                                                          x='pressure', y='sf',\n",
    "#                                                                          label='[{} : {})'.format(_lat, _lat+step),\n",
    "#                                                                          color=c, marker=marker)\n",
    "        \n",
    "#         if season=='JJA':\n",
    "#             for month, marker in zip(_month.sel(statistic='ERA5_50').month.values[5:8], markers):\n",
    "#                 _month.sel(statistic='ERA5_50', month=month).plot.scatter(ax=ax, \n",
    "#                                                                           x='pressure', y='sf',\n",
    "#                                                                           label='[{} : {})'.format(_lat, _lat+step),\n",
    "#                                                                           color=c, marker=marker)\n",
    "\n",
    "#         if season=='SON':\n",
    "#             for month, marker in zip(_month.sel(statistic='ERA5_50').month.values[8:11], markers):\n",
    "#                 _month.sel(statistic='ERA5_50', month=month).plot.scatter(ax=ax, \n",
    "#                                                                          x='pressure', y='sf',\n",
    "#                                                                          label='[{} : {})'.format(_lat, _lat+step),\n",
    "#                                                                          color=c, marker=marker)\n",
    "\n",
    "\n",
    "# axs[1].legend(loc='upper left', bbox_to_anchor=(1, 1), fontsize=\"small\", fancybox=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axsm = plt.subplots(2,2, figsize=[10,7],sharex=True, sharey=True)\n",
    "# axs = axsm.flatten()\n",
    "# for ax, season in zip(axs, season_mean_1deg.season.values):\n",
    "\n",
    "#     for _lat, c in zip(iteration_NH, cm.hawaii(range(0, 256, int(256/len(iteration_NH))))):\n",
    "\n",
    "#         _season=ds_era_1deg.sum('level').sel(lat = slice(_lat, _lat+step)).mean(('lon', 'lat'), skipna=True, keep_attrs=True).groupby('time.season').mean('time', skipna=True, keep_attrs=True)\n",
    "#         _month=ds_era_1deg.sum('level').sel(lat = slice(_lat, _lat+step)).mean(('lon', 'lat'), skipna=True, keep_attrs=True).groupby('time.month').mean('time', skipna=True, keep_attrs=True)\n",
    "       \n",
    "#         _season.sel(statistic='ERA5_50', season=season).plot.scatter(ax=ax, x='pressure', y='t', label='[{} : {})'.format(_lat, _lat+step), color=c, marker='p')\n",
    "#         if season =='DJF':\n",
    "#             for month, marker in zip(_month.sel(statistic='ERA5_50').month.values[11:], markers):\n",
    "#                 _month.sel(statistic='ERA5_50', month=month).plot.scatter(ax=ax, \n",
    "#                                                                           x='pressure', y='t',\n",
    "#                                                                           label='[{} : {})'.format(_lat, _lat+step),\n",
    "#                                                                           color=c, marker=marker)\n",
    "#             for month, marker in zip(_month.sel(statistic='ERA5_50').month.values[:2], markers[1:]):\n",
    "#                 _month.sel(statistic='ERA5_50', month=month).plot.scatter(ax=ax, \n",
    "#                                                                          x='pressure', y='t',\n",
    "#                                                                          label='[{} : {})'.format(_lat, _lat+step),\n",
    "#                                                                          color=c, marker=marker)\n",
    "        \n",
    "#         if season=='MAM':\n",
    "#             for month, marker in zip(_month.sel(statistic='ERA5_50').month.values[2:5], markers):\n",
    "#                 _month.sel(statistic='ERA5_50', month=month).plot.scatter(ax=ax, \n",
    "#                                                                          x='pressure', y='t',\n",
    "#                                                                          label='[{} : {})'.format(_lat, _lat+step),\n",
    "#                                                                          color=c, marker=marker)\n",
    "        \n",
    "#         if season=='JJA':\n",
    "#             for month, marker in zip(_month.sel(statistic='ERA5_50').month.values[5:8], markers):\n",
    "#                 _month.sel(statistic='ERA5_50', month=month).plot.scatter(ax=ax, \n",
    "#                                                                           x='pressure', y='t',\n",
    "#                                                                           label='[{} : {})'.format(_lat, _lat+step),\n",
    "#                                                                           color=c, marker=marker)\n",
    "\n",
    "#         if season=='SON':\n",
    "#             for month, marker in zip(_month.sel(statistic='ERA5_50').month.values[8:11], markers):\n",
    "#                 _month.sel(statistic='ERA5_50', month=month).plot.scatter(ax=ax, \n",
    "#                                                                          x='pressure', y='t',\n",
    "#                                                                          label='[{} : {})'.format(_lat, _lat+step),\n",
    "#                                                                          color=c, marker=marker)\n",
    "\n",
    "\n",
    "# axs[1].legend(loc='upper left', bbox_to_anchor=(1, 1), fontsize=\"small\", fancybox=True,)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate latitude band mean of variables\n",
    "We will only use high latitudes and the extratropics the latitude bands are as follow:\n",
    "\n",
    "1. Southern Hemispher:\n",
    "    - [-30, -45)\n",
    "    - [-45, -60)\n",
    "    - [-60, -75)\n",
    "    - [-75, -90)\n",
    "2. Northern Hemisphere:\n",
    "    - [30, 45)\n",
    "    - [45, 60)\n",
    "    - [60, 75)\n",
    "    - [75, 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lat_SH = (-90, -30); lat_NH = (30,90); step = 15\n",
    "# iteration_SH = range(lat_SH[1], lat_SH[0], -step)\n",
    "# iteration_NH = range(lat_NH[0], lat_NH[1], step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for var_id in ds_era.keys():\n",
    "#     for _lat in iteration_SH:\n",
    "\n",
    "#         # ERA5 original resolution\n",
    "#         ds_era_025deg[var_id + '_season_{}_{}'.format(_lat, _lat-step)] = ds_era_025deg[var_id + '_season_mean'].sel(lat = slice(_lat, _lat-step)).mean(('lat',), keep_attrs=True, skipna=True)\n",
    "\n",
    "#         # ERA5 regridded resolution\n",
    "#         ds_era_1deg[var_id + '_season_{}_{}'.format(_lat, _lat-step)] = ds_era_1deg[var_id + '_season_mean'].sel(lat = slice(_lat-step, _lat)).mean(('lat',), keep_attrs=True, skipna=True)\n",
    "\n",
    "#     for _lat in iteration_NH:\n",
    "#         # ERA5 original resolution\n",
    "#         ds_era_025deg[var_id + '_season_{}_{}'.format(_lat, _lat+step)] = ds_era_025deg[var_id + '_season_mean'].sel(lat = slice(_lat+step, _lat)).mean(('lat',), keep_attrs=True, skipna=True)\n",
    "\n",
    "#         # ERA5 regridded resolution\n",
    "#         ds_era_1deg[var_id + '_season_{}_{}'.format(_lat, _lat+step)] = ds_era_1deg[var_id + '_season_mean'].sel(lat = slice(_lat, _lat+step)).mean(('lat',), keep_attrs=True, skipna=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for stat in ds_era_1deg.statistic.values:\n",
    "#     # Southern Hemisphere\n",
    "#     fig, axsm = plt.subplots(2, 2, figsize=[10,7], sharex=True, sharey=True)\n",
    "#     fig.suptitle('{}'.format(stat), fontsize=16, fontweight=\"bold\")\n",
    "\n",
    "#     axs = axsm.flatten()\n",
    "#     for ax, i in zip(axs, ds_era_1deg.season):\n",
    "#         for _lat, c in zip(iteration_SH, cm.romaO(range(0, 256, int(256 / 4)))):\n",
    "#             ax.scatter( x=ds_era_1deg['t_season_{}_{}'.format(_lat, _lat-step)].sel(statistic=stat).sum('level', skipna=True).sel(season=i), \n",
    "#                         y=ds_era_1deg['sf_season_{}_{}'.format(_lat, _lat-step)].sel(statistic=stat).sel(season=i), \n",
    "#                         label=\"{}, {}\".format(_lat, _lat - step), \n",
    "#                         color=c, \n",
    "#                         alpha=0.5)\n",
    "\n",
    "#     axs[1].legend(\n",
    "#             loc=\"upper left\",\n",
    "#             bbox_to_anchor=(1, 1),\n",
    "#             fontsize=\"small\",\n",
    "#             fancybox=True,\n",
    "#         );\n",
    "\n",
    "#     # Northern Hemisphere\n",
    "#     fig, axsm = plt.subplots(2, 2, figsize=[10,7], sharex=True, sharey=True)\n",
    "#     fig.suptitle('{}'.format(stat), fontsize=16, fontweight=\"bold\")\n",
    "\n",
    "#     axs = axsm.flatten()\n",
    "#     for ax, i in zip(axs, ds_era_1deg.season):\n",
    "#         for _lat, c in zip(iteration_NH, cm.romaO(range(0, 256, int(256 / 4)))):\n",
    "#             ax.scatter( x=ds_era_1deg['t_season_{}_{}'.format(_lat, _lat+step)].sel(statistic=stat).sum('level', skipna=True).sel(season=i), \n",
    "#                         y=ds_era_1deg['sf_season_{}_{}'.format(_lat, _lat+step)].sel(statistic=stat).sel(season=i), \n",
    "#                         label=\"{}, {}\".format(_lat, _lat + step), \n",
    "#                         color=c, \n",
    "#                         alpha=0.5)\n",
    "\n",
    "#     axs[1].legend(\n",
    "#             loc=\"upper left\",\n",
    "#             bbox_to_anchor=(1, 1),\n",
    "#             fontsize=\"small\",\n",
    "#             fancybox=True,\n",
    "#         );\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References <a id='references'></a>\n",
    "\n",
    "\n",
    "[1] Zelinka, M. D., Myers, T. A., McCoy, D. T., Po-Chedley, S., Caldwell, P. M., Ceppi, P., et al. (2020). Causes of higher climate sensitivity in CMIP6 models. Geophysical Research Letters, 47, e2019GL085782. https://doi-org.ezproxy.uio.no/10.1029/2019GL085782 \n",
    "\n",
    "[2] Bjordal, J., Storelvmo, T., Alterskjær, K. et al. Equilibrium climate sensitivity above 5 °C plausible due to state-dependent cloud feedback. Nat. Geosci. 13, 718–721 (2020). https://doi-org.ezproxy.uio.no/10.1038/s41561-020-00649-1 \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1zb0LHvipx8JOXLLrCxzYToJM7eNK4eaw\"  height=\"100\" />\n",
    "<img src=\"https://reliance.rohub.org/static/media/Reliance-logo.433dc2e9.png\"  height=\"100\" />\n",
    "\n",
    "<img src=\"https://www.uio.no/vrtx/decorating/resources/dist/src2/images/footer/uio-logo-en.svg\"  height=\"100\" />\n",
    "<img src=\"https://erc.europa.eu/sites/default/files/logo_0.png\"  height=\"100\" />\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7f24944080b15318569c1ef785be98f8dd5a0531d3a23558ab9e7edab213d92e"
  },
  "kernelspec": {
   "display_name": "Python 3.10.6 ('globalsnow': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7f24944080b15318569c1ef785be98f8dd5a0531d3a23558ab9e7edab213d92e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
